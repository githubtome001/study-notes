----------------------------------------------容器虚拟化技术Docker------------------------------------------------------- 
一、容器虚拟化技术Docker
	定义：解决了运行环境和配置问题软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术。
	特点:一次构建、随处运行，DevOps，内核虚拟化，依赖与宿主机内核（不同于传统的虚拟机技术）
	基本组成：镜像、容器、仓库
	docker安装：
	docker启动：centos 6+ 命令：service docker start
				centos 7+ 命令：systemctl start docker 
	docker关闭重启：centos 7+ 命令：systemctl restart docker  systemctl stop docker systemctl enable docker //开机自启
二、常用命令：
	帮助命令：docker version/docker info/docker --help
	镜像命令：docker images 
			  docker search XXX镜像名，可选参数：
			  docker pull XXX镜像名
			  docker rmi -f XXX镜像名ID
			  移除所有镜像：docker rmi -f $(docker images -qa)
	容器命令：docker run [OPTIONS] IMAGE [COMMAND] [ARG...]
			   OPTIONS说明（常用）：有些是一个减号，有些是两个减号
				--name="容器新名字": 为容器指定一个名称；
				-d: 后台运行容器，并返回容器ID，也即启动守护式容器；
				-i：以交互模式运行容器，通常与 -t 同时使用；
				-t：为容器重新分配一个伪输入终端，通常与 -i 同时使用；
				-P: 随机端口映射；
				-p: 指定端口映射，有以下四种格式
					  ip:hostPort:containerPort
					  ip::containerPort
					  hostPort:containerPort
					  containerPort 
			当前所有正在运行的容器：docker ps [OPTIONS] 
									OPTIONS说明（常用）：
									-a :列出当前所有正在运行的容器+历史上运行过的
									-l :显示最近创建的容器。
									-n：显示最近n个创建的容器。
									-q :静默模式，只显示容器编号。
									--no-trunc :不截断输出。
			退出容器：容器停止退出exit
					  容器不停止退出ctrl+P+Q
			启动/停止/重启/强制停止容器：docker start/stop/restart/kill 容器ID或者容器名
			删除已停止的容器：docker rm -f 容器ID 
			删除所有容器：docker rm -f $(docker ps -a -q) 或docker ps -a -q | xargs docker rm
			启动守护式容器：docker run -d 容器名
			查看容器日志：docker logs -f -t --tail n 容器ID
			查看容器内运行的进程：docker top 容器ID
			查看容器内部细节：docker inspect 容器ID
			进入正在运行的容器并以命令行交互：docker exec -it 容器ID bashShell
											  重新进入docker attach 容器ID
			从容器内拷贝文件到主机上：docker cp 容器ID:容器内路径 目的主机路径
三、Docker镜像
	镜像加速原理：bootfs(加载kernel) / rootfs(不同的操作系统发行版)			
	分层的联合文件系统：最大的一个好处就是 - 共享资源
	Docker镜像commit操作补充：docker commit -m=“提交的描述信息” -a=“作者” 容器ID 要创建的目标镜像名:[标签名]
四、Docker容器数据卷
	作用：容器间继承+共享数据、容器的持久化
	卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷
	特点：
		1：数据卷可在容器之间共享或重用数据
		2：卷中的更改可以直接生效
		3：数据卷中的更改不会包含在镜像的更新中
		4：数据卷的生命周期一直持续到没有容器使用它为止
	直接命令添加：
		docker run -it -v /宿主机绝对路径目录:/容器内目录[:ro] [--privileged=true] 镜像名
	查看数据卷是否挂载成功
		docker inspect 容器ID
	容器和宿主机之间数据共享				  
	容器停止退出后，宿主机修改后数据是否同步-同步				  
	DockerFile添加-生成镜像：
		使用方式：在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷，格式：VOLUME["/dataVolumeContainer1","/dataVolumeContainer2","/dataVolumeContainer3"]
		构建：docker build -f Dockerfile路径 -t 新的镜像名
	数据卷容器(容器间继承+传递共享数据)：
		语法：--volumes-from，例如：docker run -it --name doc2 --volumes-from doc01 zzyy/centos
五、DockerFile解析
	定义：Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。
	构建三步骤：编写Dockerfile文件、docker build、docker run
	关系：
		Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石
	保留字:
六、Docker发布
	aliyun镜像的使用：
	登录阿里云Docker Registry：$ sudo docker login --username=ali域逸诚 registry.cn-hangzhou.aliyuncs.com
	从Registry中拉取镜像：$ sudo docker pull registry.cn-hangzhou.aliyuncs.com/ali_yuyicheng/redis_test:[镜像版本号]				  
	将镜像推送到Registry：				  
					$ sudo docker login --username=ali域逸诚 registry.cn-hangzhou.aliyuncs.com
					$ sudo docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/ali_yuyicheng/redis_test:[镜像版本号]
					$ sudo docker push registry.cn-hangzhou.aliyuncs.com/ali_yuyicheng/redis_test:[镜像版本号]  
					
----------------------------------------------文档数据结构MongoDB的使用-------------------------------------------------------					  
一、npm（Node Package Manager 包管理器）工具的使用
	- 通过npm可以对node中的包进行上传、下载、搜索等操作
		- npm会在安装完node以后，自动安装
		- npm的常用指令
			npm -v 查看npm的版本
			npm version 查看所有模块的版本
			npm init 初始化项目-创建package.json
			npm i/install 包名 安装指定的包
			npm i/install 包名 --save 安装指定的包并添加依赖
			npm i/install 包名 -g 全局安装（一般都是一些工具）
			npm i/install 安装当前项目所依赖的包
			npm s/search 包名 搜索包	
			npm r/remove 包名 删除一个包
二、MongoDB	
	定义：为快速开发WEB应用而设计的数据库，面向文档，类似JSON数据(BSON),注意：MongoDB数据库使用的是JavaScript进行操作的，在MongoDB含有一个对ES标准实现的引擎，
		在MongoDB中所有ES中的语法中都可以使用
	启动：
		- 打开cmd命令行窗口
			- 输入 mongod 启动mongodb服务器
			- 32位注意：
				启动服务器时，需要输入如下内容
					mongod --storageEngine=mmapv1
					mongod --dbpath 数据库路径 --port 端口
			
		- 再打开一个cmd窗口
			- 输入 mongo 连接mongodb ，出现 >				  
					  
	- 基本概念
		数据库（database）- 类比SQL数据库、集合（collection）- 类比SQL表、文档（document）- 类比SQL表记录
		- 在MongoDB中，数据库和集合都不需要手动创建，当我们创建文档时，如果文档所在的集合或数据库不存在会自动创建数据库和集合		
		
	- 基本指令
		show dbs/databases 
			- 显示当前的所有数据库
		use 数据库名
			- 进入到指定的数据库中
		db
			- db表示的是当前所处的数据库
		show collections
			- 显示数据库中所有的集合
			
	- 数据库的CRUD（增删改查）的操作
		  
		- 向数据库中插入文档
		- db.collection.insert()
			- insert()可以向集合中插入一个或多个文档
		- db.collection.insertOne()
			- 向集合中插入一个文档
		- db.collection.insertMany()
			- 向集合中插入多个文档
		如：db.stus.insert([
				{name:"沙和尚",age:38,gender:"男"},
				{name:"白骨精",age:16,gender:"女"},
				{name:"蜘蛛精",age:14,gender:"女"}
			]);
			db.stus.find({}); //查询所有文档
				
		- 查询数据库中的文档
			- db.collection.find()
				- 可以根据指定条件从集合中查询所有符合条件的文档
				- 返回的是一个数组
			- db.collection.findOne()
				- 查询第一个符合条件的文档
				- 返回的是一个对象
			- db.collection.find().count()
				- 查询符合条件的文档的数量
			查询条件：
				-$gt $eq $lt $lte limit() $or
				如：db.emp.find({sal:{$lt:2000 , $gt:1000}}).limit(5);
					db.emp.find({$or:[{sal:{$lt:1000}} , {sal:{$gt:2500}}]});
			分页：
			skip()用于跳过指定数量的数据，MongoDB会自动调整skip和limit的位置
			如：db.numbers.find().skip(10).limit(10); -第11条到20条数据	
			如：db.stus.find({age:16 , name:"白骨精"});
				
		- 修改数据库中的文档
			- db.collection.update()
				- 可以修改、替换集合中的一个或多个文档（默认修改一个）
				注意：  - update()默认情况下会使用新对象来替换旧的对象
						- 如果需要修改指定的属性，而不是替换需要使用“修改操作符”来完成修改
							$set 可以用来修改文档中的指定属性
							$unset 可以用来删除文档的指定属性
							内嵌文档中：
							$push 用于向数组中添加一个新的元素
							$addToSet 向数组中添加一个新元素 ，如果数组中已经存在了该元素，则不会添加
							$inc 增加到	，如:db.emp.updateMany({sal:{$lte:1000}} , {$inc:{sal:400}});						
						- update()默认只会修改一个
			- db.collection.updateOne()
				- 修改集合中的一个文档
			- db.collection.updateMany()
				- 修改集合中的多个文档
			- db.collection.replaceOne()
				- 替换集合中的一个文档
			如：db.stus.update(
				{"_id" : ObjectId("59c219689410bc1dbecc0709")},
				{$set:{
					gender:"男",
					address:"流沙河"
				}}    
			)
			又如：db.stus.update(
					{"name" : "猪八戒"},
					
					{
						$set:{
						address:"呵呵呵"
						}
					}  ,
					{
						multi:true
					}    
				)
				
		- 删除集合中的文档
			- db.collection.remove()
				- 删除集合中的一个或多个文档（默认删除多个），可以第二个参数传递一个true，则只会删除一个
			- db.collection.deleteOne()
				- 删除集合中的一个文档
			- db.collection.deleteMany()
				- 删除集合中的多个文档
			- 清空一个集合
				db.collection.remove({})
			- 删除一个集合
				db.collection.drop()
			- 删除一个数据库
				db.dropDatabase()	
			注意：一般数据库数据中添加一个字段，来表示数据是否被删除
				如：db.stus.insert([
					{
						name:"zbj",
						isDel:0
						},
						{
						name:"shs",
						isDel:0
						},
					{
					name:"ts",
						isDel:0
					}

				]);
				db.stus.updateOne({name:"ts"},{$set:{isDel:1}});	
				db.stus.find({isDel:0});
	文档关系：一对一（one to one），内嵌文档的形式
			  如：db.wifeAndHusband.insert([
					{
						name:"黄蓉",
						husband:{
							name:"郭靖"
						}
					},{
						name:"潘金莲",
						husband:{
							name:"武大郎"
						}
					}

				]);
			  一对多（one to many）/多对一(many to one)，也可以用内嵌文档的形式
			  多对多(many to many) 用两个集合表示
	排序与投影：
			查询文档时，默认情况是按照_id的值进行排列（升序）
			sort()可以用来指定文档的排序的规则,sort()需要传递一个对象来指定排序规则 1表示升序 -1表示降序
			limit skip sort 可以以任意的顺序进行调用
			如：db.emp.find({}).sort({sal:1,empno:-1});
			在查询时，可以在第二个参数的位置来设置查询结果的投影，0不显示；1：显示
			如：db.emp.find({},{ename:1 , _id:0 , sal:1});
	分组查询：
		分组分片查询，aggregate的使用
		//源数据
		db.items.insert( [
		  {
		   "quantity" : 2,
		   "price" : 5.0,
		   "pnumber" : "p003",
		  }...,{
		   "quantity" : 5,
		   "price" : 10.0,
		   "pnumber" : "p002"
		  }
		])    
		//$group语法:{ $group: { _id: <expression>, <field1>: { <accumulator1> : <expression1> }, ... } }
		/*
		 *_id 分组的key,expression分组的字段，如果_id为null 相当于SQL:select count(*) from table
		 *field1 分组后展示的字段
		 *accumulator1 分组管道函数，如：$sum\$avg\$min\$max\$push\$addToSet\$first\$last
		 *expression1  分组显示数据相关
		 */	
		//查询总数,相当于SQL:select count(1) as count from items
		db.items.count();
		db.items.aggregate([{$group:{_id:null,count:{$sum:1}}}])
		//统计数量，相当于SQL:select sum(quantity) as total  from  items
		db.items.aggregate([{$group:{_id:null,total:{$sum:"$quantity"}}}]);
		//按产品类型来进行分组，然后在统计卖出的数量是多少，相当于SQL：select sum(quantity) as total from  items  group by pnumber
		db.items.aggregate([{$group:{_id:"$pnumber",total:{$sum:"$quantity"}}}])
		//通过相同的产品类型来进行分组，然后查询相同产品类型卖出最多的订单详情，相当于SQL:select max(quantity) as quantity from  items  group by pnumber
		db.items.aggregate([{$group:{_id:"$pnumber",max:{$max:"$quantity"}}}])
		db.items.aggregate([{$group:{_id:"$pnumber",min:{$min:"$quantity"}}}])
		//通过相同的产品类型来进行分组，统计各个产品数量，然后获取最大的数量，相当于SQL:select max(t.total) from (select sum(quantity) as total from  items  group by pnumber) t
		db.items.aggregate([{$group:{_id:"$pnumber",total:{$sum:"$quantity"}}}])
		db.items.aggregate([{$group:{_id:"$pnumber",total:{$sum:"$quantity"}}},{$group:{_id:null,max:{$max:"$total"}}}])
		//通过相同的产品类型来进行分组，然后查询每个订单详情相同产品类型卖出的平均价格，相当于SQL:select avg(price) as price from  items  group by pnumber
		db.items.aggregate([{$group:{_id:"$pnumber",price:{$avg:"$price"}}}])
		//通过相同的产品类型来进行分组，然后查询每个相同产品卖出的数量放在数组里面,，注意值数组中的值不要超过16M
		db.items.aggregate([{$group:{_id:"$pnumber",quantitys:{$push:"$quantity"}}}])
		db.items.aggregate([{$group:{_id:"$pnumber",quantitys:{$push:{quantity:"$quantity",price:"$price"}}}}])
		//表达式的值添加到一个数组中（无重复值），这个值不要超过16M
		db.items.aggregate([{$group:{_id:"$pnumber",quantitys:{$addToSet:"$quantity"}}}])
		//$first：返回每组第一个文档，如果有排序，按照排序，如果没有按照默认的存储的顺序的第一个文档。
		//$last：返回每组最后一个文档，如果有排序，按照排序，如果没有按照默认的存储的顺序的最后个文档。
		db.items.aggregate([{$group:{_id:"$pnumber",quantityFrist:{$first:"$quantity"}}}])
		//$project显示或不显示字段，相当于SQL:select
		db.items.aggregate([{$group:{_id:null,count:{$sum:1}}},{$project:{"_id":0,"count":1}}])
		//通过滤订单中，想知道卖出的数量大于8的产品有哪些产品，相当于SQL:select sum(quantity) as total from  items  group by pnumber having total>8   
		db.items.aggregate([{$group:{_id:"$pnumber",total:{$sum:"$quantity"}}},{$match:{total:{$gt:8}}}])
		//$match如果是放在$group之前就是当做where来使用，我们只统计pnumber =p001 产品卖出了多少个  select sum(quantity) as total from  items where pnumber='p001'
		db.items.aggregate([{$match:{"pnumber":"p001"}},{$group:{_id:null,total:{$sum:"$quantity"}}}])
		//$skip、$limit使用顺序不同，结果也不同，注意：$limit、$skip、$sort、$match可以使用在阶段管道，如果使用在$group之前可以过滤掉一些数据，提高性能
		db.items.aggregate([{ $skip: 2 },{ $limit: 4 },{ $sort: { quantity : -1 }}])
		db.items.aggregate([{ $limit: 4 },{ $skip: 2 }])
		//将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值
		db.items.aggregate([{$group:{_id:"$pnumber",quantitys:{$push:"$quantity"}}}])
		db.items.aggregate([{$group:{_id:"$pnumber",quantitys:{$push:"$quantity"}}},{$unwind:"$quantitys"}])
		//$out必须为pipeline最后一个阶段管道，因为是将最后计算结果写入到指定的collection中
		db.items.aggregate([{$group:{_id:"$pnumber",quantitys:{$push:"$quantity"}}},{$unwind:"$quantitys"},{$project:{"_id":0,"quantitys":1}},{$out:"result"}])
		db.result.find()
	模糊查询：$options - 可选参数				  
		如：db.students.find({"user_name": {$regex: /尚/, $options:'i'}}); 
			db.students.find({"user_name": {$regex:/尚.*/i}}); 
			db.students.find({user_name:{$in:[/^孙尚香/i,/^胡歌/]}});
			db.students.find({user_name:{$regex:/^孙尚香/i,$nin:['孙尚香II']}});
			db.students.find({user_name:{$regex:/香/,$options:"si"}});
	
			
----------------------------------------------SpringCloud技术栈的使用---------------------------------------------------------						  
	基础概念：
	    微服务：强调的是服务的大小，它关注的是某一个点，是具体解决某一个问题/提供落地对应服务的一个服务应用
		-最早提出：马丁福勒
				微服务化的核心是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底
		地去耦合,每一个微服务提供单个业务功能的服务，一个服务做一件事，
		从技术角度看就是一种小而独立的处理过程，类似进程概念，能够自行单独启动
		或销毁，拥有自己独立的数据库。
		微服务架构：架构模式，微服务之间互相协调、互相配合，每个服务运行在其独立的进程中，
			服务与服务间采用轻量级的通信机制互相协作（通常是基于HTTP协议的RESTful API），包括一些环境，有机构成的整体。
		优点缺点：
		-优点：内聚小，松耦合，拥有独立的进程，开发简单；
			   易于与第三方集成，如：与持续自动化构建部署（Jenkins, Hudson）
			   微服务只是业务逻辑的代码，不会和HTML,CSS 或其他界面组件混合（前后端分离思想）。
		-缺点：开发人员要处理分布式系统的复杂性
			   多服务运维难度，随着服务的增加，运维的压力也在增大
			   系统部署依赖
			   服务间通信成本、数据一致性、系统集成测试、性能监控……
		微服务技术栈：
			微服务条目			落地技术											备注			               
			
			服务开发			Springboot、Spring、SpringMVC
			服务配置与管理		Netflix公司的Archaius、阿里Diamond等
			服务注册与发现		Eureka、Consul、Zookeeper、AliBabaNacos等
			服务调用			Rest、RPC、gRPC
			服务熔断器			Hystrix、Envoy等
			负载均衡			Ribbon、Feign Nginx等
			服务接口调用(客户端调用服务的简化工具)
								Feign等
			消息队列			Kafka、RabbitMQ、ActiveMQ等
			服务配置中心管理  	SpringCloudConfig、Chef等
			服务路由（API网关） Zuul、Spring Cloud Gateway等
			服务监控			Zabbix、Nagios、Metrics、Spectator等
			全链路追踪			Zipkin，Brave、Dapper、Sleuth等
			服务部署			Docker、OpenStack、Kubernetes等
			数据流操作开发包	SpringCloud Stream（封装与Redis,Rabbit、Kafka等发送接收消息）
			事件消息总线    	Spring Cloud Bus
			......
    SpringCloud入门介绍：
		含义：SpringCloud=分布式微服务架构下的一站式解决方案，是各个微服务架构落地技术的集合体，俗称微服务全家桶
		与springboot之间关系：SpringBoot可以离开SpringCloud独立使用开发项目，
							  但是SpringCloud离不开SpringBoot，属于依赖的关系.
							  SpringBoot专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架。
		与Dubbo的比较： 最大区别：SpringCloud抛弃了Dubbo的RPC通信，采用的是基于HTTP的REST方式。
						REST相比RPC更为灵活，服务提供方和调用方的依赖只依靠一纸契约，这在强调快速演化的微服务环境下，显得更加合适。
						品牌机与组装机的区别
						Spring Cloud的功能比DUBBO更加强大，涵盖面更广，而且作为Spring的拳头项目，它也能够与Spring Framework、Spring Boot、Spring Data、Spring Batch等其他Spring项目完美融合，这些对于微服务而言是至关重要的。
						使用Dubbo构建的微服务架构就像组装电脑，各环节我们的选择自由度很高；
						而Spring Cloud就像品牌机，在Spring Source的整合下，做了大量的兼容性测试，保证了机器拥有更高的稳定性。
						社区支持与更新力度
						最为重要的是，DUBBO停止了5年左右的更新，虽然2017.7重启了(刘军)。对于技术发展的新需求，需要由开发者自行拓展升级（比如当当网弄出了DubboX），这对于很多想要采用微服务架构的中小软件组织，显然是不太合适的，中小公司没有这么强大的技术能力去修改Dubbo源码+周边的一整套解决方案，并不是每一个公司都有阿里的大牛+真实的线上生产环境测试过。
	简单服务调用：
			-RestTemplate + API 使用方式： 
			 RestTemplate提供了多种便捷访问远程Http服务的方法，是一种简单便捷的访问restful服务模板类，是Spring提供的用于访问Rest服务的客户端模板工具集 
			使用步骤：
			1，注入Bean：
			如：@Configuration
				public class ConfigBean
				{
				 @Bean
				public RestTemplate getRestTemplate()
					{
				return new RestTemplate();
					}
				}
			2，声明使用：
			@Autowired
			private RestTemplate restTemplate;
			
			方法中：
			restTemplate.postForObject(REST_URL_PREFIX+"/dept/add", dept, Boolean.class); - 参数：REST请求地址、请求参数、HTTP响应转换被转换成的对象类型

	服务注册与发现：
			-Eureka(C/S)：
				简介：Netflix公司的子模块，是一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。Netflix在设计Eureka时遵守的就是AP原则
				CAP原则：CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得
				三大角色：1、Eureka Server 提供服务注册和发现；2、Service Provider服务提供方将自身服务注册到Eureka，从而使服务消费方能够找到；3、Service Consumer服务消费方从Eureka获取注册服务列表，从而能够消费服务
			使用注册：
				服务端：
				1,引用模块：POM.XML
					<dependency>
					<groupId>org.springframework.cloud<groupId>
					<artifactId>spring-cloud-starter-eureka-server<artifactId>
					</dependency>
				2,配置参数：application.properties或YML文件的配置
				格式：eureka.instance.hostname=localhost
					  #不要向注册中心注册自己
					  eureka.client.register-with-eureka=false
					  #禁止检索服务
					  eureka.client.fetch-registry=false
					  eureka.client.service-url.defaultZone=http://${eureka.instance.hostname}:${server.port}/eureka	
				3,申明使用：程序main入口，添加@EnableEurekaServer注解，来开启服务注册中心
				客户端：
				1,引用模块：POM.XML
					 <dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-eureka</artifactId>
					</dependency>
				2,配置参数：application.properties或YML文件的配置
				格式：
					#设置服务名
					spring:
					  application:
						name: 服务名
					eureka:
						  client: #客户端注册进eureka服务列表内
							service-url: 
							  defaultZone: http://localhost:7001/eureka
							  instance:
								instance-id: 服务实例名（设置后可以隐藏主机名）
							  prefer-ip-address: true #访问路径可以显示IP地址
							  #点击显示
							  info:
								  app.name: 服务程序名
								  company.name: 公司名
								  build.artifactId: $project.artifactId$
								  build.version: $project.version$
							  
				3,申明使用：主类上添加@EnableEurekaClient注解以实现Eureka中的DiscoveryClient实现，可以使用@EnableDiscoveryClient代替
			使用发现：
				申明使用，主启动类添加@EnableDiscoveryClient
			    使用时注入， @Autowired
							 private DiscoveryClient client;即可使用，注：DiscoveryClient是spring clould 对治理体系的一个抽象
			自我保护机制：某时刻某一个微服务不可用了，eureka不会立刻清理，依旧会对该微服务的信息进行保存
			集群处理：
				在Eureka服务器中添加配置：
				eureka: 
					instance:
						hostname: eureka1.com #eureka服务端的实例名称
					client: 
						register-with-eureka: false #false表示不向注册中心注册自己。
						fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务
					service-url: 
					#单机 defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/       #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址（单机）。
					defaultZone: http://eureka1.com:7002/eureka/,http://eureka3.com:7003/eureka/
			与Dubbo的Zookeeper的比较：
				著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性P在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。
				因此,Zookeeper保证的是CP,Eureka则是AP。因此，Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。

			-Consul：
				简介：Spring Cloud Consul是分布式的、高可用、横向扩展(AP原则)，它包含多个组件，但是作为一个整体，在微服务架构中为我们的基础设施提供服务发现和服务配置的工具。它包含了下面几个特性：
					  服务发现（service discovery）
					  健康检查（health checking）
					  Key/Value存储（一个用来存储动态配置的系统）
					  多数据中心（multi-datacenter）
				使用-客户端：
				1,引用模块：POM.XML
					<dependency>
					  <groupId>org.springframework.cloud</groupId>
					  <artifactId>spring-cloud-starter-consul-discovery</artifactId>
					</dependency>
				2,配置参数：application.properties或YML文件的配置
				格式：
					  spring.cloud.consul.host=localhost #域名
					  spring.cloud.consul.port=8500 #端口
	
				3,申明使用：程序main入口，添加@EnableDiscoveryClient注解，开启服务治理
				注意：consul不需要创建类似eureka-server的服务端吗？由于Consul自身提供了服务端，所以我们不需要像之前实现Eureka的时候创建服务注册中心，直接通过下载consul的服务端程序就可以使用。
				启动consul服务：$consul agent -dev
				
			-Nocas：
				参考Spring Cloud Alibaba 技术站的Nocas相关内容；
				
				详情参考：https://blog.csdn.net/qq_38765404/article/details/89521124

	负载均衡：
			-Ribbon（结合Eureka使用）：
			 简介：基于Netflix实现的一套客户端负载均衡的工具，主要功能是提供客户端的软件负载均衡算法。
			 负载均衡：将用户的请求基于某种规则平摊的分配到多个服务上，从而达到系统的HA，常见的负载均衡有软件Nginx，LVS，硬件F5等
				分类：集中式LB-即在服务的消费方和提供方之间使用独立的LB设施(可以是硬件，如F5, 也可以是软件，如nginx), 由该设施负责把访问请求通过某种策略转发至服务的提供方；
					  进程内LB-将LB逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。Ribbon就属于进程内LB。
			 使用：
				1,引用模块：POM.XML
					 <dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-ribbon</artifactId>
					</dependency>
					<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-eureka</artifactId>
					</dependency>
				2,配置参数：application.properties或YML文件的配置
				格式：（eureka配置）
					server:
						port: 80
						eureka:
							client:
							register-with-eureka: false
							service-url: 
							defaultZone: http://eureka1.com:7001/eureka/,http://eureka2.com:7002/eureka/,http://eureka3.com:7003/eureka/
	
				3,申明使用：1、程序main入口，添加@EnableDiscoveryClient注解，开启服务治理；2、在配置文件ConfigBean 中添加@LoadBalanced注解
				  -》结论：Ribbon和Eureka整合后服务消费方可以直接调用服务而不用再关心地址和端口号
			    4,策略IRule：简单轮询负载均衡RoundRobinRule，区别于RetryRule，随机负载均衡，加权响应时间负载均衡 ，区域感知轮询负载均衡
			 自定义Ribbon：	
				主启动类添加@RibbonClient注解，格式：@RibbonClient(name="MICROSERVICECLOUD",configuration=MySelfRule.class)，注意：这个自定义配置类不能放在@ComponentScan所扫描的当前包下以及子包下，
					否则我们自定义的这个配置类就会被所有的Ribbon客户端所共享，也就是说我们达不到特殊化定制的目的了。
				在配置文件ConfigBean中添加@LoadBalanced注解
			
			-Feign（声明式服务调用，WebService客户端）：它的使用方法是定义一个接口，然后在上面添加注解，同时也支持JAX-RS（Java API for RESTful Web Services）标准的注解。
			 特性：
				可插拔式的注解支持，包括Feign注解和JAX-RS注解;
				支持可插拔的HTTP编码器和解码器;
				支持Hystrix和它的Fallback;
				支持Ribbon的负载均衡;
				支持HTTP请求和响应的压缩
			 使用：
				1,引用模块：POM.XML
					 <dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-feign</artifactId>
					</dependency>
					　<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-eureka</artifactId>
						<version>1.3.5.RELEASE</version>
					</dependency>
				2,配置参数：application.properties或YML文件的配置
				格式：（整合eureka配置）
					server:
						port: 80
						eureka:
							client:
							register-with-eureka: false
							service-url: 
							defaultZone: http://eureka1.com:7001/eureka/,http://eureka2.com:7002/eureka/,http://eureka3.com:7003/eureka/
	
				3,申明使用：1、程序main入口，添加@EnableDiscoveryClient注解，开启服务治理；同时添加@EnableFeignClients 来开启feign
			                2, 定义接口：
							   格式：value=“服务名称”,configuration = xxx.class 这个类配置Hystrix的一些精确属性
			                   @FeignClient(value = "serviceName",fallback = FeignFallBack.class)
							    public interface FeignService {
									@RequestMapping(value = "/ml", method= RequestMethod.GET)
									String method1(@RequestParam("name") String name) ;
								}
								@Component
								public class FeignFallBack implements FeignService{
							　　//实现的方法是服务调用的降级方法
								@Override
								public String method1() {
									return "error";
								}
								
	服务熔断器（断路器）：		 
			-Hystrix：
			 概念来源：服务雪崩：
				服务扇出：多个微服务之间调用的时候，假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其它的微服务，这就是所谓的“扇出”效应。
				解决方案：熔断模式（容错处理机制）、隔离模式（容错处理机制）、限流模式（预防模式）
			 简介：Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统（SOA面向服务架构）里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。
				   “断路器”本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），
				   向调用方返回一个符合预期的、可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常，这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。
			 服务熔断
				使用：
					概念：当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回"错误"的响应信息。 
						  SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。熔断机制的注解是@HystrixCommand。
					1,引用模块：POM.XML
						 <dependency>
							<groupId>org.springframework.cloud</groupId>
							<artifactId>spring-cloud-starter-hystrix</artifactId>
						</dependency>
					2,配置参数：application.properties或YML文件的配置
					格式：（配合eureka配置）
						server:
							port: 80
							eureka:
								client:
								register-with-eureka: false
								service-url: 
								defaultZone: http://eureka1.com:7001/eureka/,http://eureka2.com:7002/eureka/,http://eureka3.com:7003/eureka/
		
					3,申明使用：1、程序main入口，添加@EnableCircuitBreaker注解，开启熔断支持；
								2、在控制层申明使用
									格式示例：fallbackMethod 快速应急的处理方法，进行服务降级处理
									@RequestMapping(value="/dept/get/{id}",method=RequestMethod.GET)
									@HystrixCommand(fallbackMethod = "processHystrix_Get")
									public Dept get(@PathVariable("id") Long id){
									    Dept dept =  this.service.get(id);
										if(null == dept){
										throw new RuntimeException("该ID："+id+"没有没有对应的信息");
									    }
										return dept;	
									}
									public Dept processHystrix_Get(@PathVariable("id") Long id){ 
										return new Dept().setDeptno(id)
											   .setDname("该ID："+id+"没有没有对应的信息,null--@HystrixCommand")
											   .setDb_source("no this database in MySQL");
									}
			 
			 服务降级Fallback
				使用： 
					概念：整体资源快不够了，忍痛将某些服务先关掉，待渡过难关，再开启回来。
						  服务降级处理是在客户端实现完成的，与服务端没有关系。
				    申明使用（与Feign结合使用）：	
						格式示例：
						@FeignClient(value = "MICROSERVICECLOUD",fallbackFactory=DeptClientServiceFallbackFactory.class)
						public interface DeptClientService{
							@RequestMapping(value = "/dept/get/{id}",method = RequestMethod.GET)
							public Dept get(@PathVariable("id") long id);
			            }
						
						@Component
						public class DeptClientServiceFallbackFactory implements FallbackFactory{
						
							@Override
							public Dept get(@PathVariable("id") long id){
								...
							}  
						}
						
						注意：在application.properties或YML文件的配置添加一行配置
						      feign: 
								hystrix: 
									enabled: true //开启   
			 
			 服务监控HystrixDashboard
			    使用：
				    概念：Hystrix还提供了准实时的监控（Hystrix Dashboard），Spring Cloud也提供了Hystrix Dashboard的整合，对监控内容转化成可视化界面。
					1,引用模块：POM.XML
						 <dependency>
							<groupId>org.springframework.cloud</groupId>
							<artifactId>spring-cloud-starter-hystrix</artifactId>
						</dependency>
						<dependency>
							<groupId>org.springframework.cloud</groupId>
							<artifactId>spring-boot-starter-actuator</artifactId>
						</dependency>
						
						在服务的监控一方添加如下配置
						<dependency>
							<groupId>org.springframework.cloud</groupId>
							<artifactId>spring-cloud-starter-hystrix-dashboard</artifactId>
						</dependency>
					2,配置参数：application.properties或YML文件的配置
					格式：
						server:
							port: 9901
		
					3,申明使用：1、程序main入口，添加@EnableHystrixDashboard注解，开启熔断监控的支持；
	
	路由网关：
			-Zuul：
			 概念：代理+路由+过滤三大功能，可以与Eureka整合并注册到注册中心里面
			 使用：
				1,引用模块：POM.XML
					<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-eureka</artifactId>
					</dependency>
					<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-zuul</artifactId>
					</dependency>
				2,配置参数：application.properties或YML文件的配置
				格式：
					zuul: 
					    prefix: /pre //前缀
						ignored-services: 服务名  //多个指定微服务以半角逗号分隔，所有可以用"*"通配符代替
						routes: 
							serverName.path: /serverName/**
							serverName.serviceId: 服务名 //给微服务起别名
							//或使用：serverName.url: http://${IP}:${PORT}/  //这种基于未使用服务注册中心的
						

				3,申明使用：1、程序main入口，添加@EnableZuulProxy注解，开启Zuul的支持；
				
	
	分布式配置中心：
				 -SpringCloud Config（与Git整合使用）
				 概念来源：分布式系统面临的---配置问题
				 概念：为微服务架构中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置
				    分类：服务端和客户端
						  服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密/解密信息等访问接口
						  客户端则是通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息配置服务器默认采用git来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过git客户端工具来方便的管理和访问配置内容。
					规则：1、不同环境不同配置，动态化的配置更新
						  2、运行期间动态调整配置，不再需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息
						  3、当配置发生变动时，服务不需要重启即可感知到配置的变化并应用新的配置
						  4、将配置信息以REST接口的形式暴露
				 使用：
					 1、用自己的Github账户建一个统一配置中心仓库，并克隆到本地；
					 2、新建配置文件application.yml（保存格式必须为UTF-8）
					    格式参考示例：
						spring:
						  profiles:
							active:
							- dev
						---
						spring:
						  profiles: dev     #开发环境
						  application: 
							name: microservicecloud-config-dev
						---
						spring:
						  profiles: test   #测试环境
						  application: 
							name: microservicecloud-config-test
						#  请保存为UTF-8格式
					 3、push到git仓库
					 服务端配置使用：
						1,引用模块：POM.XML
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-config-server</artifactId>
							</dependency>
						2,配置参数：application.properties或YML文件的配置
									格式：	
									spring:
										application:
											name: microservicecloud-config
										cloud:
											config:
												server:
													git:
														uri: git@github.com***.git #GitHub上面的git仓库名字
						3,申明使用：1、程序main入口，添加@EnableConfigServer注解，开启Config的支持；		
						配置读取规则：
						  1、/{application}-{profile}.yml 如：http://config-3344.com:3344/application-dev.yml
						  2、/{application}/{profile}[/{label}] 如：http://config-3344.com:3344/application/dev/master
						  3、/{label}/{application}-{profile}.yml 如：http://config-3344.com:3344/master/application-dev.yml
					 客户端配置使用：
					      前提准备：
							本地仓库新建配置文件yml并提交到git仓库里，如：microservicecloud-config-client.yml
								参考示例：
									spring:
										profiles:
											active:
												- dev
									---
									server: 
										port: 8201 
									spring:
										profiles: dev
										application: 
											name: microservicecloud-config-client
									eureka: 
										client: 
											service-url: 
												defaultZone: http://eureka-dev.com:7001/eureka/   
									---
									server: 
										ort: 8202 
									spring:
										profiles: test
										application: 
											name: microservicecloud-config-client
									eureka: 
										client: 
											service-url: 
												defaultZone: http://eureka-test.com:7001/eureka/

						  1,引用模块：POM.XML
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-starter-config</artifactId>
							</dependency>
						  2,配置参数：bootstrap.yml配置文件的配置（系统级），而application.yml是用户级
									格式：	
									spring:
										cloud:
											config:
												name: microservicecloud-config-client #需要从github上读取的资源名称，注意没有yml后缀名
												profile: dev #本次访问的配置项
												label: master   
												uri: http://config-3344.com:3344  #本微服务启动后先去找3344号服务（链接Config服务端），通过SpringCloudConfig获取GitHub的服务地址
						  3,申明使用：
									测试：在控制层使用，如：@Value("${spring.application.name}")
															 private String applicationName;
						  
----------------------------------------------Spring Cloud Alibaba 技术栈的使用---------------------------------------------------------						  
					 
	基本介绍：Spring Cloud Alibaba 致力于提供微服务开发的一站式解决方案，依托 Spring Cloud Alibaba，只需要添加一些注解和少量配置，就可以将 Spring Cloud 应用接入阿里微服务解决方案，
			  通过阿里中间件来迅速搭建分布式应用系统。
    基本技术栈：
			  1、服务限流降级：默认支持 Servlet、Feign、RestTemplate、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控
			  2、服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持
			  3、分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新
			  4、消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力
			  5、分布式事务：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题
			  6、阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据
			  7、分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务
			  8、阿里云短信服务：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道
					  
	1、服务注册与发现
		-Nocas（= Spring Cloud Eureka + Spring Cloud Config）：
				简介：Spring Cloud Alibaba 项目中开发分布式应用微服务的子组件，致力于服务发现、配置和管理微服务，基于 DNS 和基于 RPC 的服务发现。
				关键特性：
					1、服务发现和服务健康监测；
					2、动态配置服务（通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-config 实现配置的动态变更）；
					3、动态DNS服务与服务及其元数据管理（通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-discovery 实现服务的注册与发现）
				使用-客户端（服务注册与发现）：
				1,引用模块：POM.XML
					<!--Nacos的服务注册与发现模块-->
					<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
					</dependency>
					<!--统一管理-->
					<dependencyManagement>
						<dependencies>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-dependencies</artifactId>
								<version>Greenwich.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>0.2.2.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
						</dependencies>
				</dependencyManagement>
				2,配置参数：application.properties或YML文件的配置
				格式：
					 spring:
						  application:
							name: 程序名
						  cloud:
							nacos:
							  discovery:
								server-addr: 192.168.43.142:8848 # 服务IP与端口
									metadata: # 元数据管理
									  name1: healthy1 
									  name2: healthy2

				3,申明使用：程序main入口，添加@EnableDiscoveryClient注解，开启服务治理与发现
				
				使用-客户端（动态配置，相当于SpringCloud Config）：
				1,引用模块：POM.XML	
				<!--Nacos分布式配置模块-->
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					  <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
				</dependency>
				<!--统一管理-->
				<dependencyManagement>
					<dependencies>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-dependencies</artifactId>
								<version>Greenwich.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>0.2.2.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
					</dependencies>
				</dependencyManagement>

				2,配置参数：创建bootstrap.yml或创建bootstrap.properties文件
				格式：
					spring:
					  application:
						name: nacos-config-client # 统一配置中心客户端
					  cloud:
						nacos:
						  config:
							server-addr: 192.168.43.142:8848 # nacos服务端
							file-extension: yml
					server:
					  port: 9094 
				3、准备外部的统一配置文件：
							本地仓库新建配置文件yml（properties）并提交到git仓库里，如：nacos-config-client.properties或nacos-config-client.yml								
							# nacos默认加载的是nacos-config-client.properties文件，如果需要加载yml，需要在yml增加一行配置：file-extension: yml
				3,申明使用：程序main入口，添加@EnableDiscoveryClient注解，开启服务治理与发现	  
					  ACM （应用配置管理）配置加载规则说明：
						参考文档：https://www.alibabacloud.com/help/zh/doc-detail/94708.htm?spm=a2c63.p38356.b99.56.547b66ae7aDsVW
							# Nacos Spring Cloud 中，dataId 的完整格式如下：
							${prefix}-${spring.profile.active}.${file-extension}
							prefix 默认为 spring.application.name 的值，也可以通过配置项 spring.cloud.nacos.config.prefix来配置。
							spring.profile.active 即为当前环境对应的 profile，详情可以参考 Spring Boot文档。
							注意：当 spring.profile.active 为空时，对应的连接符 - 也将不存在，dataId 的拼接格式变成 ${prefix}.${file-extension}
							file-exetension 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型。
							# @RefreshScope 实现配置自动更新
							
							采用默认值的应用要加载的配置规则就是：
							Data ID=${spring.application.name}.properties，Group=DEFAULT_GROUP。
	2、服务调用：
		 -Feign：声明式服务调用，与Netflix Feign 功能相似
				使用-客户端：
				1,引用模块：POM.XML	
				<!--openfeign依赖-->
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					  <artifactId>spring-cloud-starter-openfeign</artifactId>
				</dependency>
				<!--Nacos的服务注册与发现模块-->
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
				</dependency>
				<!--统一管理-->
				<dependencyManagement>
					<dependencies>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-dependencies</artifactId>
								<version>Greenwich.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>0.2.2.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
					</dependencies>
				</dependencyManagement>

				2,配置参数：创建bootstrap.yml或创建bootstrap.properties文件
				格式：
					spring:
					  application:
						name: nacos-discovery-consumer-feign # 客户端名称
					  cloud:
						nacos:
						  config:
							server-addr: 192.168.43.142:8848 # nacos服务端
					server:
					  port: 9091
				3、申明使用：程序main入口，添加@EnableFeignClients注解，开启服务调用
						具体使用：定义一个接口
						@FeignClient("nacos-discovery-provider") //调用的服务名称
						public interface TestService {
							@GetMapping("/m1")
							String m1(@RequestParam(name = "name") String name);
						}

	3、其他：
		 -Webflux：替换了旧的Servlet线程模型。
		 - Spring Cloud Gateway ：网关配置，目标是替代Netflix ZUUL，其不仅提供统一的路由方式，并且基于Filter链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。
				使用-客户端：
				1,引用模块：POM.XML	
				<!--gateway依赖-->
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					  <artifactId>spring-cloud-starter-gateway</artifactId>
				</dependency>
				<!--Nacos的服务注册与发现模块-->
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
				</dependency>
				<!--统一管理-->
				<dependencyManagement>
					<dependencies>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-dependencies</artifactId>
								<version>Greenwich.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>0.2.2.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
					</dependencies>
				</dependencyManagement>

				2,配置参数：创建bootstrap.yml或创建bootstrap.properties文件
				格式：
					spring:
						  application:
							name: nacos-discovery-gateway-server
						  cloud:
							nacos:
							  discovery:
								server-addr: 192.168.43.142:8848
								metadata:
								  name: healthy
							gateway:
							  routes:
								- id: nacos-discovery-provider
								  uri: lb://nacos-discovery-provider
								  predicates:
									- Path=/provider/**
								  filters:
									- StripPrefix=1
							  
							  discovery:
								locator:
								  enabled: true  #表明gateway开启服务注册和发现的功能，并且spring cloud gateway自动根据服务发现为每一个服务创建了一个router，这个router将以服务名开头的请求路径转发到对应的服务。
								  lowerCaseServiceId: true   #是将请求路径上的服务名配置为小写（因为服务注册的时候，向注册中心注册时将服务名转成大写的了），比如以/service-hi/*的请求路径被路由转发到服务名为service-hi的服务上。
								  filters:
									- StripPrefix=1
									
						server:
						  port: 9093

				3、申明使用：程序main入口，添加@EnableDiscoveryClient注解，开启服务调用
						使用示例：http://localhost:9093/provider/hello?name=zhansan
		 -Sentinel：
				介绍：随着微服务的流行，服务和服务之间的稳定性变得越来越重要。Sentinel 作为流量防卫组件，以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。
				特征：
					 1、丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等
					 2、完备的实时监控
					 3、广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合
					 4、完善的 SPI（串行外设接口） 扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。
				组成：
					 核心库（Java 客户端）
					 控制台（Dashboard）		
					 使用：		
					 	 1、部署Sentinel Dashboard	
							下载地址：https://github.com/alibaba/Sentinel/releases
							启动(默认端口：8080)：
							java -jar sentinel-dashboard-1.6.0.jar
							java -jar -Dserver.port=8888 sentinel-dashboard-1.6.0.jar
							默认用户名密码：sentinel
							
						 2、核心库的配置使用：
							1,引用模块：POM.XML	
							<!--gateway依赖-->
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								  <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
							</dependency>
							<!--Nacos存储扩展-->
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>sentinel-datasource-nacos</artifactId>
							</dependency>
							<!--统一管理-->
							<dependencyManagement>
								<dependencies>
										<dependency>
											<groupId>org.springframework.cloud</groupId>
											<artifactId>spring-cloud-dependencies</artifactId>
											<version>Greenwich.RELEASE</version>
											<type>pom</type>
											<scope>import</scope>
										</dependency>
										<dependency>
											<groupId>org.springframework.cloud</groupId>
											<artifactId>spring-cloud-alibaba-dependencies</artifactId>
											<version>0.2.2.RELEASE</version>
											<type>pom</type>
											<scope>import</scope>
										</dependency>
								</dependencies>
							</dependencyManagement>

							2,配置参数：创建bootstrap.yml或创建bootstrap.properties文件
							格式：
								spring:
								  application:
									name: nacos-discovery-sentinel
								  cloud:
									sentinel:
									  transport:
										dashboard: localhost:8888 #Sentinel Dashboard服务端地址 
									  datasource:
										ds:
										  nacos:
											server-addr: localhost:8848 #nacos服务地址
											dataId: ${spring.application.name}
											groupId: DEFAULT_GROUP
											ruleType: flow
								server:
								  port: 9095

							3、申明使用：访问sentinel服务列表并限流；通过nacos配置流控规则；
							   # 注意
								 Sentinel控制台中修改规则：仅存在于服务的内存中，不会修改Nacos中的配置值，重启后恢复原来的值。
								 Nacos控制台中修改规则：服务的内存中规则会更新，Nacos中持久化规则也会更新，重启后依然保持。
							4、优点:
									1、sentinel配置变动后通知非常的迅速, 秒杀springcloud原来的config几条街,
									   毕竟原来的config是基于git, 不提供可视化界面, 动态变更还需要依赖bus来通过所有的客户端变化
									2、与hystrix相比，sentinel更加的轻量级,并且支持动态的限流调整,更加友好的界面ui
									
----------------------------------------------Spring boot 技术的使用---------------------------------------------------------	

	一、基本概念：
			特点：开箱即用，自动配置
			配置文件：YAML 的基本使用-支持文档块
			配置文件值的注入：
				方式一：@ConfigurationProperties
					格式：
				javaBean 
					@Component
					@ConfigurationProperties(prefix = "person")
					//@Validated
					public class Person {
						//@Value("${person.last-name}")
						private String lastName;
						//@Value("#{11*2}")
						private Integer age;
						private Boolean boss;
						private Date birth;

						private Map<String,Object> maps;
						private List<Object> lists;
						private Dog dog;
						...
					}
				配置文件：
					person:
						lastName: hello
						age: 18
						boss: false
						birth: 2017/12/12
						maps: {k1: v1,k2: 12}
						lists:
						  - lisi
						  - zhaoliu
						dog:
						  name: 小狗
						  age: 12
						  
				方式二：@Value
				区别：
					|            	| @ConfigurationProperties | @Value 	 |
					| 功能         	| 批量注入配置文件中的属性 | 一个个指定  |
					| 松散绑定（松散语法）| 支持               | 不支持    	 |
					| SpEL       	| 不支持                      | 支持     |
					| JSR303数据校验| 支持                       | 不支持    |
					| 复杂类型封装  | 支持                       | 不支持    |
			配置文件加载：@PropertySource
				格式：
					@PropertySource(value = {"classpath:person.properties"})
					@Component
					@ConfigurationProperties(prefix = "person")
					public class Person {
						//@Value("#{11*2}")
						private Integer age;
						//@Value("true")
						private Boolean boss;

					```
					}
			配置文件读取：@ImportResource
				格式：
					java文件
						@ImportResource(locations = {"classpath:beans.xml"})
					xml文件
					<?xml version="1.0" encoding="UTF-8"?>
					<beans xmlns="http://www.springframework.org/schema/beans"
						   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
						   xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd">


						<bean id="helloService" class="com.atguigu.springboot.service.HelloService"></bean>
					</beans>	
			配置文件添加：	
				推荐使用全注解的方式
				1、配置类@Configuration------>Spring配置文件
				2、使用@Bean给容器中添加组件

				```java
				@Configuration
				public class MyAppConfig {

					//将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名
					@Bean
					public HelloService helloService(){
						System.out.println("配置类@Bean给容器中添加组件了...");
						return new HelloService();
					}
				}
			配置文件占位符:
				如：*.properties 配置文件中
					person.last-name=张三${random.uuid}
					person.dog.name=${person.hello:hello}_dog
					
			配置文件Profile:		
				激活指定profile的方式：
				1、配置文件中指定  spring.profiles.active=dev	
				2、命令行方式 java -jar **-0.0.1-SNAPSHOT.jar --spring.profiles.active=dev；
				3、虚拟机参数 -Dspring.profiles.active=dev
			配置文件加载位置：
				springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件
				–file:./config/
				–file:./
				–classpath:/config/
				–classpath:/
				优先级由高到底，高优先级的配置会覆盖低优先级的配置；
				SpringBoot会从这四个位置全部加载主配置文件；互补配置；
				-指定加载位置：java -jar **-0.0.1-SNAPSHOT.jar --spring.config.location=G:/application.properties
			外部配置加载顺序：
				SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置
				1.命令行参数
					java -jar **-02-0.0.1-SNAPSHOT.jar --server.port=8087  --server.context-path=/abc
				多个配置用空格分开； --配置项=值
				2.来自java:comp/env的JNDI属性
				3.Java系统属性（System.getProperties()）
				4.操作系统环境变量
				5.RandomValuePropertySource配置的random.*属性值
				6.由jar包外向jar包内进行寻找，优先加载带profile
					jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件
					jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件
				10.@Configuration注解类上的@PropertySource
				11.通过SpringApplication.setDefaultProperties指定的默认属性
			自动配置原理：
				xxxxAutoConfigurartion：自动配置类；
				xxxxProperties:封装配置文件中相关属性；
			检测自动配置类是否生效：debug=true
	二、日志框架：
			默认：springBoot底层也是使用slf4j+logback的方式进行日志记录
			| 日志门面  （日志的抽象层）               | 日志实现                                     
			| ---------------------------------------- | ---------------------------------------- |
			| JCL（Jakarta  Commons Logging）          | Log4j  JUL（java.util.logging）  Log4j2  Logback	
			| SLF4j（Simple  Logging Facade for Java） jboss-logging|	
			系统统一日志（slf4j）输出：
				1、将系统中其他日志框架先排除出去
				2、用中间包来替换原有的日志框架
				3、我们导入slf4j其他的实现
	
			日志级别：
				由低到高   trace<debug<info<warn<error
			springBoot修改默认日志配置：
				1、指定级别：logging.level.XX包名=trace
				2、logging.path=  #不指定路径在当前项目下生成springboot.log日志
				   logging.path=/spring/log #在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；使用 spring.log 作为默认文件
				3、logging.file=G:/springboot.log
				输出格式：
				4、logging.pattern.console=%d{yyyy-MM-dd} [%thread] %-5level %logger{50} - %msg%n #控制台
				5、logging.pattern.file=%d{yyyy-MM-dd} [%thread] %-5level %logger{50} - %msg%n #指定文件中
			指定具体的日志实现配置：类路径下放上每个日志框架自己的配置文件即可
				如：logback.xml：直接就被日志框架识别了；
					logback-spring.xml：日志框架就不直接加载日志的配置项，由SpringBoot解析日志配置，可以使用SpringBoot的高级Profile功能
					
					xml文件：
					<appender name="stdout" class="ch.qos.logback.core.ConsoleAppender">
							<!--
							日志输出格式：
								%d表示日期时间，
								%thread表示线程名，
								%-5level：级别从左显示5个字符宽度
								%logger{50} 表示logger名字最长50个字符，否则按照句点分割。 
								%msg：日志消息，
								%n是换行符
							-->
							<layout class="ch.qos.logback.classic.PatternLayout">
								<springProfile name="dev">
									<pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} ----> [%thread] ---> %-5level %logger{50} - %msg%n</pattern>
								</springProfile>
								<springProfile name="!dev">
									<pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} $$$$$$ [%thread] $$$$$$ %-5level %logger{50} - %msg%n</pattern>
								</springProfile>
							</layout>
						</appender>	
	二、Web模块：				
			静态资源映射：
				1、（以Jar包的方式引入静态资源）/webjars/**，都去 classpath:/META-INF/resources/webjars/找资源；如：localhost:8080/webjars/jquery/3.3.1/jquery.js
					引入如：
						<dependency>
							<groupId>org.webjars</groupId>
							<artifactId>jquery</artifactId>
							<version>3.3.1</version>
						</dependency>
				2、	/**，访问任何资源
					静态资源的文件夹：
					"classpath:/META-INF/resources/", 
					"classpath:/resources/",
					"classpath:/static/", 
					"classpath:/public/" 
					"/"：当前项目的根路径	
				3、	欢迎页：静态资源文件夹下的所有index.html页面；被"/**"映射；
				4、 所有的 **/favicon.ico  都是在静态资源文件下找	
			模板引擎：JSP、Velocity、Freemarker、Thymeleaf，SpringBoot推荐的Thymeleaf
				Thymeleaf使用：
					1、引入：POM.xml
						<dependency>
							<groupId>org.springframework.boot</groupId>
							<artifactId>spring-boot-starter-thymeleaf</artifactId>
							2.1.6
						</dependency>
				<properties>
						<thymeleaf.version>3.0.9.RELEASE</thymeleaf.version>
						<!-- 布局功能的支持程序  thymeleaf3主程序  layout2以上版本 -->
						<!-- thymeleaf2   layout1-->
						<thymeleaf-layout-dialect.version>2.2.2</thymeleaf-layout-dialect.version>
				  </properties>
					2、使用：把HTML页面放在classpath:/templates/，thymeleaf就能自动渲染；
						页面引入：<html lang="en" xmlns:th="http://www.thymeleaf.org">
						页面使用标签：
							语法规则：
								包含：th:insert th:replace th:include
								遍历：th:each
								条件判断：th:if th:unless th:switch th:case
								申明变量：th:object th:with
								属性修改：th:attr th:attrprepend th:attrapend
								属性值修改: th:value th:src th:href
								标签体内容：th:text（转义） th:utext（不转义）
								申明片段：th:fragment 
								移除：th:remove
							表达式：
								${...}：获取变量值，符合OGNL（Object Graphic Navigation Language(对象图导航语言)）标准；
									1）、获取对象的属性、调用方法
									2）、使用内置的基本对象：
										#ctx : the context object.
										#vars: the context variables.
										#locale : the context locale.
										#request : (only in Web Contexts) the HttpServletRequest object.
										#response : (only in Web Contexts) the HttpServletResponse object.
										#session : (only in Web Contexts) the HttpSession object.
										#servletContext : (only in Web Contexts) the ServletContext object.
									3）、内置的一些工具对象：
									#execInfo : information about the template being processed.
									#messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #{…} syntax.
									#uris : methods for escaping parts of URLs/URIs
									#conversions : methods for executing the configured conversion service (if any).
									#dates : methods for java.util.Date objects: formatting, component extraction, etc.
									#calendars : analogous to #dates , but for java.util.Calendar objects.
									#numbers : methods for formatting numeric objects.
									#strings : methods for String objects: contains, startsWith, prepending/appending, etc.
									#objects : methods for objects in general.
									#bools : methods for boolean evaluation.
									#arrays : methods for arrays.
									#lists : methods for lists.
									#sets : methods for sets.
									#maps : methods for maps.
									#aggregates : methods for creating aggregates on arrays or collections.
									#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration).
										Selection Variable Expressions: *{...}：选择表达式：和${}在功能上是一样；补充：配合 th:object="${session.user}：	
									    <div th:object="${session.user}">
										<p>Name: <span th:text="*{firstName}">Sebastian</span>.</p>
										<p>Surname: <span th:text="*{lastName}">Pepper</span>.</p>
										<p>Nationality: <span th:text="*{nationality}">Saturn</span>.</p>
										</div>
										Message Expressions: #{...}：获取国际化内容
										Link URL Expressions: @{...}：定义URL；@{/order/process(execId=${execId},execType='FAST')}		
										Fragment Expressions: ~{...}：片段引用表达式;<div th:insert="~{commons :: main}">...</div>				
									Literals（字面量）
										  Text literals: 'one text' , 'Another one!' ,…
										  Number literals: 0 , 34 , 3.0 , 12.3 ,…
										  Boolean literals: true , false
										  Null literal: null
										  Literal tokens: one , sometext , main ,…
									Text operations:（文本操作）
										String concatenation: +
										Literal substitutions: |The name is ${name}|
									Arithmetic operations:（数学运算）
										Binary operators: + , - , * , / , %
										Minus sign (unary operator): -
									Boolean operations:（布尔运算）
										Binary operators: and , or
										Boolean negation (unary operator): ! , not
									Comparisons and equality:（比较运算）
										Comparators: > , < , >= , <= ( gt , lt , ge , le )
										Equality operators: == , != ( eq , ne )
									Conditional operators:条件运算（三元运算符）
										If-then: (if) ? (then)
										If-then-else: (if) ? (then) : (else)
										Default: (value) ?: (defaultvalue)
									Special tokens:
										No-Operation: _ 
				禁用Thymeleaf缓存：spring.thymeleaf.cache=false 
			SpringMVC自动配置：
					SpringBoot对SpringMVC的默认配置，WebMvcAutoConfiguration
					做的主要几件事：
							1、自动配置了ViewResolver（视图解析器）
							2、支持静态资源文件夹路径,webjars
							3、支持静态首页访问
							4、支持favicon.ico系统图标
							5、自动注册了Converte转换器Formatter格式化器
			扩展SpringMVC：
					Bean.XML 参考：
						 <mvc:view-controller path="/hello" view-name="success"/>
							<mvc:interceptors>
								<mvc:interceptor>
									<mvc:mapping path="/hello"/>
									<bean></bean>
								</mvc:interceptor>
							</mvc:interceptors>
					编写一个配置类（@Configuration），是WebMvcConfigurerAdapter类型；不能标注@EnableWebMvc（会全面接管），既保留了所有的自动配置，也能用自定义扩展的配置；
					如：@Configuration
						//@EnableWebMvc
						public class MyMvcConfig extends WebMvcConfigurerAdapter {
							@Override
							public void addViewControllers(ViewControllerRegistry registry) {
							   // super.addViewControllers(registry);
								registry.addViewController("/atguigu").setViewName("success");
							}
						}
			全面接管SpringMVC：需要在自定义的配置类中添加@EnableWebMvc即可		
					原理：底层有@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)
						WebMvcConfigurationSupport只是保留SpringMVC最基本的功能
			国际化：
				使用步骤：	1）、编写国际化配置文件
							2）、使用ResourceBundleMessageSource管理国际化资源文件
							3）、在页面使用fmt:message取出国际化内容
				结论：根据浏览器语言设置的信息切换了国际化
				原理：国际化Locale（区域信息对象）；LocaleResolver（获取区域信息对象）；	
						@Bean
						@ConditionalOnMissingBean
						@ConditionalOnProperty(prefix = "spring.mvc", name = "locale")
						public LocaleResolver localeResolver() {
							if (this.mvcProperties
									.getLocaleResolver() == WebMvcProperties.LocaleResolver.FIXED) {
								return new FixedLocaleResolver(this.mvcProperties.getLocale());
							}
							//默认的就是根据请求头带来的区域信息获取Locale进行国际化
							AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver();
							localeResolver.setDefaultLocale(this.mvcProperties.getLocale());
							return localeResolver;
						}
				点击链接切换国际化: 自定义个类实现LocaleResolver重写resolveLocale方法（对请求参数进行处理）返回区域对象，并注入容器里
						参考：
						public class MyLocaleResolver implements LocaleResolver {
							@Override
							public Locale resolveLocale(HttpServletRequest request) {
								String l = request.getParameter("l");
								Locale locale = Locale.getDefault();
								if(!StringUtils.isEmpty(l)){
									String[] split = l.split("_");
									locale = new Locale(split[0],split[1]);
								}
								return locale;
							}

							@Override
							public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) {

							}
						}
						 @Bean
							public LocaleResolver localeResolver(){
								return new MyLocaleResolver();
							}
						}
			Rest风格:
					URI：/资源名称/资源标识（主表Id），以HTTP请求方式（GET、POST、PUT、DELETE）区分对资源CRUD操作；
					注意要点：如果添加/修改页面（二合一版），页面创建一个input项，name="_method";值就是我们指定的请求方式，原理：SpringMVC中配置HiddenHttpMethodFilter;（SpringBoot自动配置好的）
			模板抽取：
					1、抽取公共片段
					<div th:fragment="copy">
						&copy; 2011 The Good Thymes Virtual Grocery
					</div>
					2、引入公共片段
					<div th:insert="~{footer :: copy}"></div> // ~{templatename::selector}：模板名::选择器;~{templatename::fragmentname}:模板名::片段名									
					3、默认效果：
					如果使用th:insert等属性进行引入，可以不用写~{}：行内写法可以加上：[[~{}]];[(~{})]；
					三种引入公共片段的th属性：
					th:insert：将公共片段整个插入到声明引入的元素中
					th:replace：将声明引入的元素替换为公共片段
					th:include：将被引入的片段的内容包含进这个标签中
					如：
						<footer th:fragment="copy">
							&copy; 2011 The Good Thymes Virtual Grocery
						</footer>
						引入方式
						<div th:insert="footer :: copy"></div>
						<div th:replace="footer :: copy"></div>
						<div th:include="footer :: copy"></div>
						效果
						<div>
							<footer>
							&copy; 2011 The Good Thymes Virtual Grocery
							</footer>
						</div>
						
						<footer>
							&copy; 2011 The Good Thymes Virtual Grocery
						</footer>
						
						<div>
							&copy; 2011 The Good Thymes Virtual Grocery
						</div>
			错误处理：
				效果：1、浏览器：返回一个默认的错误页面；2、其他客户端，默认响应一个json数据
				原理：ErrorMvcAutoConfiguration；错误处理的自动配置类
					一但系统出现4xx或者5xx之类的错误；
					1、ErrorPageCustomizer就会生效（定制错误的响应规则）；就会来到/error请求；
					2、就会来到BasicErrorController处理（针对浏览器与其他客户端分别处理）；
					3、浏览器：调用DefaultErrorViewResolver类，交给它处理（有模板引擎，按模板引擎处理；没有默认处理error/状态码.html）
					4、其他客户端：调用getErrorAttributes方法，返回封装的Json数据；
				定制处理处理：
					基本规则：1、有模板引擎：模板引擎文件夹里面的error文件夹下找状态码.HTML（支持精准匹配与模糊匹配如：5xx.html）
								包含信息：
									timestamp：时间戳
					​				status：状态码
					​				error：错误提示
					​				exception：异常对象
					​				message：异常消息
					​				errors：JSR303数据校验的错误
							  2、没有模板引擎：静态资源文件夹
							  3、默认：来到SpringBoot默认的错误提示页面
				定制错误的JSON返回数据（思路）：
					自定义异常处理&返回定制json数据	
					转发到/error进行自适应响应效果处理
					将定制数据携带出去
			配置嵌入式Servlet容器：
				结论：SpringBoot默认使用Tomcat作为嵌入式的Servlet容器
				定制：
					1、ServerProperties方式，在配置文件中配置
					//通用的Servlet容器设置，server.xxx
					  如：server.port=8081 server.context-path=/crud
					//Tomcat的设置，server.tomcat.xxx
					  如：server.tomcat.uri-encoding=UTF-8
					2、EmbeddedServletContainerCustomizer方式，在java代码中配置
					  如：
						@Bean  //一定要将这个定制器加入到容器中
						public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer(){
							return new EmbeddedServletContainerCustomizer() {
								//定制嵌入式的Servlet容器相关的规则
								@Override
								public void customize(ConfigurableEmbeddedServletContainer container) {
									container.setPort(8083);
								}
							};
						}
				注册Servlet三大组件：Servlet、Filter、Listener 替代在web.xml中的配置	
				自动的注册前端控制器：DispatcherServletAutoConfiguration	
				修改配置：server.servletPath修改默认拦截的请求路径		
				修改为其他的servlet容器：
					支持Tomcat Undertow Jetty
					如：
						<!-- 引入web模块 -->
						<dependency>
						   <groupId>org.springframework.boot</groupId>
						   <artifactId>spring-boot-starter-web</artifactId>
						   <exclusions>
							  <exclusion>
								 <artifactId>spring-boot-starter-tomcat</artifactId>
								 <groupId>org.springframework.boot</groupId>
							  </exclusion>
						   </exclusions>
						</dependency>
						<!--引入其他的Servlet容器-->
						<dependency>
						   <artifactId>spring-boot-starter-undertow</artifactId>
						   <groupId>org.springframework.boot</groupId>
						</dependency>
				各自区别：
					Tomcat 是Apache下的一款重量级的基于HTTP协议的服务器
					Undertow 基于NIO（非阻塞式输入输出，相对于BIO（Blocking I/O，阻塞IO））实现的高并发轻量级的服务器 支持JSP
					Jetty 基于NIO（非阻塞式输入输出，相对于BIO（Blocking I/O，阻塞IO））实现的高并发轻量级的服务器 支持长链接
					Netty是一款基于NIO（Nonblocking I/O，非阻塞IO）开发的网络通信框架，客户端服务器框架
				嵌入式Servlet容器自动配置原理：EmbeddedServletContainerAutoConfiguration
					步骤：
					1）、SpringBoot根据导入的依赖情况，给容器中添加相应的EmbeddedServletContainerFactory
					2）、容器中某个组件要创建对象就会惊动后置处理器；EmbeddedServletContainerCustomizerBeanPostProcessor；只要是嵌入式的Servlet容器工厂，后置处理器就工作；
					3）、后置处理器，从容器中获取所有的EmbeddedServletContainerCustomizer，调用定制器的定制方法
				启动原理（步骤）：
					springBoot应用启动运行run方法->refreshContext(context);SpringBoot创建并刷新IOC容器(如果是web应用创建AnnotationConfigEmbeddedWebApplicationContext，否则：AnnotationConfigApplicationContext)
					->refresh(context)刷新IOC容器->onRefresh()->webIoc容器会创建嵌入式的Servlet容器；createEmbeddedServletContainer()
					->获取嵌入式的Servlet容器工厂EmbeddedServletContainerFactory->后置处理器EmbeddedServletContainerCustomizerBeanPostProcessor工作->
					->定制器来先定制Servlet容器的相关配置->嵌入式的Servlet容器创建对象并启动Servlet容器
				结论：IOC容器启动创建嵌入式的Servlet容器并启动
			外置的Servlet容器：
				嵌入式Servlet容器将应用打成可执行的jar，优点：简单、便携；缺点：默认不支持JSP、优化定制比较复杂；
				外置的Servlet容器一般指外面安装Tomcat---应用war包的方式打包；
				使用步骤：
					1）、必须创建一个war项目；（利用idea创建好目录结构）
					2）、将嵌入式的Tomcat指定为provided；
					<dependency>
					   <groupId>org.springframework.boot</groupId>
					   <artifactId>spring-boot-starter-tomcat</artifactId>
					   <scope>provided</scope>
					</dependency>
					3）、必须编写一个SpringBootServletInitializer的子类，并调用configure方法
					public class ServletInitializer extends SpringBootServletInitializer {
					   @Override
					   protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {
						   //传入SpringBoot应用的主程序
						  return application.sources(SpringBoot04WebJspApplication.class);
					   }

					}
					4）、启动服务器就可以使用；
				原理：
					jar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器；
					war包：启动服务器，服务器启动SpringBoot应用【SpringBootServletInitializer】，启动ioc容器；
	三、springboot与docker：
			详情参考docker技术；
			Docker是一个开源的应用容器引擎；是一个轻量级容器技术；
				
	四、springboot与数据访问：
			链接JDBC:
				1、导入依赖：
					<dependency>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-starter-jdbc</artifactId>
					</dependency>
					<dependency>
						<groupId>mysql</groupId>
						<artifactId>mysql-connector-java</artifactId>
						<scope>runtime</scope>
					</dependency>
				2、添加配置参数：
					spring:
					  datasource:
						username: root
						password: 123456
						url: jdbc:mysql://192.168.15.22:3306/jdbc
						driver-class-name: com.mysql.jdbc.Driver
						schema:
						- classpath:department.sql #指定位置
				3、申明使用
			相关结论：
				默认是用org.apache.tomcat.jdbc.pool.DataSource作为数据源，数据源的相关配置都在DataSourceProperties里面
				DataSourceInitializer：ApplicationListener 可以运行建表与运行数据插入（runSchemaScripts();运行建表语句；runDataScripts();运行插入数据的sql语句；）
				默认只需要将文件重命名为：schema-*.sql、data-*.sql 默认规则：schema.sql，schema-all.sql；
					可以使用   
					schema:
						- classpath:department.sql #指定位置
				自动配置了JdbcTemplate操作数据库；
			整合：Druid数据源，参考：https://blog.csdn.net/weixin_41404773/article/details/82592719
				1、导入依赖：
					<dependency>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-starter-jdbc</artifactId>
					</dependency>
					<!--引入druid-->
					<!-- https://mvnrepository.com/artifact/com.alibaba/druid -->
					<dependency>
						<groupId>com.alibaba</groupId>
						<artifactId>druid</artifactId>
						<version>1.1.8</version>
					</dependency>
				2、添加配置参数：在aplication.yml或aplication.properties
					spring:
					  datasource:
						username: root
						password: 123456
						url: jdbc:mysql://localhost:3306/testwkn
						driver-class-name: com.mysql.jdbc.Driver
						type: com.alibaba.druid.pool.DruidDataSource
					 
						initialSize: 5
						minIdle: 5
						maxActive: 20
						maxWait: 60000
						timeBetweenEvictionRunsMillis: 60000
						minEvictableIdleTimeMillis: 300000
						validationQuery: SELECT 1 FROM DUAL
						testWhileIdle: true
						testOnBorrow: false
						testOnReturn: false
						poolPreparedStatements: true
					#   配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
						filters: stat,wall,log4j
						maxPoolPreparedStatementPerConnectionSize: 20
						useGlobalDataSourceStat: true
						connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500
					#    schema:
					#      - classpath:department.sql

				3、	读取配置
					@Configuration
					public class DruidConfig {
					 
						@ConfigurationProperties(prefix = "spring.datasource")
						@Bean
						public DataSource druid(){
						   return  new DruidDataSource();
						}
					 
						//配置Druid的监控
						//1、配置一个管理后台的Servlet
						@Bean
						public ServletRegistrationBean statViewServlet(){
							ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), "/druid/*");
							Map<String,String> initParams = new HashMap<>();
					 
							initParams.put("loginUsername","admin");
							initParams.put("loginPassword","123456");
							initParams.put("allow","");//默认就是允许所有访问
							initParams.put("deny","192.168.15.21");
					 
							bean.setInitParameters(initParams);
							return bean;
						}
					 
					 
						//2、配置一个web监控的filter
						@Bean
						public FilterRegistrationBean webStatFilter(){
							FilterRegistrationBean bean = new FilterRegistrationBean();
							bean.setFilter(new WebStatFilter());
					 
							Map<String,String> initParams = new HashMap<>();
							initParams.put("exclusions","*.js,*.css,/druid/*");
					 
							bean.setInitParameters(initParams);
					 
							bean.setUrlPatterns(Arrays.asList("/*"));
					 
							return  bean;
						}
					}
			整合Mybatis:
				1、导入依赖：
					<dependency>
						<groupId>org.mybatis.spring.boot</groupId>
						<artifactId>mybatis-spring-boot-starter</artifactId>
						<version>1.3.1</version>
					</dependency>
				2、申明使用：
					1、注解版-写好提供的接口，程序的入口添加@MapperScan
					2、配置文件版
						mybatis:
						  config-location: classpath:mybatis/mybatis-config.xml 指定全局配置文件的位置
						  mapper-locations: classpath:mybatis/mapper/*.xml  指定sql映射文件的位置
			整合Jpa（Java Persistence API，通过注解或者XML描述【对象-关系表】之间的映射关系，并将实体对象持久化到数据库中）:
				Jpa特点：ORM映射元数据：JPA支持XML和注解两种元数据的形式，元数据描述对象和表之间的映射关系，框架据此将实体对象持久化到数据库表中；如：@Entity、@Table、@Column、@Transient等注解
						 JPA 提供API：用来操作实体对象，执行CRUD操作，框架在后台替我们完成所有的事情，开发者从繁琐的JDBC和SQL代码中解脱出来；如：entityManager.merge(T t)；
						 JPQL查询语言：通过面向对象而非面向数据库的查询语言查询数据，避免程序的SQL语句紧密耦合；如：from Student s where s.name = ?
						 JPA仅仅是一种规范，也就是说JPA仅仅定义了一些接口，而接口是需要实现才能工作的。Hibernate就是实现了JPA接口的ORM框架。
				spirng data jpa：
					是spring提供的一套简化JPA开发的框架，按照约定好的【方法命名规则】写dao层接口，就可以在不写接口实现的情况下，实现对数据库的访问和操作。同时提供了很多除了CRUD之外的功能，如分页、排序、复杂查询等等。		 
					Spring Data JPA 可以理解为 JPA 规范的再次封装抽象，底层还是使用了 Hibernate 的 JPA 技术实现。
					接口约定命名规则：如：findByNameAndPwd xxAndxx
					使用：
						1、导入依赖：
							<dependency>
								<groupId>org.springframework.boot/groupId>
								<artifactId>spring-boot-starter-data-jpa</artifactId>
							</dependency>
						2、添加配置：
							spring:  
								jpa:
									hibernate:
								# 更新或者创建数据表结构
										ddl-auto: update
								#控制台显示SQL
									show-sql: true
									database: mysql
						3、使用：
							编写一个实体类（bean）和数据表进行映射，并且配置好映射关系；
							编写一个Dao接口来操作实体类对应的数据表（Repository）；
							如：
								//使用JPA注解配置映射关系
								@Entity //告诉JPA这是一个实体类（和数据表映射的类）
								@Table(name = "tbl_user") //@Table来指定和哪个数据表对应;如果省略默认表名就是user；
								public class User {

									@Id //这是一个主键
									@GeneratedValue(strategy = GenerationType.IDENTITY)//自增主键
									private Integer id;

									@Column(name = "last_name",length = 50) //这是和数据表对应的一个列
									private String lastName;
									@Column //省略默认列名就是属性名
									private String email;
									...
								}
								
								public interface UserRepository extends JpaRepository<User,Integer> {
								}
			Springboot启动配置：
				构造过程（initialize(sources)）
					ApplicationContextInitializer，应用程序初始化器，做一些初始化的工作，	
					ApplicationListener，应用程序事件(ApplicationEvent)监听器，
					默认情况下，initialize方法从spring.factories文件中找出对应的key为ApplicationContextInitializer的类与ApplicationListener的类；
				SpringApplication执行
					SpringApplication构造完成之后调用run方法，启动SpringApplication，run方法执行的时候会做以下几件事
					构造Spring容器、刷新Spring容器、从Spring容器中找出ApplicationRunner和CommandLineRunner接口的实现类并排序后依次执行！
					
	五、Springboot与缓存：
			JSR107标准：
				Java Caching定义了5个核心接口，分别是CachingProvider, CacheManager, Cache, Entry和Expiry
					CachingProvider定义了创建、配置、获取、管理和控制多个CacheManager。
					CacheManager定义了创建、配置、获取、管理和控制多个唯一命名的Cache。
					Cache是一个类似Map的数据结构并临时存储以Key为索引的值。
					Entry是一个存储在Cache中的key-value对。Expiry 每一个存储在Cache中的条目有一个定义的有效期。
			使用：
				<dependency>
					<groupId>javax.cache</groupId>
					<artifactId>cache-api</artifactId>
				</dependency>
			Spring缓存抽象：
				简介：Spring从3.1开始定义了org.springframework.cache.Cache和org.springframework.cache.CacheManager接口来统一不同的缓存技术；
					并支持使用JCache（JSR-107）注解简化开发；
				特点：
					Cache接口为缓存的组件规范定义，包含缓存的各种操作集合；
					Cache接口下Spring提供了各种xxxCache的实现；如RedisCache，EhCacheCache , ConcurrentMapCache等；
					每次调用需要缓存功能的方法时，Spring会检查检查指定参数的指定的目标方法是否已经被调用过；如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户。下次调用直接从缓存中获取。
					使用Spring缓存抽象时我们需要关注以下两点； 
						确定方法需要被缓存以及他们的缓存策略
						从缓存中读取之前缓存存储的数据
				注解：
					@Cacheable - 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存
					@CacheEvict - 清空缓存（删除）
					@CachePut - 保证方法被调用，又希望结果被缓存（更新）
					@EnableCaching	- 开启基于注解的缓存
					keyGenerator - 缓存数据时key生成策略
					serialize - 缓存数据时value序列化策略
				主要参数：
					cacheNames - 缓存的名称 如：@Cacheable(cacheNames={"emp"})
					value - 缓存的名称 如：@Cacheable(value=”cache0”) 或者 @Cacheable(value={”cache1”,”cache2”}
					key - 缓存的key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合，如：@Cacheable(value=”cache0”,key=”#userName”)
					condition - 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存/清除缓存，在调用方法之前之后都能判断，如：@Cacheable(value=”cache0”,condition=”#userName.length()>2”)
					allEntries -是否清空所有缓存内容，缺省为 false，如果指定为 true，则方法调用后将立即清空所有缓存，如：@CachEvict(value=”cache0”,allEntries=true)
					beforeInvocation - 是否在方法执行前就清空，缺省为 false，如果指定为 true，则在方法还没有执行的时候就清空缓存，缺省情况下，如果方法执行抛出异常，则不会清空缓存，如：@CachEvict(value=”cache01”，beforeInvocation=true)
					unless - 用于否决缓存的，条件为true不会缓存，fasle才缓存（与condition恰恰相反），如：@Cacheable(value=”cache01”,unless=”#result == null”)
				Cache SpEL available metadata：
					#root.methodName  #当前被调用的方法名
					#root.method.name #当前被调用的方法
					#root.target	  #当前被调用的目标对象
					#root.targetClass #当前被调用的目标对象类
					#root.args[0]     #当前被调用的方法的参数列表
					#root.caches[0].name #当前方法调用使用的缓存列表（如@Cacheable(value={"cache1", "cache2"})），则有两个cache
					#iban、 #a0、#p0 #方法参数的名字.可以直接#参数名，也可以使用#p0或#a0的形式，0代表参数的索引；
					#result           #方法执行后的返回值
				使用：
					•1、引入spring-boot-starter-cache模块
					•2、开启缓存：@EnableCaching
					•3、使用缓存注解
				
			Spring-data-redis：参考：https://blog.csdn.net/qq_26545305/article/details/80559902
				使用:
					1、引入依赖（2.0以后的版本）：
					<dependency>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-starter-data-redis</artifactId>
					</dependency>
					2、application.yml配置文件配置redis的相关信息
					spring:
					  redis:
						host: 192.168.224.225
						port: 6379
						password:
					3、配置redis整入spring的缓存框架
						@Configuration
						@EnableCaching  //继承CachingConfigurerSupport并重写方法，配合该注解实现spring缓存框架的使用
						public class RedisConfig extends CachingConfigurerSupport {
							/**载入配置文件配置的连接工厂**/
							@Autowired
							RedisConnectionFactory redisConnectionFactory;
							/*不提示警告信息*/
							@SuppressWarnings("rawtypes")
							@Autowired
							RedisTemplate redisTemplate;
						 
							@Bean
							RedisTemplate<String,Object> objectRedisTemplate(){
								RedisTemplate<String,Object> redisTemplate=new RedisTemplate<>();
								redisTemplate.setConnectionFactory(redisConnectionFactory);
								return redisTemplate;
							}
						 
							@Bean 
							@Override
							public CacheManager cacheManager(){
								RedisCacheManager redisCacheManager=new RedisCacheManager(redisTemplate);
								//设置缓存过期时间
						       // redisCacheManager.setDefaultExpiration(60);//秒
								return redisCacheManager;
							}
						 
							/**
							 * 重写缓存key生成策略，可根据自身业务需要进行自己的配置生成条件
							 * @return
							 */
							@Bean 
							@Override
							public KeyGenerator keyGenerator() {
								return new KeyGenerator() {
									@Override
									public Object generate(Object target, Method method, Object... params) {
										StringBuilder sb = new StringBuilder();
										sb.append(target.getClass().getName());
										sb.append(method.getName());
										for (Object obj : params) {
											sb.append(obj.toString());
										}
										return sb.toString();
									}
								};
							}
						 
						}
					
				操作: spring data redis中用来操作redis的一是采用注解的方式，常用两个注解@Cacheable、@CacheEvit，二是采用RedisTemplate的方式；
					  对应于redis的5中结构，RedisTemplate中定义了对应5种数据结构的操作
					  redisTemplate.opsForValue();//操作字符串
					  redisTemplate.opsForHash();//操作hash
					  redisTemplate.opsForList();//操作list
					  redisTemplate.opsForSet();//操作set
					  redisTemplate.opsForZSet();//操作有序Zset
			
			Ace-Cache：参考：https://gitee.com/geek_qi/ace-cache
				基于spring boot上的注解缓存，自带轻量级缓存管理页面。@Cache比spring cache更轻量的缓存，支持单个缓存设置过期时间，可以根据前缀移除缓存。采用fastjson序列化与反序列化，以json串存于缓存之中。
				使用：
					1、添加依赖：
						<dependency>
							<groupId>com.github.wxiaoqi</groupId>
							<artifactId>ace-cache</artifactId>
							<version>0.0.2</version>
						</dependency>
					2、配置文件中添加配置：
						redis:
							pool:
								 maxActive: 300
								 maxIdle: 100
								 maxWait: 1000
							host: 127.0.0.1
							port: 6379
							password:
							timeout: 2000
							# 服务或应用名
							sysName: ace
							enable: true
							database: 0
					3、程序入口开启：缓存开启@EnableAceCache
					具体使用：在Service上进行@Cache注解或@CacheClear注解；
						配置缓存：@Cache
								注解参数	类型	说明
								key	字符串	缓存表达式，动态运算出key
								expires	整形	缓存时长，单位：分钟
								desc	描述	缓存说明
								parser	Class<? extends ICacheResultParser>	缓存返回结果自定义处理类
								generator	Class<? extends IKeyGenerator>	缓存键值自定义生成类
						清除缓存：@CacheClear
								注解参数	类型	说明
									pre	字符串	清除某些前缀key缓存
									key	字符串	清除某个key缓存
									keys	字符串数组	清除某些前缀key缓存
									generator	Class<? extends IKeyGenerator>	缓存键值自定义生成类
					轻量管理端：访问地址：http://IP:PORT/cache					
						
	六、Springboot与消息：（异步通信、系统解耦、流量削峰填谷）
				消息的使用：利用消息中间件来提升系统异步通信、扩展解耦能力；
				基本概念：
					消息代理：（message broker）
					目的地：（destination）
					消息队列2种形式目的地
						1. 队列（queue） ：点对点消息通信（point-to-point）
						2. 主题（topic） ：发布（publish） /订阅（subscribe）消息通信
					点对点：
						消息发送者发送消息，消息代理将其放入一个队列中，消息接收者从队列中获取消息内容，消息读取后被移出队列，消息只有唯一的发送者和接受者，但并不是说只能有一个接收者
					发布订阅式：
						发送者（发布者）发送消息到主题，多个接收者（订阅者）监听（订阅）这个主题，那么就会在消息到达时同时收到消息
					JMS（Java Message Service） JAVA消息服务：基于JVM消息代理的规范。 ActiveMQ、 HornetMQ是JMS实现
					AMQP（Advanced Message Queuing Protocol）：高级消息队列协议，也是一个消息代理的规范，兼容JMS；RabbitMQ是AMQP的实现
				JMS与AMQP区别：
					AMQP网络线级协议，跨语言跨平台，支持5种消息模型；（1）direct exchange（2）、fanout exchange（3）、topic change（4）、headers exchange（5）、system exchange
				Springboot的支持：
					– spring-jms提供了对JMS的支持
					– spring-rabbit提供了对AMQP的支持
					– 需要ConnectionFactory的实现来连接消息代理
					– 提供JmsTemplate、 RabbitTemplate来发送消息
					– @JmsListener（JMS）、 @RabbitListener（AMQP）注解在方法上监听消息代理发布的消息
					– @EnableJms、 @EnableRabbit开启支持
					– JmsAutoConfiguration
					– RabbitAutoConfiguration
				RabbitMQ：
					简介：是一个由erlang开发的AMQP(Advanved Message Queue Protocol)的开源实现。
					核心概念：
						Message：消息，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组
								 成，这些属性包括routing-key（路由键）、 priority（相对于其他消息的优先权）、 delivery-mode（指出
								 该消息可能需要持久性存储）等。
						Publisher：消息的生产者，也是一个向交换器发布消息的客户端应用程序。
						Exchange：交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。
						Exchange有4种类型： direct(默认)，fanout, topic, 和headers，不同类型的Exchange转发消息的策略有所区别。							
						Queue：消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。
							   消息一直在队列里面，等待消费者连接到这个队列将其取走。
						Binding：绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连
								 接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange 和Queue的绑定可以是多对多的关系。
						Connection：网络连接，比如一个TCP连接。
						Channel：信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内的虚
								 拟连接， AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这
								 些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所
								 以引入了信道的概念，以复用一条 TCP 连接。
						Consumer：消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 
						Virtual Host：虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。
									  每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有									  
									  自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。
						Broker：表示消息队列服务器实体   
					运行机制：
						一句话概括：生产者将消息发送到Exchange（交换机），Exchange根据路由规则（routing-key）将消息Binding绑定到不同的消息队列，消费者
									从消息队列里取出消息
						Exchange的类型：Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：
										direct、 fanout、 topic、 headers 。 headers 匹配 AMQP 消息的 header而不是路由键，
										headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，
										所以直接看另外三种类型：
									1、Direct Exchange：完全匹配、单播模式（消息中的routing-key与消息队列中的binding key完全一致）
									2、Fanout（扇出） Exchange：广播模式（与消息中的routing-key无关的，交换器将消息转发到所有的消息队列里）
									3、Topic Exchange：模式匹配分发消息，将路由键与绑定键的字符用.隔开，支持识别通配符：符号“#”和符号 “*” 。
													   #匹配0个或多个单词，*匹配一个单词。
				与Springboot的整合：
					1、添加依赖：
						 <dependency>
							<groupId>org.springframework.boot</groupId>
							<artifactId>spring-boot-starter-amqp</artifactId>
						</dependency>
					2、配置文件中添加配置：
							#RabbitMQ相关的配置信息
							spring.rabbitmq.host=127.0.0.1
							spring.rabbitmq.port=5672
							spring.rabbitmq.username=guest
							spring.rabbitmq.password=guest
					3、申明使用：
						AmqpTemplate - 发送消息
						@RabbitListener(queues="") - 监听消息
						
		七、Springboot与检索ElasticSearch：
				简介：ElasticSearch是一个分布式搜索服务，提供Restful API，底层基于Apache Lucene，采用多shard（分片）的方式保证数据安全，并且提供自动resharding的功能，
					  github等大型的站点也是采用了ElasticSearch作为其搜索服务。
				特点：1、分布式实时文件存储，并将每一个字段都编入索引
					  2、实时分析的分布式搜索引擎
					  3、可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据（1PB=1024TB）
				与Mysql对比：
					关系数据库     ⇒ 数据库         ⇒ 表          ⇒ 行              ⇒ 列(Columns)
					Elasticsearch  ⇒ 索引(Index)    ⇒ 类型(type)  ⇒ 文档(Docments)  ⇒ 属性/字段(Fields)
				注意与MongoDB的区别：数据库         ⇒ 集合        ⇒ 文档            ⇒ 属性
				交互：与Elasticsearch的交互，可以使用Java API，也可以直接使用HTTP的Restful API方式
				核心：提供了强大的索引功能；
				与Springboot的整合：
					1、添加依赖：
						 <dependency>
							<groupId>org.springframework.boot</groupId>
							<artifactId>spring-boot-starter-data-elasticsearch</artifactId>
							<version>2.0.2.RELEASE</version>
						</dependency>
					2、配置文件中添加配置：	
						#elasticsearch集群名称，默认的是elasticsearch
						spring.data.elasticsearch.cluster-name=
						#节点的地址 注意api模式下端口号是9300，千万不要写成9200
						spring.data.elasticsearch.cluster-nodes=192.168.11.24:9300
						#是否开启本地存储
						spring.data.elasticsearch.repositories.enable=true
					3、申明使用：
						参考：https://blog.csdn.net/linzhiqiang0316/article/details/80343401	
						ElasticsearchRepository、 ElasticsearchTemplate、 JestClient
						
		八、Springboot与任务：
				异步：与第三方系统交互的时候，往往采用异步任务。在Spring 3.x之后，就已经内置了@Async来完美解决这个问题。使用@EnableAysnc、 @Aysnc即可；
				定时：Spring为我们提供了异步执行任务调度的方式，提供TaskExecutor、TaskScheduler接口。@EnableScheduling（开启）、@Scheduled（定时）
					  如：@Scheduled(fixedRate = 1000)//fixedRate函数每隔1S执行一次，也可以使用@Scheduled(cron="...")
						  public void method1(){}
						  @EnableScheduling
						  public class DemoApplication {}
					  cron表达式：
						秒（0-59） 分（0-59） 小时（0-23） 日期或天（1-31） 月份（1-12） 星期（0-7或SUN-SAT 0,7是SUN）
						特殊字符的含义：,（枚举）-（区间）*（任意）/（步长）？（日/星期冲突匹配）L（最后）W（工作日）C（和calendar联系后计算过的值）#（星期，4#2，第2个星期四）
						举例："0 0 12 * * ?" 每天中午12点触发
		
		九、Springboot与邮件任务：				
				参考：https://www.cnblogs.com/zhangyinhua/p/9277684.html
				自动装配类JavaMailSender.send();//发送邮件
		十、Springboot与安全：
				简介： Security是针对Spring项目的安全框架，也是Spring Boot底层安全模块默认的技术选型。
					   它可以实现强大的web安全控制。对于安全控制，仅需引入spring-boot-starter-security模块，进行少量的配置，即可实现强大的安全管理。
				使用：参考：https://www.cnblogs.com/ealenxie/p/9293768.html
				    重要的类：
					   WebSecurityConfigurerAdapter：自定义Security策略
					   AuthenticationManagerBuilder：自定义认证策略
					   @EnableWebSecurity：开启WebSecurity模式
					核心概念：
					   “认证”（Authentication）和“授权”（Authorization或者访问控制）两大目标
					web/http与安全：
						1. 登陆/注销
						– HttpSecurity配置登陆、注销功能
						2. Thymeleaf提供的SpringSecurity标签支持，需要引入thymeleaf-extras-springsecurity4						
						– sec:authentication=“” 获得当前用户的用户名
						– sec:authorize=“hasRole(‘ADMIN’)” 当前用户必须拥有ADMIN权限时才会显示标签内容
						3. remember me
						– 表单添加remember-me的checkbox
						– 配置启用remember-me功能
						4. CSRF（Cross-site request forgery）跨站请求伪造
						– HttpSecurity启用csrf功能，会为表单添加_csrf的值，提交携带来预防CSRF；
					
					
					与shiro的区别：
						Shiro是Apache 的Java的一个安全框架。
						Shiro的特点 易于理解的 Java Security API；
									简单的身份认证（登录），支持多种数据源；
									对角色的简单的签权（访问控制），支持细粒度的签权；
									支持一级缓存，以提升应用程序的性能；
									内置的基于POJO企业会话管理，适用于Web以及Web的环境；
									非常简单的加密 API；
									异构客户端会话访问；
									不跟任何的框架或者容器捆绑，可以独立运行；
									Authentication：身份认证/登录；
									Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情，常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限；
									Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通JavaSE环境的，也可以是如Web环境的；
									Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储；
									Web Support：Web支持，可以非常容易的集成到Web环境；
									Caching：缓存，比如用户登录后，其用户信息、拥有的角色/权限不必每次去查，这样可以提高效率；
									Concurrency：shiro支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去；
									Testing：提供测试支持；
									Run As：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问；
									Remember Me：记住我，即一次登录后，下次再来的话不用登录了
						
						Shiro四大核心功能：Authentication,Authorization,Cryptography,Session Management
						Shiro三个核心组件：Subject, SecurityManager 和 Realms.
							Subject：主体，代表了当前“用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是Subject，如网络爬虫，机器人等；
									 即一个抽象概念；所有Subject都绑定到SecurityManager，与Subject的所有交互都会委托给SecurityManager；可以把Subject认为是一个门面；SecurityManager才是实际的执行者；
							SecurityManager：安全管理器；即所有与安全有关的操作都会与SecurityManager交互；且它管理着所有Subject；
											 可以看出它是Shiro的核心，它负责与后边介绍的其他组件进行交互，如果学习过SpringMVC，你可以把它看成DispatcherServlet前端控制器；
							Realms：域，Shiro从从Realm获取安全数据（如用户、角色、权限），就是说SecurityManager要验证用户身份，那么它需要从Realm获取相应的用户进行比较以确定用户身份是否合法；
									也需要从Realm得到用户相应的角色/权限进行验证用户是否能进行操作；可以把Realm看成DataSource，即安全数据源。
						
						Security：基于Spring的企业应用系统提供声明式的安全访问控制解决方案的安全框架。
							      在Spring应用上下文中配置的Bean，充分利用了Spring IoC，DI（IoC：Inversion of Control控制反转,DI:Dependency Injection依赖注入）和AOP（面向切面编程）功能，
								  为应用系统提供声明式的安全访问控制功能，减少了为企业系统安全控制编写大量重复代码的工作。
								  Web/Http 安全：通过建立 filter 和相关的 service bean 来实现框架的认证机制。
								  AuthenticationManager：处理来自于框架其他部分的认证请求。
								  AccessDecisionManager：为 Web 或方法的安全提供访问决策。
								  AuthenticationProvider：AuthenticationManager 是通过它来认证用户的。
								  UserDetailsService：跟 AuthenticationProvider 关系密切，用来获取用户信息的。
						
							授权含义：OAuth2.0（三方授权登录）授权协议
								
													OAuth2.0 内部回调先获取token，再用token（令牌，权限范围
													和有效期）授权登录，获取用户信息
								客户端（豆瓣）  ----------------------------------------------------->   授权层、服务提供商（QQ认证服务器）
												<----------------------------------------------------
												     向客户端开放用户储存的信息，提供信息
								OpenID ：身份认证，即如何通过 URI 来认证用户身份。如果使用OpenID，你的网站地址（URI）就是你的用户名，而你的密码安全的存储在一个 OpenID 服务网站上。
								         主要原理：
										 主要原理是
											1. 首先得拥有一个合法的OpenID帐号，也就是说需要在一个验证服务器申请了一个帐号。
											2. 你有了这个帐号之后，就可以在任何一个其他支持OpenID验证的网站，并且用你上面申请的OpenID进行登录
											3. 因为这个网站并不知道你的身份是否正确，所以它会请求你的验证服务器对你的身份进行验证。
											4. 验证服务器告诉网站说，你是合法用户
											5. 网站接受你的身份，让你进入。
 
								区别：OAuth关注的是authorization；而OpenID侧重的是authentication
							
		十一、Springboot与分布式：				
				常见的分布式：zookeeper+dubbo组合，springCloud全栈技术以及spring Cloud Alibaba分布式解决方案
				架构演变：
					单一应用架构（all in one）：流量1~10，用于简化增删改查工作量的数据访问框架(ORM)是关键
					垂直应用架构：流量10~1000，将应用拆成互不相干的几个应用，以提升效率。此时，出现用于加速前端页面开发的Web框架(MVC)是关键
					分布式服务架构：流量1000~10000 ，当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心。此时，
									用于提高业务复用及整合的分布式服务框架(RPC)是关键
					流动计算架构：流量10000+，当服务越来越多，容量的评估，此时需增加一个调度中心基于访问压力实时管理集群容量，
								  提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键
				
				zookeeper：（服务的注册中心）ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务。它是
						   一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等
						   压力较小，zk 将全量数据存储在内存，高性能，而且支持集群。
				dubbo：（服务的治理）Dubbo是Alibaba开源的分布式服务框架，特点分层架构解耦，从服务的模型分为服务的提供者与消费者。- DubboX当当网
				使用参考：https://blog.csdn.net/zhengzhaoyang122/article/details/81877595	   
								
				SpringCloud：是一个分布式的整体解决方案。Spring Cloud为开发者提供了在分布式系统（配置管理，服务发现，熔断，路由，微代理，控制总线，
							 一次性token，全局琐，leader选举，分布式session，集群状态）中快速构建的工具，使用Spring Cloud的开发者可以快速的启动服务
							 或构建应用、同时能够快速和云平台资源进行对接。（详情参考SpringCloud技术栈）
		
		十二、Springboot与热部署：	
				1、禁用模板引擎的cache，使用ctrl+F9重新编译页面
				2、官方提供的Spring Loaded
				    方式一：添加依赖
					<dependency>
						<groupId>org.springframework</groupId>
						<artifactId>springloaded</artifactId>
						<version>1.2.8.RELEASE</version>
					</dependency>
					方式二：maven仓库下载，下载地址：http://mvnrepository.com/artifact/org.springframework/springloaded
				    运行（VM arguments）时参数；-javaagent:C:/springloaded-1.2.5.RELEASE.jar –noverify
				3、devtools
				   添加依赖：
				   <dependency>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-devtools</artifactId>
						<optional>true</optional>
					</dependency>
					配置设置：
					#热部署生效
					spring.devtools.restart.enabled: true
					#设置重启的目录
					#spring.devtools.restart.additional-paths: src/main/java
					#classpath目录下的WEB-INF文件夹内容修改不重启
					spring.devtools.restart.exclude: WEB-INF/**
					IDEA配置，修改了Java类后，IDEA默认是不自动编译的。自动编译：（1）File-Settings-Compiler-Build Project automatically
																				（2）ctrl + shift + alt + /,选择Registry,勾上 Compiler autoMake allow when app running
				4、JRebel热部署插件
				    安装：
						点击File -> Settings -> Plugins,如下图：搜索JRebel安装，重启IDEA工具即可
		十三、Springboot与监控：		
				使用：
					添加依赖
					<dependency>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-starter-actuator</artifactId>
					</dependency>
				监控端点分类：
					应用配置类：/autoconfig /configprops /beans /env /mappings /info
					
					度量指标类：/metrics /health /dump /trace /auditevents
					
					操作控制类：/shutdown（关闭应用，默认关闭的） 可以在application.properties中配置开启：endpoints.shutdown.enabled=true
				
				通过http方式访问监控端点
				定制端点信息：endpoints+端点名+属性名进行设置，如：
					修改端点id（endpoints.beans.id=mybeans）
					– 开启远程应用关闭功能（endpoints.shutdown.enabled=true）
					– 关闭端点（endpoints.beans.enabled=false）
					– 开启所需端点
					• endpoints.enabled=false
					• endpoints.beans.enabled=true
					– 定制端点访问根路径
					• management.context-path=/manage
					• management.port=20001
					– 关闭http端点
					• management.port=-1																
					
----------------------------------------------NoSQL缓存技术Redis的使用----------------------------------------------------------------------------------								
	一、NoSQL入门：
			时代背景[架构演变]：
				单机MySQL:APP->DAL（数据访问层）->MySQL Instance （读写混合）
				Memcached(缓存)+MySQL+垂直拆分：（原因：数据量上升，复杂的数据结构[B+Tree]），引入了Memcached分布式缓存技术，为web服务提供缓存（弊端：hash的一致性引发缓存失效），
												APP->DAL（数据访问层）->cache
				Mysql主从复制读写分离：（原因：Memcached(缓存)解决了数据库读的问题，而写的问题依然严重），出现了Mysql的master-slave模式
																				  ->S（Read）
												APP->DAL（数据访问层）->cache -> M（Write）
																				  ->S（Read）
				分表分库+水平拆分+MySQL集群：（原因：流量数据，Mysql主从复制读写分离写的问题日趋严重），出现了MySQL Cluster集群
				Mysql扩展瓶颈：大文本字段效率低
				NOSQL：处理大数据运用而生				
																																								中间件
				当今架构流程：					企业级防火墙/负载均衡Nginx设备										DAL											  ------------>实时通讯/流媒体/移动信息/电子邮件等服务器					
								客户请求  --------------------------------------->  APP服务器（多台） ------------------------------> Mysql Cluster集群数据库 ------------>缓存服务器
																																							  ------------>文件（图片）服务器																
			扩展：高性能架构思路，对于高性能网站，请求量大，如何支撑?				
				1）必要减少请求
				   对于开发人员，提高开发质量（合并css，处理背景图片，优化mysql查询等），对于运维人员善用缓存，如：nginx的expires，利用浏览器缓存等,减少查询
				2）利用cdn技术来响应请求
				   CDN：Content Delivery Network，即内容分发网络。
				   CDN是构建在网络之上的内容分发网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。
				   CDN的关键技术主要有内容存储和分发技术。
				3）对请求的处理：服务器集群+负载均衡来支撑，这一步思考如何更好的响应高并发请求，既然请求是不可避免的，我们要做的是把工作内容”平均”分给每台服务器，最理想的状态每台服务器的性能都被充分利用

			含义：NoSQL(NoSQL = Not Only SQL )，意即“不仅仅是SQL”，泛指非关系型的数据库
			特点：易扩展、大数据量高性能、多样灵活的数据模型（适合于查询、增删字段是一件非常麻烦的事情）、
			比较：
				RDBMS vs NoSQL
				RDBMS
				- 高度组织化结构化数据
				- 结构化查询语言（SQL）
				- 数据和关系都存储在单独的表中。
				- 数据操纵语言，数据定义语言
				- 严格的一致性
				- 基础事务
				NoSQL
				- 代表着不仅仅是SQL
				- 没有声明性查询语言
				- 没有预定义的模式
				- 键值对存储，列存储，文档存储，图形数据库
				- 最终一致性，而非ACID属性（数据库事务四个基本要素，原子性[Atomicity]、一致性[Consistency]、隔离性/独立性[Isolation]、持久性[Durability]）
				- 非结构化和不可预知的数据
				- CAP定理（指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得）
				- 高性能，高可用性和可伸缩性
			基础产品：Redis、memcache、Mongdb
			大数据时代的特点：
				3V + 3高
					3V：海量Volume、多样Variety、实时Velocity
					3高：高并发、高可扩、高性能
			经典案例：以阿里巴巴的商品信息存放为例
					  架构演变：Orcale -> Java Servlet -> EJB -> 去EJB重构（Ibatis）-> 海量数据：Memcached 集群，分布式存储，Mysql数据拆分 -> 安全、镜像（敏捷、开放[网站开放，允许第三方接入]、体验）
					  数据源数据类型存储问题（数据层）：关系数据库MySQL、搜索引擎、缓存Memcached、KV、文档数据库、外部数据接口（银行）、列数据库...
					  商品信息存放：
						基础信息：存储关系型数据库（MySQL，去Oracle），如：商品价格、名称
						描述性、评价详情信息（多文字类）：文档数据库MongDB中，多文字信息，关系型数据库IO读写性能变差
						图片：分布式的文件存储系统，如：TFS（淘宝）、GFS（Google）、HDFS（Hadoop）
						商品关键字：搜索引擎
						商品的波段性的热点高频信息：缓存数据库
						计算类：外部系统，外部第3方支付接口
						
			大数据时代数据DAL的解决方案：UDSL（统一数据服务层）
				统一数据服务层的特征：
					映射：传统的ORM框架是不能实现跨多数据源与类型的映射，UDSL提供了解决方案
					API：UDSL提供了统一的查询与更新API，类似JPA
					热点缓存：二级缓存，实现流程：
							  网站 ->UDSL ->缓存（设置规则） ->查询/更新（根据索引、Key）
			
			NoSQL的数据模型：
				对比关系型与非关系型数据库设计：RDBMS使用ER图，NoSQL常用的BSON（是一种类似json的一种二进制形式的存储格式，				
				简称Binary JSON，支持内嵌的文档对象和数组对象）结构
				
			聚合模型：
				KV键值、bson、列族（方便数据压缩）、图形
				
			NoSQL分类：
					KV键值对：阿里、百度：memcache+redis，美团：redis+tair，新浪：BerkeleyDB+redis
					文档型数据库(bson格式比较多)：MongoDB（基于分布式文件存储的数据库）、CouchDB
					列存储数据库：分布式文件系统（Cassandra, HBase）
					图关系数据库：它不是放图形的，放的是关系比如:朋友圈社交网络、广告推荐系统，如：Neo4J, InfoGrid
			
			分布式数据库中CAP原理CAP+BASE[重点]：
				ACID：数据库事务四个基本要素，原子性[Atomicity]、一致性[Consistency]、隔离性/独立性[Isolation]、持久性[Durability]
						  原子性：说的是事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚
						  一致性：数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束，也即在事务开始之前和事务结束以后，数据库的完整性没有被破坏
						  独立性：指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响
						  持久性：一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失
						  
				CAP：指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得（三选二）
					 说明：由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的，所以只能在一致性和可用性之间进行权衡
						对比：   
								产品				   		原则 	特点
							传统关系型数据库RDBMS			AC    	单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大
							大多数网站架构					AP		满足可用性，分区容忍性的系统，通常可能对一致性要求低一些
							Redis、Mongodb、HBase			CP		满足一致性，分区容忍性的系统，通常性能不是特别高
						   
						结论：强一致性和可用性之间取一个平衡。大多数web应用，其实并不需要强一致性（尤其是读的一致性）。因此牺牲C换取A，这是目前分布式数据库产品的方向
				BASE：
					为了解决关系数据库强一致性引起的问题而引起的可用性降低而提出的解决方案
					BASE其实是下面三个术语的缩写：
					基本可用（Basically Available）
					软状态（Soft state）
					最终一致（Eventually consistent）
					它的思想是通过让系统放松对某一时刻数据一致性的要求（实时性）来换取系统整体伸缩性和性能上改观。
					因为在于大型系统往往由于地域分布和极高性能的要求，不可能采用分布式事务来完成这些指标，
					要想获得这些指标，我们必须采用另外一种方式来完成，这里BASE就是解决这个问题的办法。
				分布式与集群：
					分布式系统：由多台计算机和通信的软件组件通过计算机网络连接组成，具有高度的内聚性和透明性。
					分布式：不同的多台服务器上面部署不同的服务模块（工程），他们之间通过Rpc/Rmi（Remote Method Invocation,远程方法调用，仅支持Java语言）之间通信和调用，对外提供服务和组内协作
					集群：不同的多台服务器上面部署相同的服务模块，通过分布式调度软件进行统一的调度，对外提供服务和访问
					
	一、NoSQL入门：
		Redis:
			含义：REmote DIctionary Server(远程字典服务器)，是完全开源免费的，用C语言编写的，遵守BSD开源协议，是一个高性能的(key/value)分布式内存数据库，
			      基于内存运行，并支持持久化的NoSQL数据库，是当前最热门的NoSql数据库之一，也称为数据结构服务器
			与其他的KV缓存产品对比优势：
				1、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用
				2、Redis不仅仅支持简单的Key-Value类型的数据，同时还提供string，list，set，zset，hash等数据结构的存储
				3、Redis支持数据的备份，即Master-Slave模式的数据备份
			作用：
				1、内存存储和持久化RDB/AOF：Redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务
				2、发布、订阅消息系统
				3、取最新N个数据的操作，如：可以将最新的10条评论的ID放在Redis的List集合里面；模拟类似于HttpSession这种需要设定过期时间的功能；定时器、计数器
			官网：
				http://redis.io/           
				http://www.redis.cn/
				
			安装使用：
				Windows: redis-server.exe redis.conf
				Linux：企业级开发
					主要步骤：
						1、下载压缩包，下载获得redis-3.0.4.tar.gz后将它放入Linux目录/opt
						2、解压进入，/opt目录下，解压命令:tar -zxvf redis-3.0.4.tar.gz，得到redis-3.0.4文件
						3、安装，cd redis-3.0.4，在redis-3.0.4目录下执行make命令，注意：可能报错，可能原因：缺少gcc库（一款支持c语言的编译工具）的支持，下载yum install gcc-c++
						   再次安装，若报“jemalloc/jemalloc.h：没有那个文件或目录”错误，运行make distclean之后再make，测试环境：macke test（需要TCL环境支持，http://www.linuxfromscratch.org/blfs/view/cvs/general/tcl.html）
						   make完成后继续执行make install-安装
					默认安装目录：/usr/local/bin
						目录结构或文件：
							redis-benchmark：性能测试工具
							redis-check-aof：修复有问题的AOF文件
							redis-check-dump：修复有问题的dump.rdb文件
							redis-cli：客户端，操作入口
							redis-sentinel：redis集群使用[哨兵]
							redis-server：Redis服务器启动命令
					启动：	
						回到解压目录redis-3.0.4/下，主要目录或文件：redis.conf、sentinel.conf、src
						1）修改redis.conf文件将里面的daemonize no 改成 yes，让服务在后台启动
						2）备份配置文件redis.conf，如：redis.conf.bak
						3）启动：
							   安装路径下执行：redis-server [解压路径下]/redis-3.0.4/redis.conf
						   客户端测试：redis-cli（测试是否连通：ping 出现PONG说明服务通的）或 redis-cli -h 127.0.0.1 -p 6379
					关闭：
						单实例关闭：redis-cli shutdown
						多实例关闭，指定端口关闭:redis-cli -p 6379 shutdown
					基础知识：
						单进程模型：单进程模型来处理客户端的请求。对读写RW等事件的响应是通过对epoll函数的包装来做到的。
									Redis的实际处理速度完全依靠主进程的执行效率。
									epoll是Linux内核为处理大批量文件描述符而作了改进的epoll，是Linux下多路复用IO接口select/poll的增强版本，
									它能显著提高程序在大量并发连接中的系统CPU利用率。
						数据库：默认16个数据库，类似数组下标从零开始，初始默认使用零号库，切换库：select 数据库Id
								dbsize：查看当前数据库的key的数量，查看所有key： Keys *；keys 支持ANT风格，如：keys k？
								flushdb：清空当前库
								Flushall；通杀全部库
								统一密码管理：16个库都是同样密码，要么都OK要么一个也连接不上，Redis索引都是从零开始
			数据类型[重点]：
				string：是redis最基本的类型，可以理解成与Memcached一模一样的类型，一个key对应一个value。
						string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象。一个redis中字符串value最多可以是512M。
				hash：哈希无序，类似java里的Map；Redis hash 是一个键值对集合。hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。
				list：列表，是简单的字符串列表，按照插入顺序排序元素可以重复。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。它的底层实际是个链表。
				set：集合，Redis的Set是string类型的无序不可重复集合，它是通过HashSet实现实现的。
				zset：sorted set有序集合，Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。
					  不同的zset是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。
				使用：参考http://redisdoc.com/
			基本操作：
				Key：
					查看所有key： keys *
					判断某个key是否存在：exists key的名字，1表示存在，-1表示不存在
					移除key：move key db   --->当前库就没有了，被移除了
					给key设置过期时间：expire key 秒钟
					查看还剩多少秒过期：ttl key ，-1表示永不过期，-2表示已过期
					查看key的类型：type key
				String[单值单value]：
					 set/get/del/append/strlen：
					 Incr/decr/incrby/decrby：只有数字才能进行加减
					 getrange/setrange：
						 getrange：获取指定区间范围内的值，类似between......and的关系;从零到负一表示全部,如：getrange key 0 -1
						 setrange：设置指定区间范围内的值，格式是setrange key值 具体值，如：setrange key 1 xxx，1索引
					 setex(set with expire) 键 秒值/setnx(set if not exist) 键 秒值
						setex：设置带过期时间的key，动态设置。setex 键 秒值 真实值，如：setex k1 15 v1
						setnx：setnx:只有在key不存在时设置 key 的值，如：setnx k1 v1
				List[单值多value]：		
					 lpush/rpush/lrange：如：lrange list1 0 -1 	
					 lpop/rpop key：取元素[出栈]
					 lindex：通过索引获取列表中的元素，语法：lindex key index，如：lindex k1 5
					 llen：
					 lrem key 删N个value： * 从left往right删除2个值等于v1的元素，返回的值为实际删除的数量，如：LREM list1 2 v1
										   * LREM list1 0 value1，表示删除全部给定的值，零个就是全部值
					 ltrim key 开始索引 结束索引，截取指定范围的值后再赋值给key，如： ltrim key1 0 3
					 rpoplpush 源列表 目的列表：移除列表的最后一个元素，并将该元素添加到另一个列表并返回，如：rpoplpush l1 l2
					 lset key index value：指定索引位置添加元素
					 linsert key  before/after 值1 值2：插入元素，如：linsert k1 after 3 5
					总结：
						它是一个字符串链表，left、right都可以插入添加；
						如果键不存在，创建新的链表；如果键已存在，新增内容；如果值全移除，对应的键也就消失了。
						链表的操作无论是头和尾效率都极高，但假如是对中间元素进行操作，效率就很惨淡了。
				Set[单值多value，底层是hashSet]：		
					sadd/smembers/sismember：如：sadd set1 v1 v2 v3
					scard：获取集合里面的元素个数，如：scard set1
					srem key value：删除集合中元素，如：srem set1 v1
					srandmember key 某个整数：随机出几个指定个数的元数，如：srandmember set1 2
					spop key ：随机出栈，如：spop set1
					smove key1 key2 在key1里某个值：作用是将key1里的某个值赋给key2，如：smove set1 set2 v1
					数学集合类：
						差集：sdiff，在第一个set里面而不在后面任何一个set里面的项
						交集：sinter
						并集：sunion
				Hash[KV模式不变，但V是一个键值对，类似Java中的Map<String,Object>]：
					 hset/hget/hmset/hmget/hgetall/hdel：如：hmset hash1 id 1 name lisi age 20
					 hlen：
					 hexists key 在key里面的某个值的key：如：hexists hash1 id
					 hkeys/hvals key：如：hkeys hash1  ----->结果：id name age
					 hincrby/hincrbyfloat：给key里面的某个key的值增加整数/浮点数
					 hsetnx：给key里面的某个key不存在才赋值，如：hsetnx hash1 k1 44
				Zset[sorted set有序集合]：	 
					在set基础上，加一个score值。之前set是k1 v1 v2 v3，现在zset是k1 score1 v1 score2 v2	
					zadd/zrange：如：zadd z1 70 v1 80 v2 90 v3，zrange z1 0 -1 withscores
					zrangebyscore key 开始score 结束score [withscores] [limit]：如：zrangebyscore z1 (60 90 withscores			-(不包含，limit 开始下标步 多少步
					删除元素：zrem key 某score下对应的value值，如：zrem z1 v1
					zcard/zcount key score区间/zrank key 下标值/zscore key 对应值，获得分数
						zcard：获取集合中元素个数，如：zcard z1
						zcount：获取分数区间内元素个数，zcount key 开始分数区间 结束分数区间，如：zcount z1 60 80 
						zrank： 获取下标位置，如：zrank z1 v1
						zscore：按照key获得对应的分数，如：zscore z1 v1
					zrevrank key values值，作用是逆序获得下标，如：zrerank z1 v1
					zrevrange：与zrange相反
					zrevrangebyscore  key 结束score 开始score：与zrangebyscore相反
					
			配置文件[redis.conf]：
				位置：指定的配置文件目录下
				解析：
					units[单位]：
					如：
					# 1k => 1000 bytes
					# 1kb => 1024 bytes
					# 1m => 1000000 bytes
					# 1mb => 1024*1024 bytes
					# 1g => 1000000000 bytes
					# 1gb => 1024*1024*1024 bytes
						1  配置大小单位,开头定义了一些基本的度量单位，只支持bytes，不支持bit
						2  对大小写不敏感
					
					INCLUDES[包含]：
						通过includes包含，redis.conf可以作为总闸，包含其他的配置信息
						如：
						# include /path/to/local.conf
						# include /path/to/other.conf
					
					GENERAL[通用]:
						daemonize：是否开启守护进程模式，默认没有开启
						pidfile：进程ID所在文件，默认位置：pidfile /var/run/redis.pid
						port：监听端口，默认6379，如果设为0，禁用监听TCP socket
						tcp-backlog：
							设置tcp的backlog，backlog其实是一个连接队列，backlog队列总和=未完成三次握手队列 + 已经完成三次握手队列。
							在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。
							注意Linux内核会将这个值减小到/proc/sys/net/core/somaxconn的值，所以需要确认增大somaxconn和tcp_max_syn_backlog两个值来达到想要的效果。
						timeout：超时断开连接，默认0，表示关闭
						bind：绑定IP/域名，非必配项
						tcp-keepalive：单位为秒，如果设置为0，则不会进行Keepalive检测，建议设置60，长链接保持心跳，超出设置的数值，会断开socket连接
						loglevel：日志级别，默认有4种，默认是：verbose
							# debug (a lot of information, useful for development/testing)
							# verbose (many rarely useful info, but not a mess like the debug level)
							# notice (moderately verbose, what you want in production probably)
							# warning (only very important / critical messages are logged)
						logfile：日志文件名，默认：stdout # output for logging but daemonize, logs will be sent to /dev/null
						syslog-enabled：是否把日志输出到syslog中，默认NO
						syslog-ident：指定syslog里的日志标志，默认redis
							# Specify the syslog identity.
							# syslog-ident redis
						syslog-facility：指定syslog设备，值可以是USER或LOCAL0-LOCAL7，默认为local0
						databases：数据库，默认16个库，切换库：select dbid # dbid is a number between 0 and 'databases'-1
					
					SNAPSHOTTING[快照]：RDB是整个内存的压缩过的Snapshot，RDB的数据结构，可以配置复合的快照触发条件！
						Save：# Save the DB on disk 将数据保存到磁盘，语法：save <秒钟> <写操作次数> 或 save <seconds> <changes>
							默认：
							#   after 900 sec (15 min) if at least 1 key changed
							#   after 300 sec (5 min) if at least 10 keys changed
							#   after 60 sec if at least 10000 keys changed
							save 900 1 						 -15分钟内改了1次                      
							save 300 10						 -5分钟内改了10次
							save 60 10000					 -1分钟内改了1万次
						
						    注意：如果想禁用RDB持久化的策略，只要不设置任何save指令，或者给save传入一个空字符串参数也可以，如：save ""
						
						stop-writes-on-bgsave-error：默认yes，快照关闭了导致不能持久化的问题
													 即当bgsave快照操作出错时停止写数据到磁盘，这样后面写错做均会失败，为了不影响后续写操作，故需将该项值改为no。
						rdbcompression：默认开启yes，
										对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis会采用LZF算法进行压缩。
										如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能
						rdbchecksum：在存储快照后，还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约
									 10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能
						dbfilename：默认：dump.rdb # The filename where to dump the DB
						dir：本地数据库存放的路径，redis的路径目录，默认./
					
					REPLICATION[复制]:	# Master-Slave replication 主从复制，默认关闭该功能
										主从复制含义：主机数据更新之后，根据配置和策略，自动同步到备机的机制。Master以写为主，Slave以读为主。实现了读写分离与容灾恢复。
										Redis 支持简单易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(master server)的精确复制品
						语法：
							方式一：配置从服务器，只需在配置文件中增加一行：slaveof <masterip> <masterport>，如果主机有密码，需要配置访问密码：masterauth <master-password>
							方式二：通过redis客户端工具连接到从（slave）服务器，输入主服务器的IP和端口，然后同步就会开始：SLAVEOF [masterip] [masterport]						
						参数：
							slave-serve-stale-data：默认yes，表示主从复制中，从服务器可以响应客户端请求；
					SECURITY[安全]:	访问密码的查看、设置和取消
						# Require clients to issue AUTH <PASSWORD> before processing any other commands.This might be useful in environments in which you do not trust
						# others with access to the host running redis-server.
						使用[客户端连接工具设置]：
							config get requirepass												-获取
							config set requirepass "123456"										-设置，设置成功后，下次操作需要Auth
							auth 123456															-认证
					
					LIMITS[限制]:
						maxclients：客户端最大连接数限制，设置redis允许同时可以与多少个客户端进行连接。默认情况下为10000个客户端。
									如果达到了此限制，redis则会拒绝新的连接请求，并且向这些连接请求方发出“max number of clients reached”以作回应。
						maxmemory：设置redis可以使用的最大内存量。一旦到达内存使用上限，redis将会试图移除内部数据，移除规则可以通过maxmemory-policy来指定。
								   如果redis无法根据移除规则来移除内存中的数据，或者设置了“不允许移除”，那么redis则会针对那些需要申请内存的指令返回错误信息，比如SET、LPUSH等。							   
								   但是对于无内存申请的指令，仍然会正常响应，比如GET等。如果你的redis是主redis（说明你的redis有从redis），
								   那么在设置内存使用上限时，需要在系统中留出一些内存空间给同步队列缓存，只有在你设置的是“不移除”的情况下，才不用考虑这个因素。
							使用：maxmemory <bytes>
							# maxmemory can be a good idea mainly if you want to use Redis as a 'state' server or cache, not as a real DB. 
						
						maxmemory-policy[设置内存使用上限策略]：
							1）volatile-lru：使用LRU算法移除key，只对设置了过期时间的键，volatile：易发辉的意思
							2）allkeys-lru：使用LRU算法移除key
							3）volatile-random：在过期集合中移除随机的key，只对设置了过期时间的键
							4）allkeys-random：移除随机的key
							5）volatile-ttl：移除那些TTL值最小的key，即那些最近要过期的key
							6）noeviction：不进行移除。针对写操作，只是返回错误信息
						
						maxmemory-samples：设置样本数量，LRU算法和最小TTL算法都并非是精确的算法，而是估算值，所以你可以设置样本的大小，	redis默认会检查这么多个key并选择其中LRU的那个。						
										   
					APPEND ONLY MODE[追加]：默认关闭no，是否开启aof持久化
						appendonly：no
						appendfilename：append only 文件名，默认：appendonly.aof
						appendfsync：是否同步，默认有3中配置
							1）always：同步持久化，每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性比较好
							2）everysec：出厂默认推荐，异步操作，每秒记录，如果一秒内宕机，有数据丢失
							3）no 从不同步
						no-appendfsync-on-rewrite：重写时是否可以运用Appendfsync，用默认no即可，保证数据安全性。
						auto-aof-rewrite-min-size：设置重写的基准值，当文件大于基准值时触发
						auto-aof-rewrite-percentage：设置重写的基准值
				
				详细参数说明[附]：
					1. Redis默认不是以守护进程的方式运行，可以通过修改该配置项，使用yes启用守护进程
					   daemonize no
					2. 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定
					   pidfile /var/run/redis.pid
					3. 指定Redis监听端口，默认端口为6379，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字
					   port 6379
					4. 绑定的主机地址
					   bind 127.0.0.1
					5. 当客户端闲置多长时间后关闭连接，超时断开连接，如果指定为0，表示关闭该功能
					   timeout 300
					6. 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose
					   loglevel verbose
					7. 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null
					   logfile stdout
					8. 设置数据库的数量，默认数据库为0，可以使用SELECT 命令切换数据库，SELECT dbid
					   databases 16
					9. 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合
						  save[快照保存] 
						  Redis默认配置文件中提供了三个条件，来保存数据：
						  save 900 1
						  save 300 10
						  save 60 10000
						  分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。
					10. 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大
					    rdbcompression yes
					11. 指定本地数据库文件名，默认值为dump.rdb
					    dbfilename dump.rdb
					12. 指定本地数据库存放目录
					    dir ./
					13. 设置当本机为slave服务时，设置master的IP及端口，在Redis启动时，它会自动从master进行数据同步
					    slaveof 
					14. 当master服务设置了密码保护时，slave服务连接master的密码
					    masterauth 
					15. 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH 命令提供密码，默认关闭
					    requirepass foobared
					16. 设置同一时间最大客户端连接数，默认无限制，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息
					    maxclients 128
					17. 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区
					    maxmemory 
					18. 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no
					    appendonly no
					19. 指定aof文件名，默认为appendonly.aof
					    appendfilename appendonly.aof
					20. 指定更新日志条件，共有3个可选值： 
					    appendfsync:
							no：表示等操作系统进行数据缓存同步到磁盘（快） 
							always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） 
							everysec：表示每秒同步一次（折中，默认值），appendfsync everysec
					21. 指定是否启用虚拟内存机制，默认值为no，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中
					    vm-enabled no
					22. 虚拟内存文件路径，默认值为/tmp/redis.swap，注意：不可多个Redis实例共享
					    vm-swap-file /tmp/redis.swap
					23. 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0
					    vm-max-memory 0
					24. Redis swap文件[redis.swap]分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的数据大小来设定的，建议如果存储很多小对象，page大小最好设置为32或者64 bytes；如果存储很大的数据对象，则可以使用更大的page，如果不确定，就使用默认值
					    vm-page-size 32						
					25. 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，在磁盘上每8个pages将消耗1byte的内存。
					    vm-pages 134217728
						# The total swap size is vm-page-size * vm-pages
					26. 设置访问swap文件[redis.swap]的线程数,最好不要超过机器的核数,如果设置为0，那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4
					    vm-max-threads 4
					27. 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启
					    glueoutputbuf yes
					28. 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法
					    hash-max-zipmap-entries 64
					    hash-max-zipmap-value 512
					29. 指定是否激活重置哈希，默认为开启
					    activerehashing yes
					30. 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件
					    include /path/to/local.conf
				
			持久化：
				RDB（Redis DataBase）：SNAPSHOTTING[快照]的使用
					含义：在指定的时间间隔内将内存中的数据集快照写入磁盘[save]，也就是行话讲的Snapshot快照，它恢复时是将dump.rdb快照文件直接读到内存里，
						  Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。					
						  整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能，如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，
						  但是RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。						  
					
										子进程								  持久化结束
						  fork ---------------------------> 临时文件	----------------------> 临时文件替换上次持久化好的文件dump.rdb
					
					Fork的作用：fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等）
								数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程
				
					保存文件名：dump.rdb
					触发RDB快照：
						默认配置位置：安装目录/usr/local/bin/dump.rdb，备份后再使用：cp dump.rdb dump.bak.rdb
						save或者是bgsave：
							Save：save时只管保存，其它不管，全部阻塞
							BGSAVE[后台保存]：Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求，可以通过lastsave命令获取最后一次成功执行快照的时间。
							注意：执行flushall命令，也会产生dump.rdb文件，但里面是空的，无意义；	  
					恢复数据：
						将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可；提示：如何获取安装目录？CONFIG GET dir
					优势与劣势：
						优势：适合大规模的数据恢复，对数据完整性和一致性要求不高；
						劣势：fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑；
					
					禁用RDB：动态停止RDB保存规则：redis-cli config set save ""	
					
				AOF（Append Only File）：
					含义：以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，
						  redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。
						  
					保存文件名：appendonly.aof
					AOF启动/修复/恢复：
						正常恢复：
							启动：设置Yes，修改默认的appendonly no，改为yes
							恢复：将有数据的aof文件复制一份保存到对应安装目录(config get dir)，重启redis然后重新加载
							
						异常恢复：
							启动：设置Yes，修改默认的appendonly no，改为yes，然后备份被写坏的AOF文件，注：appendonly.aof文件损坏后重新加载启动会报错
							修复：redis-check-aof --fix
							恢复：重启redis然后重新加载
					rewrite：
						含义：AOF采用文件追加的方式，文件会越来越大为避免出现此种情况，新增了重写机制，
							  当AOF文件的大小超过所设定的阈值时，redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集，keyi使用bgrewriteaof。
						原理：AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，遍历新进程的内存中的数据，
						      每条记录有一条set语句。重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令方式重写了一个新的aof文件，这点和快照有点类似。
						触发机制：redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发。
						优势与劣势：
							优势：appendfsync always  [每修改同步]同步持久化，每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性比较好
								  appendfsync everysec [每秒同步]异步操作，每秒记录，如果一秒内宕机，有数据丢失
								  appendfsync no 从不同步
							劣势：相同数据集的数据而言aof文件要远大于rdb文件，恢复速度慢于rdb；
								  aof运行效率要慢于rdb，每秒同步策略效率较好，不同步效率和rdb相同
				总结：
					官方建议：
						1）RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储
						2）AOF持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，
						   AOF命令以redis协议追加保存每次写的操作到文件末尾，Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大；	
						3）只做缓存，如果只希望数据在服务器运行的时候存在，你也可以不使用任何持久化方式  
					同时开启两种持久化方式：
						在这种情况下，当redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整
						RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。
						那要不要只使用AOF呢？建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)，
						快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。
						
					建议：
						因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这条规则。
						如果开启AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了。
						代价一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。
						只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上。默认超过原大小100%大小时重写可以改到适当的数值。
						如果不开启AOF，仅靠Master-Slave Replication 实现高可用性也可以。能省掉一大笔IO也减少了rewrite时带来的系统波动。
						代价是如果Master/Slave同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个Master/Slave中的RDB文件，载入较新的那个。
						
			事务[区分于RDBMS中的事务ACID]：
				含义：可以一次执行多个命令，本质是一组命令的集合。一个事务中的所有命令都会序列化，按顺序地串行化执行而不会被其它命令插入，不许加塞。
				作用：一个队列中，一次性、顺序性、排他性的执行一系列命令。
				命令：
					discard：取消事务
					exec：执行事务块内的命令
					multi：事务块的开始
					unwatch：取消对key的监视
					watch：监视key
				使用：
					正常执行，语法：multi 命令块 exec
					放弃事务，语法：multi 命令块 discard
					全体连坐，语法：multi 命令块[含有错误指令] exec ，结果一个出错，都不执行
					冤头债主，语法：multi 命令块[含有错误执行结果] exec ，抛出错误信息
					监视[监控]：
						悲观锁/乐观锁/CAS(Check And Set)：
							悲观锁：顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。
									传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。
							乐观锁(Optimistic Lock)：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。
													 乐观锁适用于多读的应用类型，这样可以提高吞吐量，乐观锁策略:提交版本必须大于记录当前版本才能执行更新。
						语法：
							watch key                                                       -监控了key，如果key被修改了，后面一个事务的执行失效;一旦执行了exec之前加的监控锁后面执行会被取消掉了
							
						结论：
							1）Watch指令，类似乐观锁，事务提交时，如果Key的值已被别的客户端改变，比如某个list已被别的客户端push/pop过了，整个事务队列都不会被执行
							2）通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化，EXEC命令执行的事务都将被放弃，同时返回Nullmulti-bulk应答以通知调用者事务执行失败
							
							3）单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。
							4）没有隔离级别的概念：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，
												   也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题。
							5）不保证原子性：redis同一个事务中如果有一条命令执行结果失败，其后的命令仍然会被执行，没有回滚					   
						
						事务执行流程：
							1）开启：以MULTI开始一个事务
							2）入队：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面[QUEUED]
							3）执行：由EXEC命令触发事务
						
			发布与订阅[注意与jMS的区别]：
				含义：
					进程间的一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。
				流程图：
					
												订阅SUBSCRIBE						发布消息PUBLISH
					客户端（支持多个）	---------------------------->channel <------------------------- 服务端
										<---------------------------
												消息MESSAGE
				结论：
					先订阅后发布后才能收到消息，可以一次性订阅多个，如：SUBSCRIBE c1 c2 c3；订阅支持通配符*，如：PSUBSCRIBE new*
					发息发布，PUBLISH c2 xxxxx；PUBLISH new1 xxxxx
					 
 			REPLICATION[复制，(Master/Slave)]：
				含义：
					也就是我们所说的行话主从复制，主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主。
				作用：
					读写分离
					容灾恢复
				使用：
					原则：坚持配从(库)不配主(库)
					从库配置：slaveof <masterip> <masterport>，注意：每次与master断开之后，都需要重新连接[客户端操作]，除非你配置进redis.conf文件；查看Master-Slaves复制信息：info replication
					细节[修改配置文件]:
						1）需要拷贝多份[redis.conf]配置文件；如：cp redis.conf [自定义配置路径]/redis6379.conf；
																 cp redis.conf [自定义配置路径]/redis6380.conf；
																 cp redis.conf [自定义配置路径]/redis6381.conf；
						2）开启守护进程[配置文件]：
							如：
								daemonize yes							-开启守护进程
								pidfile /var/run/redis6379.pid			-指定进程ID
								port 6379								-指定端口号
								logfile	"log6379.log"					-指定日志文件
								dbfilename dump6379.rdb					-指定RDB文件名
					3种方案：
						一主二仆[一个Master两个Slave]：
							开启Master和Slave服务：
							   启动服务主SERVER：安装路径下执行redis-server [自定义配置路径]/redis6379.conf，客户端连接：redis-client -p 6379
							   启动服务从SERVER：安装路径下执行redis-server [自定义配置路径]/redis6380.conf，客户端连接：redis-client -p 6380，配置从服务器：SLAVEOF 127.0.0.1 6379
							   启动服务从SERVER：安装路径下执行redis-server [自定义配置路径]/redis6381.conf，客户端连接：redis-client -p 6381，配置从服务器：SLAVEOF 127.0.0.1 6379
							   
							观察日志：
							   主机日志：观察控制台数据同步到从服务器成功！
							   备机日志：Master-Slaver数据同步成功！
							   查看Master-Slaves复制信息：info replication
							问题：
								1）切入点问题？slave1、slave2是从头开始复制还是从切入点开始复制?比如从k4进来，那之前的K123是否也可以复制?全量复制/增量复制
								2）从机是否可以写W？set可否？
								3）主机shutdown后情况如何？从机是上位还是原地待命？投票选举新的Master
								4）主机又回来了后，主机新增记录，从机还能否顺利复制？
								5）其中一台从机down后情况如何？重启后它能跟上大部队吗？可以
						薪火相传：
							结论：
								1）上一个Slave可以是下一个slave的Master，Slave同样可以接收其他slaves的连接和同步请求，
								   那么该slave作为了链条中下一个的master，可以有效减轻master的写压力
								2）中途变更转向:会清除之前的数据，重新建立拷贝最新的数据；slaveof 新主库IP 新主库端口
						反客为主：配置文件中或客户端工具添加一行：SLAVEOF no one					-使当前数据库停止与其他数据库的同步，转成主数据库	
						
				原理分析：
					1）slave启动成功连接到master后会发送一个同步[sync]命令；
					2）Master接到同步[sync]命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，
					   master将传送整个数据文件到slave,以完成一次完全同步；
					3）全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中
					4）增量复制：Master继续将新的所有收集到的修改命令依次传给slave，完成同步；但是只要是重新连接master,一次完全同步（全量复制)将被自动执行
				
				哨兵模式：
					含义：反客为主的自动版，能够后台监控主机Master是否故障，如果故障了根据投票数自动将从库Slaver转换为主库；
					使用：
						1）[自定义配置文件目录下]/新建文件：sentinel.conf
						2）配置内容：sentinel monitor 被监控数据库名字[自定义，如：host6379] 127.0.0.1 6379 1 
									 上面最后一个数字1，表示Master主机挂掉后Salve投票看让谁接替成为主机，得票数多少后成为主机;
						3）启动哨兵：redis-sentinel [自定义配置文件目录下]/sentinel.conf，在启动一主二从；
					结论：
						1）当Master服务宕机后，所有的Slaver进行投票选举推出新的Master
						2）当主机重启后，只能以Slaver的角色加入队伍当中
						3）一组sentinel能同时监控多个Master	
					缺点：
						由于所有的写操作都是先在Master上操作，然后同步更新到Slave上，所以从Master同步到Slave机器有一定的延迟，
						当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重。
				
			Redis对应Java客户端：Jedis、Spring-data-redis

----------------------------------------------MysqL高级运维知识---------------------------------------------------------------------
	一、Mysql运算符：
		安全等于运算符[<=>]：规则均为NULL时，其返回值为1，否则返回0；
						如：
						SELECT NULL <=>1;  						-0
						SELECT 1<=>0;							-0
						SELECT NULL <=>NULL; 					-1
		最值运算符：
			LEAST()或least()运算符，返回最小值；如：SELECT LEAST(2,0),LEAST('a','b','c'),LEAST(10,NULL);					-0,a,null
			GREATEST()或greatest()运算符，返回最大值；SELECT GREATEST(2,0),GREATEST('a','b','c'),GREATEST(10,NULL);			-2,c,null
		REGEXP[正则]运算符:
			正则表达式：在不同的开发语言中，正则表达式的基本语法都是一样的，只是在使用方式上有所差别
			注意：MySQL 支持转义字符 
				# 单引号：\'
				# 双引号：\''，如：INSERT INTO test(`name`) value('\'');
				# 反斜杠：\\
				# 回车符：\r
				# 换行符：\n
				# 制表符：\tab
				# 退格符：\b
			规则：
				# '\b' 匹配一个字边界，即字与空格间的位置
				# '\B' 匹配\b之外的任意字符
				# '\d' 匹配0-9的任意一个数字，如：/^\d$/ 表示数字
				# '\D' 匹配\d之外的任意字符
				# '\f' 匹配换页符
				# '\r' 匹配回车符
				# '\s' 匹配任何空白字符(包括空格、制表符、换页符、换行符等)
				# '\S' 匹配任何非空白字符
				# '\t' 匹配制表符
				# '\v' 匹配垂直制表符
				# '\n' 匹配换行符
				# '\w' 匹配包括下划线在内的任意单词字符(包括A-Z、a-z、0-9、_，就是行话:数字、字母、下划线)，
						如：/^\w{6,16}$/ 表示6-16位字母、数字、下划线组成的
				# '\W' 匹配\w之外的所有字符
				# '^'  匹配以该字符后面的字符开头的字符串，如：SELECT name FROM t_person WHERE name REGEXP '^st';
				# '$'  匹配以该字符前面的字符结尾的字符串，如：SELECT name FROM t_person WHERE name REGEXP 'ok$';
				# '.'  表示任何一个单字符，除\n外
				# '[]' 匹配在方括号内的任一个字符
						 如：“[abc]" 匹配a、b或c。“[a-z]”匹配任何单个字母，而“[0-9]”匹配任何单个数字	
				# '*' 匹配零个或多个在他前面的字符或子表达式，如：'zx*' 可以匹配z后面跟0到无数个x(「z」或「zxx」皆可)
					  如：“x*”匹配任何数量的'*'字符,等价于'{0,}'，“[0-9]*”匹配任何数量的数字，而“.*”匹配任何数量的任何字符
				# '?' 匹配零次或一次匹配前面的字符或子表达式 
				# '+' 匹配至少一个在他前面的字符，等价于{1,}
				# '|' 或，如：p1|p2|p3表示匹配 p1 或 p2 或 p3；g|food 匹配g或food；(g|f)ood 匹配good或food
				# '[^字符集合]' 匹配不在指定集合中的任何字符，如： '[^abc]' 可以匹配不在abc内的任意字符
				# '{n,}'或'{n,m}'，如：“字符串{n,}”表示前面的字符串至少匹配n次；“字符串{n,m}”表示匹配前面的字符串不少于n次，不多于m次
				  示例：'a{5,8}'匹配5-8个a；'123{3}'匹配结果12333；'(456){2}'匹配结果456456
			  
		逻辑运算符：
			逻辑与：AND 或&&；逻辑或 OR或||
				如：
				SELECT * FROM testuser WHERE gender ='女' AND `name` LIKE '%赵%';
				SELECT * FROM testuser WHERE gender ='女' && `name` LIKE '%赵%';
			异或XOR运算符：
				当任意一个操作数为NULL时，返回值为NULL;对于非NULL的操作数，
				如果两个操作数都是非0值或者都是0值，则返回结果为0，反之为1，
				XOR等同于a AND (NOT b))或者NOT a AND (b)
				如：SELECT 1 XOR 1, 0 XOR 0,1 XOR 0,1 XOR NULL,1 XOR 1 XOR 1;					-0,0,1,null,1
		
		Mysql事务管理：
			Mysql事务是必须满足4个条件（ACID）：
				原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）
			注意：
				1）在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务；
				2）事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行；
				3）事务用来管理 insert,update,delete 语句；
		    事务控制语句：
				BEGIN 或 START TRANSACTION 显式地开启一个事务
				COMMIT 或 COMMIT WORK。COMMIT 会提交事务，并使已对数据库进行的所有修改成为永久性的
				ROLLBACK 或 ROLLBACK WORK。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改
				SAVEPOINT identifier，SAVEPOINT 允许在事务中创建一个保存点，一个事务中可以有多个 SAVEPOINT
				RELEASE SAVEPOINT identifier 删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常
				ROLLBACK TO identifier 把事务回滚到标记点
				SET TRANSACTION 用来设置事务的隔离级别。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE
			事务的处理方式：
				方式一：用 BEGIN, ROLLBACK, COMMIT来实现
				方式二：直接用 SET 来改变 MySQL 的自动提交模式；SET AUTOCOMMIT=0 禁止自动提交；SET AUTOCOMMIT=1 开启自动提交；
			结论：
				在 MySQL 命令行的默认设置下，事务都是自动提交的，即执行 SQL 语句后就会马上执行 COMMIT 操作。
				因此要显式地开启一个事务务须使用命令 BEGIN 或 START TRANSACTION，或者执行命令 SET AUTOCOMMIT=0，用来禁止使用当前会话的自动提交。
				如：
					BEGIN;
					SQL insert,update,delete 语句；
					COMMIT/ROLLBACK;					-数据库表有改变/数据库表没有改变
		
		Mysql视图[虚表]：
			含义：视图（view）是一种虚拟存在的表，是一个逻辑表，本身并不包含数据，而是由一个SELECT语句保存在数据字典中的。
				  通过视图，可以展现基表的部分数据；视图数据来自定义视图的查询中使用的表，使用视图动态生成。
			优点：
				为了保障数据安全性，提高查询效率，简单（是过滤好的复合条件的结果集）、安全（用户只能查询或修改所能见到得到的数据）、数据独立（屏蔽真实表结构变化带来的影响）
			
			语法：	  
				# CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]     -- 创建、替换、选择算法[未定义的[默认]，合并，临时的] 
				# VIEW view_name [(column_list)] 										-- 视图名、列名
				# AS select_statement 													-- 查询SELECT语句
				# [WITH [CASCADED | LOCAL] CHECK OPTION] 								-- 控制权限，推荐使用，可以保证数据的安全性
			基本格式：
			　　create view <view_name>[(column_list)]
				as select语句
				with check option;
			示例：
				-- 建表、插入数据
				CREATE TABLE student (stuno INT ,stuname NVARCHAR(60));											-学生表（学号、姓名）
				CREATE TABLE stuinfo (stuno INT ,class NVARCHAR(60),city NVARCHAR(60));							-学生信息表（学号、班级、城市）
				INSERT INTO student VALUES(1,'wanglin'),(2,'gaoli'),(3,'zhanghai');
				INSERT INTO stuinfo VALUES(1,'wuban','henan'),(2,'liuban','hebei'),(3,'qiban','shandong');
				
				-- 创建视图
				CREATE VIEW stu_class(id,NAME,class) AS SELECT student.`stuno`,student.`stuname`,stuinfo.`class`
				FROM student,stuinfo WHERE student.`stuno`=stuinfo.`stuno` with check option;
				
				-- 查看视图														  
					方法：
						方式一：SELECT * FROM stu_class								-- 类似查表的方式
						方式二：DESCRIBE 或 DESC
								DESCRIBE stu_class; 或 DESC stu_class;
						方式三：SHOW TABLE STATUS LIKE
								SHOW TABLE STATUS LIKE 'stu_class'; 								-- comment项为view表示视图
								SHOW TABLE STATUS LIKE 'stuinfo';							    	-- 基表
						方式四：SHOW CREATE VIEW
								SHOW CREATE VIEW stu_class;
								SELECT * FROM `information_schema`.`VIEWS`; 						-- 查看MySQL数据库中所有视图的详细信息
				-- 修改视图
					含义：通过视图更新的时候都是转到基表进行更新，如果对视图增加或者删除记录，实际上是对基表增加或删除记录
					语法：
						# ALTER OR REPLACE [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}]
						# VIEW view_name [(column_list)]
						# AS select_statement
						# [WITH [CASCADED | LOCAL] CHECK OPTION]
						如：
						DELIMITER $$																-- 分隔符

						CREATE OR REPLACE VIEW `stu_class` AS 										-- 创建或替换
						SELECT
						  `student`.`stuno`  AS `id`
						FROM (`student` JOIN `stuinfo`)
						WHERE (`student`.`stuno` = `stuinfo`.`stuno`) WITH CHECK OPTION $$

						DELIMITER ;
						
						DESC stu_class;																-- 查看
						SELECT * FROM stu_class;													-- 查看
					DML操作更新视图[单表]：
						结论：
							如：
							ALTER VIEW stu_class AS SELECT stuno,stuname FROM student;					-- 修改视图
							
							UPDATE stu_class SET stuname='xiaofang' WHERE stuno=2; 						-- 修改视图数据
							SELECT * FROM student; 														-- 原表的数据也改变
							INSERT INTO stu_class VALUES(6,'haojie');
							DELETE FROM stu_class WHERE stuno=1;
						注意：
							并非所有视图都可以做DML操作，以下情况例外：
								①select子句中包含distinct
							　　②select子句中包含组函数
							　　③select语句中包含group by子句
							　　④select语句中包含order by子句
							　　⑤select语句中包含union 、union all等集合运算符
							　　⑥where子句中包含相关子查询
							　　⑦from子句中包含多个表
							　　⑧如果视图中有计算列，则不能更新
							　　⑨如果基表中有某个具有非空约束的列未出现在视图定义中，则不能做insert操作
				
				-- 删除视图
					# 语法 ：视图本身没有数据，因此对视图进行的DML操作最终都体现在基表中
					# DROP VIEW [IF EXISTS]
					# view_name [, view_name] ...
					# [RESTRICT | CASCADE]
					如：
					DROP VIEW IF EXISTS stu_class; 
					-- SHOW CREATE VIEW stu_class; 														-- Table '[库名].stu_class' doesn't exist
		
		Mysql触发器：
			含义：触发器是与表有关的数据库对象，在满足定义条件时触发，并执行触发器中定义的语句集合。
			特点：
				1、有begin end体，begin end之间的语句可以写的简单或者复杂的
			　　2、什么条件会触发：I、D、U
			　　3、什么时候触发：在增删改前或者后
			　　4、触发频率：针对每一行执行
			　　5、触发器定义在表上，附着在表上
				也就是由事件来触发某个操作，事件包括INSERT语句，UPDATE语句和DELETE语句；可以协助应用在数据库端确保数据的完整性
			注意：
				尽量少使用触发器，不建议使用！
				触发器是针对每一行的；对增删改非常频繁的表上切记不要使用触发器，因为它会非常消耗资源
			使用：
				语法：
				# CREATE TRIGGER trigger_name trigger_time trigger_event  
				# ON tbl_name FOR EACH ROW trigger_stmt
				 
				参数解释 ：trigger_name 触发器名称; trigger_time 触发时间，值有：BEFORE或AFTER; 
				           trigger_event 激发事件，值有：INSERT UPDATE DELETE; tbl_name 表名 
		   	               trigger_stmt 触发程序激活时执行的语句，是BEGIN ... END复合语句结构。
				名词解释：
					OLD & NEW ：表示触发器的所在表中，触发了触发器的那一行数据，来引用触发器中发生变化的记录内容[原数据/新数据]；
				
				
				-- 单执行[有一个执行语句]触发器
				CREATE TABLE account(acct_num INT ,amount DECIMAL(10,2));								-- 建表
				CREATE TRIGGER ins_sum BEFORE INSERT ON account											-- 触发器
				FOR EACH ROW SET @SUM=@SUM+new.amount;

				#申明变量
				DECLARE @SUM INT
				SET @SUM=0
				INSERT INTO account VALUES(1,1.00),(2,2.00)
				SELECT @SUM
				select * from account;
				
				-- 多执行[多个执行语句]触发器
				CREATE TABLE `user` (																	-- 建表
				  `id` BIGINT(20) NOT NULL AUTO_INCREMENT COMMENT '用户ID',
				  `account` VARCHAR(255) DEFAULT NULL COMMENT '用户账号',
				  `name` VARCHAR(255) DEFAULT NULL COMMENT '用户姓名',
				  `address` VARCHAR(255) DEFAULT NULL COMMENT '用户地址',
				  PRIMARY KEY (`id`)
				) ENGINE=INNODB DEFAULT CHARSET=utf8;

				CREATE TABLE `user_history` (															-- 建表
				  `id` BIGINT(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID',
				  `user_id` BIGINT(20) NOT NULL COMMENT '用户ID',
				  `operatetype` VARCHAR(200) NOT NULL COMMENT '操作的类型',
				  `operatetime` DATETIME NOT NULL COMMENT '操作时间',
				  PRIMARY KEY (`id`)
				) ENGINE=INNODB DEFAULT CHARSET=utf8;
				
				
				DROP TRIGGER IF EXISTS `tri_insert_user`;												-- 清空触发器										
				
				DELIMITER ;;
				CREATE TRIGGER `tri_insert_user` AFTER INSERT ON `user` FOR EACH ROW BEGIN 				-- 新增
					INSERT INTO user_history(user_id, operatetype, operatetime) VALUES (new.id, 'add a user', NOW());
				END
				;;
				DELIMITER ;

				DROP TRIGGER IF EXISTS `tri_update_user`;												-- 清空触发器				
				DELIMITER $$
				CREATE TRIGGER `tri_update_user` AFTER UPDATE ON `user` FOR EACH ROW BEGIN 				-- 修改
					INSERT INTO user_history(user_id,operatetype, operatetime) VALUES (new.id, 'update a user', NOW());
				END $$
				DELIMITER ;

				DROP TRIGGER IF EXISTS `tri_delete_user`;												-- 清空触发器
				DELIMITER |
				CREATE TRIGGER `tri_delete_user` AFTER DELETE ON `user` FOR EACH ROW BEGIN 				-- 删除
					INSERT INTO user_history(user_id, operatetype, operatetime) VALUES (old.id, 'delete a user', NOW());
				END
				|
				DELIMITER ;
				
				注意：分割线[语句结束符]可以使用的符号[可以自定义]：$$ ;; | // 

				-- 向user表中插入数据
				INSERT INTO `user`(account, `name`, address) VALUES ('zhangsan.@sina.cn', 'zhangsan', '合肥');
				INSERT INTO `user`(account, `name`, address) VALUES ('lisi.@sina.cn', 'lisi', '蚌埠');
				INSERT INTO `user`(account, `name`, address) VALUES ('wangwu.@sina.cn', 'wangwu', '芜湖'),('zhaoliu.@sina.cn', 'zhaoliu', '安庆');
				-- 向user表中修改数据
				UPDATE `user` SET `name` = 'qianqi', account = 'qianqi.@sina.cn', address='铜陵' WHERE `name`='zhaoliu';
				-- 向user表中删除数据
				DELETE FROM `user` WHERE `name` = 'wangwu';

				-- 查看触发器
				SHOW TRIGGERS;																  -- 查看所有的触发器
				SELECT * FROM `information_schema`.`TRIGGERS` WHERE `TRIGGER_NAME`='ins_sum'; -- 查看指定触发器
				
				-- 删除触发器，注意：基表删除后，触发器不复存在
				# 语法：DROP TRIGGER [schema_name.]trigger_name
				DROP TRIGGER `db1`.`ins_sum`;				  
				  
		Mysql存储过程[函数]：
			含义：存储过程简称过程，Stored Procedure，是一种在数据库中存储复杂程序，以便外部程序调用的一种数据库对象。
				  简单点，可以将其理解为没有返回值的函数，一般存储过程并不显示结果，而是把结果返回给你指定的变量。
				  存储过程是为了完成特定功能的SQL语句集，经编译创建并保存在数据库中，用户可通过指定存储过程的名字并给定参数(需要时)来调用执行。
				  存储过程思想上很简单，就是数据库 SQL 语言层面的代码封装与重用。
			优点：
				存储过程可封装，并隐藏复杂的商业逻辑
				存储过程可以回传值，并可以接受参数
				存储过程无法使用 SELECT 指令来运行，因为它是子程序，与查看表，数据表或用户定义函数不同
				存储过程可以用在数据检验，强制实行商业逻辑等
			缺点：
				存储过程，往往定制化于特定的数据库上，因为支持的编程语言不同。当切换到其他厂商的数据库系统时，需要重写原有的存储过程。
				存储过程的性能调校与撰写，受限于各种数据库系统。
			
			使用：参考：https://blog.csdn.net/qq_40884473/article/details/78442457 	
				基本格式：
					-- 创建存储过程，声明结束符
					DELIMITER $  													-- 声明存储过程的结束符
					CREATE PROCEDURE pro_name()    									-- 存储过程名称(参数列表)，参数：IN：表示输入参数，可以携带数据带存储过程中；
																													 OUT：表示输出参数，可以从存储过程中返回结果；
																													 INOUT：表示输入输出参数，既可以输入功能，也可以输出功能
					BEGIN
						-- 可以写多个sql语句;       								-- sql语句+流程控制
						SELECT * FROM table_name;
					END $  															-- 结束结束符
					
					-- 执行存储过程
					CALL pro_name();   												-- CALL 存储过程名称(参数);
					
					-- 删除存储过程 
					DROP PROCEDURE [IF EXISTS] pro_name;
					 
				完整格式：
					# CREATE PROCEDURE sp_name ([ proc_parameter ]) [ characteristics..] routine_body 
					# 参数：sp_name 存储过程名称
					#		proc_parameter 参数列表，形式：[IN|OUT|INOUT] param_name type
					#	
					#  characteristic: 											    -- 特征项
					#    LANGUAGE SQL  												    -- SQL语言
					#  | [NOT] DETERMINISTIC 											-- 存储过程执行的结果是否确定
					#  | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } -- 程序包含、不包含、读、写SQL语句
					#  | SQL SECURITY { DEFINER | INVOKER } 							-- 安全性  定义者 调用者
					#  | COMMENT 'string' 												-- 注释
					#   
					#  routine_body: 													-- SQL代码的内容 用BEGIN...END来表示SQL代码的开始和结束
				示例：
					输入参数：
						DELIMITER $
						CREATE PROCEDURE pro_findById(IN eid INT)  						-- IN: 输入参数
						BEGIN
							SELECT * FROM t_employee WHERE id=eid;
						END $ 
						
						DELIMITER ;
						 						
						CALL pro_findById(4);											-- 调用
					输出参数：
						DELIMITER $
						CREATE PROCEDURE pro_Out(OUT str VARCHAR(20))  					-- OUT：输出参数
						BEGIN								
							SET str='hellojava';										-- 给参数赋值
						END $
						
						DELIMITER ;
						问题：如何接收？？？答案使用Mysql变量						
						CALL pro_Out(@NAME);													-- 调用
						SELECT @NAME;
						
						Mysql变量：全局变量、会话变量、局部变量
							全局变量：全局变量又叫内置变量，是mysql数据库内置的变量，对所有连接都起作用。
									  查看所有全局变量：show variables；查看某个全局变量：select @@变量名；如：select @@basedir
									  修改全局变量：set 变量名=新值；character_set_client: mysql服务器的接收数据的编码；character_set_results：mysql服务器输出数据的编码；
							会话变量：
								只存在于当前客户端与数据库服务器端的一次连接当中。如果连接断开，那么会话变量全部丢失！
								定义会话变量: set @变量=值；查看会话变量：select @变量
							局部变量：
								在存储过程中使用的变量就叫局部变量，只要存储过程执行完毕，局部变量就丢失。
								
					输入输出参数：
						DELIMITER $
						CREATE PROCEDURE pro_InOut(INOUT n INT)  							-- INOUT：输入输出参数
						BEGIN						   
						   SELECT n;														-- 查看变量
						   SET n =500;
						END $																
			
						SET @n=10;															-- 调用				 
						CALL pro_InOut(@n);						 
						SELECT @n;
					
					条件判断：
						DELIMITER $
						CREATE PROCEDURE pro_If(IN num INT,OUT str VARCHAR(20))
						BEGIN
							IF num=1 THEN
								SET str='星期一';
							ELSEIF num=2 THEN
								SET str='星期二';
							ELSEIF num=3 THEN
								SET str='星期三';
							ELSE
								SET str='输入错误';
							END IF;
						END $
						 						
						CALL pro_If(4,@str);											--调用						 
						SELECT @str;
					
					循环功能：						
						DELIMITER $
						CREATE PROCEDURE proWhile(IN num INT,OUT result INT)
						BEGIN	
							DECLARE	i INT DEFAULT 1;										--定义一个局部变量
							DECLARE vsum INT DEFAULT 0;
							WHILE i<=num DO
								  SET vsum = vsum+i;
								  SET i=i+1;
							END WHILE;
							SET result=vsum;
						END $
						 
						CALL pro_While(100,@result);									--调用						 
						SELECT @result;
					
					将查询的结果赋值给变量[INTO]:
						DELIMITER $
						CREATE PROCEDURE pro_findById(IN eid INT,OUT vname VARCHAR(20) )
						BEGIN
							SELECT empName INTO vname FROM t_employee WHERE id=eid;
						END $
						
						CALL pro_findById2(1,@NAME);									--调用						 
						SELECT @NAME;
						
		Mysql存储函数：				
			含义：MySQL存储函数（自定义函数），函数一般用于计算和返回一个值，可以将经常需要使用的计算或功能写成一个函数。
			使用：
				语法：
					CREATE OR ALTER FUNCTION func_name ([param_name type[,...]])		--创建或修改
					RETURNS type
					[characteristic ...] 
					BEGIN
						routine_body
					END;
				
					参数说明：
						1）func_name ：存储函数的名称。
						2）param_name type：可选项，指定存储函数的参数。type参数用于指定存储函数的参数类型，该类型可以是MySQL数据库中所有支持的类型。
						3）RETURNS type：指定返回值的类型。
						4）characteristic：可选项，指定存储函数的特性。
						5）routine_body：SQL代码内容。
					
					调用存储函数：
						在MySQL中，存储函数的使用方法与MySQL内部函数的使用方法基本相同。
						用户自定义的存储函数与MySQL内部函数性质相同。区别在于，存储函数是用户自定义的。而内部函数由MySQL自带。
						语法结构：SELECT func_name([parameter[,…]]);
						示例：
							-- 创建用户信息表
							CREATE TABLE IF NOT EXISTS tb_user
							(
								id INT AUTO_INCREMENT PRIMARY KEY COMMENT '用户编号',
								name VARCHAR(50) NOT NULL COMMENT '用户姓名'
							) COMMENT = '用户信息表';
							 
							-- 添加数据
							INSERT INTO tb_user(name) VALUES('pan_junbiao的博客');
							INSERT INTO tb_user(name) VALUES('KevinPan');
							INSERT INTO tb_user(name) VALUES('pan_junbiao');
							INSERT INTO tb_user(name) VALUES('阿标');
							INSERT INTO tb_user(name) VALUES('panjunbiao');
							INSERT INTO tb_user(name) VALUES('pan_junbiao的CSDN博客');
							INSERT INTO tb_user(name) VALUES('https://blog.csdn.net/pan_junbiao');
							-- 创建存储函数
							DROP FUNCTION IF EXISTS func_user;
							CREATE FUNCTION func_user(in_id INT)
							RETURNS VARCHAR(50)
							BEGIN
								DECLARE out_name VARCHAR(50);						 
								SELECT name INTO out_name FROM tb_user
								WHERE id = in_id;						 
								RETURN out_name;
							END;
							-- 调用存储函数
							SELECT func_user(1);
							SELECT func_user(2);
							
					删除存储函数：DROP FUNCTION IF EXISTS func_name;
			
			Mysql的存储过程与存储函数的区别：
				1）存储函数有且只有一个返回值，而存储过程不能有返回值。就是说能不能使用return。
				2）存储函数只能有输入参数，而且不能带in, 而存储过程可以有多个in,out,inout参数。
				3）存储过程中的语句功能更强大，存储过程可以实现很复杂的业务逻辑，而函数有很多限制，如不能在函数中使用insert,update,delete,create等语句；
				   存储函数只完成查询的工作，可接受输入参数并返回一个结果，也就是函数实现的功能针对性比较强。比如：工期计算、价格计算。
				4）存储过程可以调用存储函数。但函数不能调用存储过程。
				5）存储过程一般是作为一个独立的部分来执行(call调用)。而函数可以作为查询语句的一个部分来调用。
		
		Mysql系统内置函数：
			字符串函数：
				SELECT CONCAT_WS('-','1st','2nd','3rd'),CONCAT_WS('-','1st',NULL,'3rd'); 					--字符串拼接，结果为：1st-2nd-3rd，1st-3rd；
				SELECT INSERT('warWalf',2,2,'BB'); 															--替换，结果为：wBBWalf
				SELECT LPAD('hello',4,'??'),RPAD('hello',10,'??'); 											--填充，结果为：hell，hello?????
				SELECT TRIM(' book ') `trim`; 																--去除两端空格，结果为：book
				SELECT TRIM('xy' FROM 'xyxboxyokxxyxy');																	结果为：xboxyokx
				SELECT REPEAT('str',3); 																	--重复，结果为：strstrstr
				SELECT STRCMP('txt','txt2') ,STRCMP('txt2','txt'),STRCMP('txt','txt'); 						--比较字符串大小，返回-1或1或0，结果为：-1,1,0
				SELECT LOCATE('ball','football'),POSITION('ball' IN 'football') ,INSTR('football','ball');  --查找位置，结果为：5,5,5
				SELECT ELT(3,'1st','2nd','3rd'),ELT(3,'net','os'); 											--查找字符串，结果为：3rd，null
				SELECT FIELD('hi','hihi','hey','hi','bas') AS col1,
					   FIELD('hi','hihi','lo','hilo','foo') AS col2; 									    --查找在字符数组中的位置[返回索引]，结果为：3,0
				SELECT FIND_IN_SET('hi','hihi,hey,hi,bas'); 												--查找在字符数组中的位置[返回索引]，结果为：3
				SELECT MAKE_SET(1,'a','b','c') AS col1,
				       MAKE_SET(1|4,'hello','nice','world') AS col2; 										--二进制返回的字符串，结果为：a hello,world
				
			数值函数：
				SELECT TRUNCATE(1.32,1); 																	--截取小数后一位，结果为：1.3
				SELECT MOD(31,8);																			--取余，结果为：7
				SELECT ROUND(1.36,1);																		--四舍五入，结果为：1.4
				SELECT HEX('this is a test str'); 															--十六进制表示，结果为：746869732069732061207465737420737472
				CHAR_LENGTH()																				--字符长度，对于char型的字符串会去除左右的空格
				LENGTH()																					--字符长度
			
			日期时间函数：
				SELECT NOW(),CURDATE(),CURRENT_DATE(),CURRENT_TIMESTAMP(),LOCALTIME(),SYSDATE(); 		    --2017-08-07 14:14:49 2017-08-07 2017-08-07 2017-08-07 14:14:49 2017-08-07 14:14:49 2017-08-07 14:14:49
				SELECT UTC_DATE(),UTC_TIME();																--2019-07-04 02:50:56
				SELECT MONTHNAME('2013-8-2')；																--获取月份；August
				SELECT QUARTER('11-04-01'); 																--返回季度 1~4
				SELECT MINUTE('11-02-03 10:10:06'); 														--分钟
				SELECT SECOND('10:23:10'); 																	--秒数
				SELECT DAYNAME('2013-2-3'); 																--返回星期；Sunday 
				SELECT EXTRACT(YEAR FROM '2013-2-3'); 														--返回年数 2013
				SELECT TIME_TO_SEC('23:22:00'); 															--转换为秒数；转换公式为：小时*3600+分钟*60+秒  84120
				SELECT SEC_TO_TIME('84120'); 																--转换为时间；23:22:00
				DATE_FORMAT()																				--where 表名.字段 = DATE_FORMAT(NOW(),'%Y-%m-%d'); 今日
				WEEKOFYEAR() 																				--表示一年中的第几周;WHERE WEEKOFYEAR( 表名.时间字段（如：create_time）) = WEEKOFYEAR(NOW());本周内;
			
			日期计算：
				SELECT DATE_ADD('2013-2-3',INTERVAL 1 MONTH); 												--增加日期：2013-03-03 
				SELECT ADDDATE('2013-2-3',INTERVAL 1 WEEK); 												--增加日期：2013-02-10 
				SELECT DATE_SUB('2013-2-3',INTERVAL 1 WEEK); 												--减少日期：2013-01-27
				SELECT SUBDATE('2013-2-3',INTERVAL 1 WEEK); 												--减少日期：2013-01-27
				SELECT ADDTIME('2013-2-3 01:05:06','10:50:20'); 											--增加时间：2013-02-03 11:55:26
				SELECT SUBTIME('2013-2-3 01:05:06','10:50:20'); 											--减少时间：2013-02-02 14:14:46
				SELECT DATEDIFF('2008-12-30','2007-12-29') AS DiffDate；									--时间差,返回天数；367
				SELECT LAST_DAY('2003-02-05');  															--每月最后一天：2003-02-28
				SELECT DATE_FORMAT(NOW(),'%Y-%m-%d %H:%i:%s');												--格式化日期：2017-08-07 10:20:11
				SELECT TIME_FORMAT('100:00:00', '%H %k %h %I %l'); 											--格式化时间 '%H %k %h %I %l' 表示小时
				SELECT DATE_FORMAT(NOW(),GET_FORMAT(TIMESTAMP,'ISO')); 										--格式化日期 2017-08-07 10:36:35 
				
				--今天
				SELECT DATE_FORMAT(NOW(),'%Y-%m-%d 00:00:00') AS '今天开始';
				SELECT DATE_FORMAT(NOW(),'%Y-%m-%d 23:59:59') AS '今天结束';

				--昨天
				SELECT DATE_FORMAT( DATE_SUB(CURDATE(), INTERVAL 1 DAY), '%Y-%m-%d 00:00:00') AS '昨天开始';
				SELECT DATE_FORMAT( DATE_SUB(CURDATE(), INTERVAL 1 DAY), '%Y-%m-%d 23:59:59') AS '昨天结束';

				--上周
				SELECT DATE_FORMAT( DATE_SUB( DATE_SUB(CURDATE(), INTERVAL WEEKDAY(CURDATE()) DAY), INTERVAL 1 WEEK), '%Y-%m-%d 00:00:00') AS '上周一';
				SELECT DATE_FORMAT( SUBDATE(CURDATE(), WEEKDAY(CURDATE()) + 1), '%Y-%m-%d 23:59:59') AS '上周末';

				--本周
				SELECT DATE_FORMAT( SUBDATE(CURDATE(),DATE_FORMAT(CURDATE(),'%w')-1), '%Y-%m-%d 00:00:00') AS '本周一';
				SELECT DATE_FORMAT( SUBDATE(CURDATE(),DATE_FORMAT(CURDATE(),'%w')-7), '%Y-%m-%d 23:59:59') AS '本周末';
				--上面的本周算法会有问题,因为mysql是按照周日为一周第一天，如果当前是周日的话,会把时间定为到下一周
				SELECT DATE_FORMAT( DATE_SUB(CURDATE(), INTERVAL WEEKDAY(CURDATE()) DAY), '%Y-%m-%d 00:00:00') AS '本周一';
				SELECT DATE_FORMAT( DATE_ADD(SUBDATE(CURDATE(), WEEKDAY(CURDATE())), INTERVAL 6 DAY), '%Y-%m-%d 23:59:59') AS '本周末';

				--上月
				SELECT DATE_FORMAT( DATE_SUB(CURDATE(), INTERVAL 1 MONTH), '%Y-%m-01 00:00:00') AS '上月初';
				SELECT DATE_FORMAT( LAST_DAY(DATE_SUB(CURDATE(), INTERVAL 1 MONTH)), '%Y-%m-%d 23:59:59') AS '上月末';

				--本月
				SELECT DATE_FORMAT( CURDATE(), '%Y-%m-01 00:00:00') AS '本月初';
				SELECT DATE_FORMAT( LAST_DAY(CURDATE()), '%Y-%m-%d 23:59:59') AS '本月末';	
				
				--示例
				SELECT * FROM test_user WHERE TO_DAYS(create_time) = TO_DAYS(NOW()); 										--当天数据
				SELECT * FROM test_user WHERE testuser.create_time LIKE CONCAT('%',DATE_FORMAT(NOW(),'%Y-%m-%d'),'%'); 		--当天数据
				SELECT * FROM test_user WHERE TO_DAYS(NOW()) - TO_DAYS(create_time) = 1; 									--昨天数据
				SELECT * FROM test_user WHERE create_time > DATE_SUB(NOW(), INTERVAL 7 DAY) AND create_time <= NOW(); 		--近7天
				SELECT * FROM test_user WHERE create_time > DATE_SUB(NOW(), INTERVAL 30 DAY) AND create_time <= NOW(); 		--近30天
				SELECT * FROM test_user WHERE WEEKOFYEAR(create_time) = WEEKOFYEAR(NOW()); 									--当前这周数据(不包括上周日)
				SELECT * FROM test_user WHERE YEARWEEK(DATE_FORMAT(create_time,'%Y-%m-%d')) = YEARWEEK(NOW()); 				--当前这周数据(包括上周日)
				SELECT * FROM test_user WHERE YEARWEEK(DATE_FORMAT(create_time,'%Y-%m-%d')) = YEARWEEK(NOW())-1; 			--上一周数据
				SELECT * FROM test_user WHERE WEEKOFYEAR(create_time) = WEEKOFYEAR(NOW())-1; 								--上一周数据
				SELECT * FROM test_user WHERE DATE_FORMAT(create_time, '%Y%m') = DATE_FORMAT(CURDATE(), '%Y%m'); 			--本月数据
				SELECT * FROM test_user WHERE PERIOD_DIFF( DATE_FORMAT(NOW(),'%Y%m'),DATE_FORMAT(create_time,'%Y%m')) =1; 	--上月数据
				SELECT * FROM test_user WHERE QUARTER(create_time)= QUARTER(NOW()); 										--本季度的数据
				SELECT * FROM test_user WHERE QUARTER(create_time)= QUARTER(DATE_SUB(NOW(),INTERVAL 1 QUARTER)); 			--上季度的数据
				SELECT * FROM test_user WHERE QUARTER(create_time)= QUARTER(DATE_ADD(NOW(),INTERVAL 1 QUARTER)); 			--下季度的数据
				SELECT * FROM test_user WHERE YEAR(create_time)=YEAR(NOW()); 												--今年的数据
				SELECT * FROM test_user WHERE YEAR(create_time)=YEAR(DATE_SUB(NOW(),INTERVAL 1 YEAR)); 						--去年的数据
				SELECT * FROM test_user WHERE create_time BETWEEN DATE_SUB(NOW(),INTERVAL 3 MONTH) AND NOW(); 				--距现在3个月的数据
				
			条件判断函数：
				SELECT IF(3>2,2,3); 										--返回2
				SELECT IFNULL(1,2),IFNULL(NULL,10); 						--假如V1不为NULL，则IFNULL(V1,V2)的返回值为v1；否则其返回值为v2
			
				# CASE函数
				# 语法一：
				#	SELECT 
				#	    表字段,
				#	CASE
				#	    WHEN （Bollean值）条件1 THEN 结果表达式1
				#	    WHEN （Bollean值）条件2 THEN 结果表达式2
				#	    ELSE 结果表达式3 END  
				#	FROM 表名
				#

				# 语法二：
				#	SELECT 
				#	    表字段1,
				#	CASE 表字段2
				#	    WHEN 值1 THEN 结果表达式1
				#	    WHEN 值2 THEN 结果表达式2
				#	    ELSE 结果表达式3  END 
				#	FROM 表名
			
			MySQL系统信息函数：
				SELECT VERSION(),CONNECTION_ID();													--返回MySQL 的版本、链接数（ID）
				SHOW PROCESSLIST; 																	--查看线程，只显示100个
				SHOW FULL PROCESSLIST; 																--包括所有线程
				SELECT DATABASE(),SCHEMA(); 														--当前数据库
				SELECT USER(),CURRENT_USER(),SYSTEM_USER(),SESSION_USER();							--用户名
				SELECT CHARSET('abc') ,CHARSET(CONVERT('abc' USING latin1)),CHARSET(VERSION()); 	--字符集
				SELECT COLLATION(_latin2 'abc'),COLLATION(CONVERT('abc' USING utf8)); 				--字符集排列方式 latin2_general_ci utf8_general_ci
			 
			加密函数：
			SELECT PASSWORD('NEWPWD'); 																--*067906D546600BF74D1435B72BDD12D45421DD17
			SELECT MD5('123'); 																		--(32位十六进制数字组成)202cb962ac59075b964b07152d234b70			 
			#加密
			SELECT ENCODE('uetec','123'); 															--'123'密码 'uetec'加密字符串，结果为：��WJ 加密后乱码			
			#解密
			SELECT DECODE(ENCODE('uetec','123'),'123'); 											--uetec
			
			SELECT FORMAT(12332.123465,4); 				 											--格式化函数,数字格式化,进行四舍五入，结果为：12,332.1235			
			SELECT CONV('a',16,2); 																	--进制转换，将十六进制的a转换为二进制表示的数值，结果为：1010 
			
			SELECT INET_ATON('192.168.1.200'); 														--3232235976，IP地址与数字相互转换的函数，优化存储空间
			SELECT INET_NTOA('3232235976'); 														--192.168.1.200
			SHOW VARIABLES LIKE 'character_set_%';  												--查看当前MySQL使用的字符集
			SELECT  CAST(100 AS CHAR(2)),CONVERT('2013-8-9 12:12:12',TIME);						    --数据类型的转换
		
----------------------------------------------Linux运维相关知识---------------------------------------------------------------------									
							
	方向：运维工程师、嵌入式工程师
	应用领域：1、个人桌面应用；2、服务器（免费、稳定、高效）；3、嵌入式领域（内核小，对网络有良好的支持，成本小，物联网运用广泛）
	一、基础篇
			含义：是一款操作系统，免费、开源、安全、高效、稳定，处理高定发十分强悍
			创始人：Linus 林纳斯
			发行版本：redhat（centos、redhat）、ubuntu、suse 、红旗（Fedora）
			Linux与Unix关系：GUN计划Linux内核
			与windows的比较：
						Linux  										windows
				收费：  费用低											高
				维护：   开源，全球爱好者提供技术支持			      微软
				安全性：  高                                          低，补丁  
				使用习惯： 命令行                                       图形界面
				定制性：   强                                            弱
				应用场景：  企业级										桌面操作系统
				
		使用：
			学习阶段：安装VM（虚拟机，提供虚拟空间，前提：BIOS开启了对虚拟化设备的支持），再安装Linux系统（centos）
				网络连接（TCP）三种形式：
					桥连接：Linux可以和其它的系统通信，但是可能造成ip冲突。
					NAT（网络地址转换方式）连接: linux可以访问外网，不会造成ip冲突。
					主机模式：linux是一个独立的主机，不能访问外网。
				vmtools工具使用：
					1、在windows与centos之间使用粘贴命令
					2、设立共享文件
					
		Linux目录结构：
			简介：Linux采用的是树状的目录结构，根目录/。在Linux世界，一切皆文件。
			基本目录介绍：
				/bin：命令目录
				/sbin：系统管理员使用的命令目录
				/home：家目录，普通用户家目录，默认以用户名命名
				/root：超级管理员用户目录
				/boot：引导目录，启动时加载的核心文件，如：镜像文件
				/proc：虚拟目录，是系统内存的映射
				/srv：service缩写，当一些服务启动需要读取的数据目录
				/tmp：临时目录
				/dev：设备，类似于windows的设备管理器，以文件形式存储
				/media：媒体目录，会自动识别设备，如：U盘
				/mnt：临时挂载目录
				/opt：安装软件目录，默认为空
				/usr/local：软件安装所在目录，一般是源码安装方式
				/var：可变目录，经常修改的文件，包括日志文件
				/selinux：安全子系统，类似360	
				/etc：配置文件目录
					  /etc/passwd  -用户（user）的配置文件，记录用户的各种信息，每行的含义：用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell，如：zwj:x:503:504::/home/zwj:/bin/bash
					  /etc/shadow -口令的配置文件，每行的含义：登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志
					  /etc/group -组(group)的配置文件，记录 Linux 包含的组的信息，每行含义：组名:口令:组标识号:组内用户列表
					  /etc/inittab -修改系统的运行级别
		远程登录：
			远程登录（安全模拟终端）软件：Xshell、SecureCRT （支持SSH(Secure SHell远程管理协议) SSH1,SSH2（密文传输）都属于TCP（传输控制协议，软件与软件之间的通讯）协议
										  ,区别于IP（计算机与计算机之间的通讯）协议 以及 Microsoft windows平台的TELNET（明文传输）协议），使用前提：Linux操作系统开启了SSHD服务，此服务监听22号端口
			上传和下载的文件：Xftp 基于windows平台强大的ftp、sftp文件传输软件
					ftp:文件传输协议，默认端口20和21，其中20用于传输数据，21用于传输控制信息，sftp：（SSH File Transfer Protocol）安全文件传送协议
			图形化操作Linux软件的工具：Xmanager
		编辑器：
			vi：所有的linux都会内置
			vim：是vi的增强版，以颜色来区分语法是否正确，代码补全等功能
			三种模式：
				正常模式：默认模式
				编辑/插入模式：按i、I、o、O、a、A、r、R，一般按i		
				命令行模式：通过提供相关指令，实现存盘、读取、替换、显示行号、离开vim等一系列动作
				相互转化：					
									按i
					正常模式	---------------->      编辑模式	
							    <----------------
							        按ESC
							
									按：或/
					正常模式	---------------->      命令行模式（wq、q、q!）
							    <----------------		
									按ESC
				快捷键：
					1) 拷贝当前行yy：拷贝当前行向下的 5 行，5yy，并粘贴（p）
					2) 删除当前行dd：删除当前行向下的 5 行 5dd		
					3) 在文件中查找某个单词[命令行下/关键字，回车查找,输入n就是查找下一个]		
					4) 设置文件的行号或取消文件的行号[命令行下:set nu和:set nonu]		
					5) 编辑/etc/profile 文件，使用快捷键到文档的最末行[G]和最首行[gg],注意这些都是在正常模式下执行的。		
					6) 在一个文件中输入"linux" ,然后又撤销这个动作，再正常模式下输入u		
					7) 编辑/etc/profile 文件，并将光标移动到第20行，第一步：显示行号:set nu，第二步：输入20这个数，第三步: 输入 shift+g	
	二、实操篇
		关机/重启：
			halt：直接使用，效果等价于关机；reboot：重启系统；syn：把内存的数据同步到磁盘
			shutdown -h now : 表示立即关机
			shutdown -h 1 : 表示 1 分钟后关机
			shutdown -r now: 立即重启				
			注意：当关机或者重启时，都应该先执行sync指令，把内存的数据写入磁盘，防止数据丢失。				
		用户登录和注销：
			注销：logout 即可注销用户
			切换系统管理员身份：su - 用户名
			注意：logout注销指令在图形运行级别无效，在运行级别3下有效
				运行级别7种：
				0 ：关机                                       -系统停机状态，系统默认运行级别不能设为 0，否则不能正常启动
				1 ：单用户【找回丢失密码】					   -单用户工作状态，root 权限，用于系统维护，禁止远程登陆
				2：多用户状态没有网络服务					   -多用户状态(没有 NFS)，不支持网络
				3：多用户状态有网络服务						   -完全的多用户状态(有 NFS)，登陆后进入控制台命令行模式
				4：系统未使用保留给用户						   -系统未使用，保留
				5：图形界面									   -X11 控制台，登陆后进入图形 GUI 模式
				6：系统重启									   -系统正常关闭并重启，默认运行级别不能设为 6，否则不能正常启动
				常用运行级别是 3 和 5 ，要修改默认的运行级别可改文件/etc/inittab中的 id:5:initdefault:这一行中的数字
				切换运行级别：
					init [012356]，如：init 5
				如何找回密码？（以root账户为例）
				思路： 进入到单用户模式，然后修改 root 密码。因为进入单用户模式，root 不需要密码就可以登录
				总结：开机->在引导时输入 回车键-> 看到一个界面输入 e -> 看到一个新的界面，选中第二行（编辑内核）在输入 e-> 在这行最后输入1 ,再输入 回车键->再次输入 b ,这时就会进入到单用户模式。
					  这时，我们就进入到单用户模式，使用passwd 指令来修改 root 密码。					  
		用户管理：
			说明：Linux系统是一个多用户多任务的操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统
				  Linux的用户需要至少要属于一个组  
			添加用户：
				useradd [选项] 用户名，如：useradd xm
					当创建用户成功后，会自动的创建和用户同名的家目录，也可以通过 useradd -d 指定目录，如：useradd -d /home/xm1 xm
					指定用户组：useradd -g 用户组 用户名，如：useradd -g xiaoming xm
					修改用户组：usermod -g 用户组 用户名，如：usermod -g xiaoqiang xm 
		    修改用户密码：
				passwd 用户名，如：passwd xm
			删除用户：
				userdel [选项] 用户名，如：userdel xm（保留家目录），userdel -r xm（删除家目录）
			查询用户信息：
				id 用户名，如：id root（会显示uid、gid、组名）
			切换用户：
				在操作Linux中，如果当前用户的权限不够，可以通过 su - 指令，切换到高权限用户
				su – 切换用户名，如：su – xf
				注意：从权限高的用户切换到权限低的用户，不需要输入密码，反之需要；当需要返回到原来用户时，使用 exit 指令
			用户组：
				类似于角色，系统可以对有共性的多个用户进行统一的管理
				增加组：groupadd 组名，如：groupadd wudang
				删除组：groupdel 组名，如：groupdel wudang
				
		帮助指令：
			man [指令或配置文件] -获得帮助信息，如：man ls
			help 指令 -获得shell内置命令的帮助信息，如：help ls
		文件目录：
			pwd：显示当前工作目录的绝对路径
			ls [选项] [目录或是文件]: list缩写，如：ls -al /etc
				常用选项
				-a ：显示当前目录所有的文件和目录，包括隐藏的。
				-l ：以列表的方式显示信息	
			cd [参数] -切换到指定目录，参数：可以是绝对路径和相对路径
			  cd ~ 或 cd 回到家目录
			  cd .. 回到上一级目录
			创建目录（make directory）：
				mkdir [选项] 要创建的目录，如：mkdir -p /home/xm/student
					常用选项
					-p ：创建多级目录
			删除目录（remove directory）：	
				rmdir [选项] 要删除的空目录，如：rmdir /home/xm/student
				提示：如果需要删除非空目录，需要使用 rm -rf 要删除的目录
			创建空文件：
				touch 文件名称1 文件名称2，如：touch 1.txt 2.txt
			拷贝文件：
				cp [选项] source dest，如：cp -r /home/test /home/zwj
				常用参数
				-r ：递归复制整个文件夹
				注意：原路径与目标路径存在相同文件时，强制覆盖不提示的方法：\cp
			删除文件/目录：
				rm [选项] 要删除的文件或目录，如：rm -rf /home/test
				常用参数
				-r ：递归删除整个文件夹
				-f ：强制删除不提示
			重命名/移动文件或目录：
				mv oldNameFile newNameFile -重命名
				mv /sourceFolder /targetFolder -移动文件或目录
			查看文件：
				cat [选项] 要查看的文件，如：cat -n /etc/profile
				常用选项
				-n ：显示行号
				cat 文件名 | more [分页浏览]  -注意：cat只能浏览文件，而不能修改文件
				
				more 要查看的文件 -基于 VI 编辑器的文本过滤器，它以全屏幕的方式按页显示文本文件的内容，more
					 指令中内置了若干快捷键，如：more /etc/profile
					快捷键：
						空白键（space）   -向下翻一页
						Enter             -向下翻一行
						q                 -离开
						ctrl+f            -向下滚动一屏
						ctrl+b            -向上滚动一屏
						=                 -输出当前行号
						:f                -输出文件名和当前行号
	
				less 指令用来分屏查看文件内容，less 指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示
					 需要加载内容，对于显示大型文件具有较高的效率。				 
					 less 要查看的文件，如：less /opt/金庸-射雕英雄传精校版.txt
					 less也指令中内置了若干快捷键
					快捷键：
						空白键（space）/pagedown   -向下翻一页
						pageup   				   -向上翻一页
						q                          -离开
						/字符串                    -向下搜索字符串，n向下查找；N向上查找
						?字符串                    -向上搜索字符串，n向上查找；N向下查找
						
				head 显示文件的开头部分内容，默认情况下 head 指令显示文件的前 10 行内容，如：head -n 5 /etc/profile
					 head 文件        -查看文件头10行内容
					 head -n 5 文件   -查看文件头5行内容
					 
				tail 用于输出文件中尾部的内容，默认情况下 tail 指令显示文件的后 10 行内容
					 tail 文件        -查看文件尾10行内容
					 tail -n 5 文件   -查看文件尾5行内容
					 tail -f 文件     -实时追踪该文档的所有更新，工作中使用多				
				
			输出重定向/追加：
				>输出重定向: 会将原来的文件的内容覆盖
				>>追加：不会覆盖原来文件的内容，而是追加到文件的尾部
				如：ls -l >文件，如：ls -l > a.txt , 将 ls -l 的显示的内容覆盖写入到 a.txt 文件，如果该文件不存在，就创建该文件
					ls -al >>文件，如：ls -l >> aa.txt , 列表的内容追加到文件 aa.txt 的末尾
					cat 文件1 > 文件2 （功能描述：将文件1的内容覆盖到文件2）	
			输出指令：
				echo [选项] [输出内容]，如：echo $PATH
			
			软连接/符号链接（link缩写）：
				类似于 windows 里的快捷方式，主要存放了链接其他文件的路径
				ln -s [原文件或目录] [软链接名]       -给原文件创建一个软链接，如：ln -s /root liknToRoot，注意：删除软链接时，rm -rf liknToRoot（后面不需要/）
				当我们使用 pwd 指令查看目录时，仍然看到的是软链接所在目录
			
			执行过的历史命令：
				history：查看已经执行过历史命令，也可以执行历史指令，如：history 10        -最近执行的10条命令
						 执行历史命令：先找到对应命令编号，然后！编号，如：!178
		
		时间日期：			 
			date：显示当前日期
				date +%Y 					显示当前年份
				date +%m 					显示当前月份
				date +%d 					显示当前是哪一天
				date "+%Y-%m-%d %H:%M:%S"   显示年月日时分秒 
			date：设置日期
			date -s 字符串时间，如：date -s 2018-10-10 11:22:22
			
		日历：
			cal [选项] 不加选项，显示本月日历
			cal 2020 显示2020年日历
	
		查找：
			find：从指定目录向下递归地遍历其各个子目录，将满足条件的文件或者目录显示在终端
			find [搜索范围] [选项] 搜索的文件或目录
			选项：
				-name				按文件名
				-user				按文件所属用户
				-size				按文件大小
			如：find /home -name 1.txt
				find /home -size +1M  （+大于，-小于）
		
		快速定位文件路径：
			locate 指令利用事先建立的系统中所有文件名称及路径的locate 数据库实现快速定位给定的文件。
				   Locate 指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新 locate 时刻 
			locate 搜索文件
			注意：由于 locate 指令基于数据库进行查询，所以第一次运行前，必须使用 updatedb 指令创建 locate 数据库
			如：updatedb 
				locate 1.txt            -返回文件路径
		
		过滤查找与管道符|：
			grep 过滤查找，管道符“|”，表示将前一个命令的处理结果输出传递给后面的命令处理
			grep [选项] 查找内容 源文件
			常用选项
				-n 显示匹配行号
				-i 忽略大小写
			如：cat 1.txt | grep -n yes          -区分大小写
			
		压缩与解压：
			gzip/gunzip：
				gzip 文件                        -压缩文件，只能将文件压缩为*.gz 文件，不会保留源文件，如：gzip 1.txt
				gunzip 文件.gz                   -解压文件
			
			zip/unzip：
				zip [选项] XXX.zip 目录路径              -压缩文件，压缩文件和目录，如：zip -r package.zip /home/
				常用参数：
					-r 递归压缩，即压缩目录
				unzip [选项] XXX.zip 			 		 -解压文件，如：unzip -d /opt/tmp/ package.zip 
				常用参数：
					-d<目录> ：指定解压后文件的存放目录
			
			tar：打包指令[重要]，最后打包后的文件是 .tar.gz 的文件
				tar [选项] XXX.tar.gz 打包的内容（目录或文件） -打包目录，压缩后的文件格式.tar.gz
				常用参数：
					-c 产生.tar打包文件
					-v 详细信息
					-f 指定压缩后的文件名
					-z 打包同时压缩
					-x 解压.tar打包文件
			
				打包并压缩：如：tar -zcvf package.tar.gz 1.txt 2.txt	
				解压文件：如：tar -zxvf package.tar.gz -C /opt/               -C指定解压路径
			
		组管理与权限管理
			在 linux 中的每个用户必须属于一个组，不能独立于组外。在 linux 中每个文件有所有者、所在组、其它组的概念
			文件或目录所有者：一般为文件的创建者,谁创建了该文件，就自然的成为该文件的所有者
				查看所有者：ls -ahl
				修改所有者：chown 用户名 文件名，如：chown -R tom kkk/	
							chown newowner:newgroup 文件名
							-R 如果是目录，则使其下所有子文件或目录递归生效
			
			文件或目录所在组：当某个用户创建了一个文件后，默认这个文件的所在组就是该用户所在的组
				查看所在组：ls -ahl
				修改所在组：chgrp 组名 文件名，如：chgrp -R police kkk/		
							-R 如果是目录，则使其下所有子文件或目录递归生效
			
			文件或目录其它组：除文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组
			
			改变用户所在的组：
				在添加用户时，可以指定将该用户添加到哪个组中，同样的用 root 的管理权限可以改变某个用户所在的组
				usermod -g 组名 用户名
				usermod -d 目录名 用户名        -改变该用户登陆的初始目录
				
			文件或目录权限：
				ls -l 中显示的内容如下：
				-rwxrw-r-- 1 root root 1213 Feb 2 09:39 abc
				0-9 位说明
				1)第 0 位确定文件类型(d[目录], -[普通文件] , l[软链接] , c[字符设备，鼠标、键盘等] , b[块文件，硬盘])
				2)第 1-3 位确定所有者（该文件的所有者）拥有该文件的权限。---User
				3)第 4-6 位确定所属组（同用户组的）拥有该文件的权限，---Group
				4)第 7-9 位确定其他用户拥有该文件的权限 ---Other
				后面的字符含义：
					第一位：如果是文件，表示硬链接数，如果是目录表示子目录数
					第二位：所有者
					第三位：所在组
					第四位：文件大小，如果是目录是4096
					第五位：时间，最后修改时间
					最后一位：文件名
			
				权限解析：
					rwx 作用到文件
					1) [ r ]代表可读(read): 可以读取,查看
					2) [ w ]代表可写(write): 可以修改,但是不代表可以删除该文件,删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.
					3) [ x ]代表可执行(execute):可以被执行
					rwx 作用到目录
					1) [ r ]代表可读(read): 可以读取，ls 查看目录内容
					2) [ w ]代表可写(write): 可以修改,目录内创建+删除+重命名目录
					3) [ x ]代表可执行(execute):可以进入该目录
				
				修改权限：
					chmod 指令，可以修改文件或者目录的权限
					变更权限方式一：+ 、-、= 变更权限
						u:所有者g:所有组o:其他人 a:所有人(u、g、o 的总和)	
						chmod u=rwx,g=rx,o=x 文件目录名
						chmod o+w 文件目录名
						chmod a-x 文件目录名
					变更权限方式二：利用数字变更权限
						规则：r=4 w=2 x=1,rwx=4+2+1=7
						chmod u=rwx,g=rx,o=x 文件目录名 相当于 chmod 751 文件目录名
			
			crond 定时任务调度：
				简单任务：直接在crontab中加入任务即可
				负责任务：写脚本（Shell编程）
				
				任务调度：是指系统在某个时间执行的特定的命令或程序
				任务调度分类：1.系统工作：有些重要的工作必须周而复始地执行。如病毒扫描等
							  2.个别用户工作：个别用户可能希望执行某些程序，比如对 mysql 数据库的备份
				
				crontab [选项]
				常用选项
					-e 编辑当前用户crontab任务
					-l 查询当前用户所有的crontab任务
					-r 删除当前用户所有的crontab任务
				重启任务调度：service crond restart
			    如：
					设置任务调度文件：/etc/crontab
					设置个人任务调度，执行 crontab –e 命令，接着输入任务到调度文件，如：*/1 * * * * ls –l /etc/ > /tmp/to.txt
					意思说每小时的每分钟执行ls –l /etc/ > /tmp/to.txt 命令
				
				cron表达式：注意与Java的cron表达式的区别，它是由5位占位符组成的，从分钟开始				
				    分钟 小时 天 月 星期
				
				简单脚本运用：
					每隔 1 分钟，就将当前的日期信息，追加到/tmp/mydate文件中
					1) 先编写一个文件 /home/mytask.sh，添加内容：date >> /tmp/mydate
					2) 给 mytask.sh 一个可以执行权限，chmod 744 /home/mytask.sh
					3) crontab -e，添加：*/1 * * * * /home/mytask.sh
					4) 成功
			
		Linux 磁盘分区、挂载
			分区基础知识：
				MBR分区:
				1.最多支持四个主分区
				2.系统只能安装在主分区
				3.扩展分区要占一个主分区
				4.MBR 最大只支持 2TB，但拥有最好的兼容性
				GTP分区：
				1.支持无限多个主分区（但操作系统可能限制，比如 windows 下最多 128 个分区）
				2.最大支持 18EB 的大容量（1EB=1024 PB，1PB=1024 TB ）
				3.windows7 64 位以后支持 gtp
			原理介绍：
				Linux 采用了一种叫“载入”的处理方法，将一个分区和linux的一个目录联系起来，这时要载入的一个分区将使它的存储空间在一个目录下获得
			硬盘介绍：
				1)Linux 硬盘分 IDE 硬盘和 SCSI 硬盘，目前基本上是 SCSI 硬盘
				2)对于 IDE 硬盘，驱动器标识符为“hdx~”,其中“hd”表明分区所在设备的类型，这里是指 IDE 硬
				  盘了。“x”为盘号（a 为基本盘，b 为基本从属盘，c 为辅助主盘，d 为辅助从属盘）,“~”代表分区，
				  前四个分区用数字 1 到 4 表示，它们是主分区或扩展分区，从 5 开始就是逻辑分区。例，hda3 表示为
				  第一个 IDE 硬盘上的第三个主分区或扩展分区,hdb2 表示为第二个 IDE 硬盘上的第二个主分区或扩展
				  分区。
				3)对于 SCSI 硬盘则标识为“sdx~”，SCSI 硬盘是用“sd”来表示分区所在设备的类型的，其余则
				  和 IDE 硬盘的表示方法一样。
			
			查看磁盘分区：
				lsblk -f 查看当前系统的分区情况，查看各分区大小：lsblk
				经典案列：如何挂载一块新的硬盘？
				主要步骤：
					1、虚拟机添加一块硬盘
					2、分区，指令：fdisk /dev/sdb         -初始化
					3、格式化：mkfs -t ext4 /dev/sdb1 
					4、设置挂载：mount 设备名称（分区） 目录名称 mount /dev/sdb1 /home/newdisk
					5、取消挂载：umount 设备名称或目录名称 umount /dev/sdb1
					6、永久挂载：编辑/etc/fstab文件，添加一行/dev/sdb1 /home/newdisk ext4 defaults 0 0，再执行mount -a 计时生效
			硬盘健康情况查询：
				查看系统磁盘使用情况：df -lh
				指定目录磁盘使用情况：默认为当前目录
					du -h /目录
					参数：-s 指定目录占用大小汇总
						  -h 带计量单位
						  -a 含文件 
						  --max-depth=1 子目录深度
						  -c 列出明细的同时，增加汇总值
					
			        如：查询/opt目录的磁盘占用情况，深度为 1，du -ach --max-depth=1 /opt
				常用指令：
					1、查询/home目录下的文件个数：ls -l /home | grep "^-" | wc -l
					2、查询/home目录下的目录个数：ls -l /home | grep "^d" | wc -l
					3、查询/home目录下的文件个数（包括子文件夹）：ls -lR /home | grep "^-" | wc -l
					4、查询/home目录下的目录个数（包括子文件夹）：ls -lR /home | grep "^d" | wc -l	
				目录结构：tree
		
		Linux 网络配置
			查看网络：ifconfig
			查看当下服务器是否连接通目标主机：ping，如： ping www.baidu.com
			测试端口：telnet，如：telnet 192.168.1.175:8080
			获取网络：
				1、自动获取：IP不固定
				2、固定网络：直接修改置文件指定IP,并可以连接到外网，编辑vim /etc/sysconfig/network-scripts/ifcfg-eth0
							 修改后，一定要重启服务（reboot重启系统/service network restart）
							 
			查看系统网络情况：
				netstat [选项]，如：netstat -anp | grep sshd
				选项参数：
					-an 按一定顺序排列输出
					-p	显示哪个进程在调用
		
		进程管理：
			含义：
				1)在 LINUX 中，每个执行的程序（代码）都称为一个进程。每一个进程都分配一个 ID 号
				2)每一个进程，都会对应一个父进程，而这个父进程可以复制多个子进程
				3)每个进程都可能以两种方式存在的。前台与后台，前台进程就是用户目前的屏幕上可以进
				  行操作的。后台进程则是实际在操作，但由于屏幕上无法看到的进程，通常使用后台方式执行
				4)一般系统服务都是以后台进程的方式存在，而且都会常驻在系统中，直到关机才才结束	
			显示系统执行的进程：
				后台运行的进程：
					ps -aux 
					参数说明：-a 终端所有的进程信息；-u 以用户格式显示进程信息； -x 显示后台运行的进程参数
					1)指令：ps –aux|grep xxx
					2)指令说明
						•System V 展示风格
						•USER：用户名称
						•PID：进程号
						•%CPU：进程占用 CPU 的百分比
						•%MEM：进程占用物理内存的百分比
						•VSZ：进程占用的虚拟内存大小（单位：KB）
						•RSS：进程占用的物理内存大小（单位：KB）
						•TTY：终端名称,缩写 .
						•STAT：进程状态，其中 S-睡眠，s-表示是会话的先导进程，N-表示进程拥有比普通优先
							   级更低的优先级，R-正在运行，D-短期等待（不可被唤醒的睡眠状态 , 通常进程可能在等待 I/O 的情况），Z-僵死进程，T-被跟踪或者被停止等等
						•STARTED：进程的启动时间
						•TIME：CPU 时间，即进程使用 CPU 的总时间
						•COMMAND：启动进程所用的命令和参数，如果过长会被截断显示
				
				显示所有进程信息（全格式）：ps -ef 
					参数说明：-e 显示所有进程 -f 全格式
					1)指令：ps –ef|grep xxx
					2)指令说明
						•UID：用户 ID
						•PID：进程 ID
						•PPID：父进程 ID
						•C：CPU 用于计算执行优先级的因子。数值越大，表明进程是 CPU 密集型运算，执行优先级会
							降低；数值越小，表明进程是 I/O 密集型运算，执行优先级会提高
						•STIME：进程启动的时间
						•TTY：完整的终端名称
						•TIME：CPU 时间
						•CMD：启动进程所用的命令和参数
				
					三种风格：
						BSD 风格的参数，前面不加破折线
						Unix 风格的参数，前面加单破折线
						GNU 风格的长参数，前面加双破折线
			终止进程：
				介绍:若是某个进程执行一半需要停止时，或是已消了很大的系统资源时，此时可以考虑停止该进程
				kill [选项] 进程号 			-通过进程号杀死进程 
				killall 进程名称			-通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时使用
				选项：-9:表示强迫进程立即停止
			查看进程树：
				pstree [选项] ,可以更加直观的来看进程信息，如：pstree -p
				常用选项：
				-p :显示进程的 PID
				-u :显示进程的所属用户
			
			动态监控进程：
				指令：Top，Top 与 ps 最大的不同之处，在于 top 在执行一段时间可以更新正在运行的的进程
				top [选项]
				选项参数：
					-d 秒数 								-每隔几秒后更新，默认3秒
					-i										-不显示僵死或闲置进程
					-p 进程号                               -指定进程ID，显示进程信息
				交互说明：
					P（以CPU使用率排序） M（以内存使用率排序） N（以PID排序） q（退出top）
				如：监视特定用户：top 回车 输入u ，再输入用户名
				    终止进程：top 回车 输入k ，再输入PID
				   
				
		服务管理：
			含义：本质就是进程，但是是运行在后台的，通常都会监听某个端口，等待其它程序的请求，比如(mysql,sshd、防火墙等)，因此又称为守护进程。
			service 服务名 [start | stop | restart | reload | status]
			注：在 CentOS7.0 后，不再使用service ,而是 systemctl
			如：service iptables start   			 -启动防火墙
			
			服务自动启或服务永久关闭：ckconfig
				查看服务名：
					1、指令： setup -> 系统服务 就可以看到系统服务
					2、/etc/init.d/服务名称，如： ls -l /etc/init.d/
					
				Linux系统开机流程：开机	-> BIOS ->/boot ->init进程名 ->运行级别 ->运行对应的服务
				
				chkconfig 指令：可以给每个服务的各个运行级别设置自启动/关闭，语法：chkconfig [服务名] --list|grep xxx
					chkconfig --level 5 服务名 on/off，如：chkconfig --level 5 sshd off
														   chkconfig [--level 5] iptables off
					
				注意：chkconfig重新设置服务后自启动或关闭，需要重启机器 reboot 才能生效
		防火墙：
			查看所有开放的端口：firewall-cmd --zone=public --list-ports
			添加：				firewall-cmd --zone=public --add-port=80/tcp --permanent    （--permanent永久生效，没有此参数重启后失效）			
			重新载入：			firewall-cmd --reload
			查看：				firewall-cmd --zone=public --query-port=80/tcp
			删除：				firewall-cmd --zone=public --remove-port=80/tcp --permanent 
			批量开放端口：
				firewall-cmd --permanent --zone=public --add-port=100-500/tcp
				firewall-cmd --permanent --zone=public --add-port=100-500/udp
				firewall-cmd --reload
					
		Linux包管理：
			rpm：一种用于互联网下载包的打包及安装工具，，它包含在某些 Linux 分发版中，它生成具有.RPM扩展名的文件。RPM 是 RedHat Package Manager（RedHat 软件包管理工具）的缩写，类似 windows
				 的 setup.exe，Linux 的分发版本都有采用（suse,redhat, centos 等等），可以算是公认的行业标准了
			    查询指令：
					查询已安装的 rpm 列表：rpm –qa|grep xx，如：rpm –qa|grep firefox
					rpm -qa :查询所安装的所有 rpm 软件包
					rpm -qa | more [分页显示]
					rpm -qa | grep X [rpm -qa | grep firefox ]
					rpm -q 软件包名 :查询软件包是否安装，如：rpm -q firefox					
					rpm -qi 软件包名 ：查询软件包信息
					rpm -ql 软件包名 :查询软件包中的文件，如：rpm -ql firefox					
					rpm -qf 文件全路径名 查询文件所属的软件包，如：rpm -qf /etc/passwd	rpm -qf /root/install.log
				卸载：
					rpm -e RPM包的名称，如：rpm -e firefox
					注意：1) 如果其它软件包依赖于您要卸载的软件包，卸载时则会产生错误信息
						  2) 如果非要卸载，添加参数：--nodeps,就可以强制删除，但是一般不推荐这样做，因为依赖于该软件包的程序可能无法运行，如：rpm -e --nodeps foo
				安装：					
					rpm -ivh RPM包全路径名称，如：rpm -ivh firefox.XXX.rpm
					参数说明：
						i=install 安装
						v=verbose 提示
						h=hash 进度条
			yum：是一个 Shell 前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载 RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，使用 yum 的前提是可以联网		
				基本指令：
					查询yum服务器是否有需要安装的软件：yum list|grep xx，如：yum list|grep firefox 
					安装指定的yum包：yum install xxx，如：yum install firefox
					
	三、搭建：JavaEE环境：
			JDK：配置环境变量，vim /etc/profile
				 JAVA_HOME=/opt/jdk1.7.0_79
				 PATH=/opt/jdk1.7.0_79/bin:$PATH
				 export JAVA_HOME PATH
			注意：需要注销用户，环境变量才能生效！	 
			
			Tomcat：解压[tar -zxvf apache-tomcat.tar.gz] -> 启动：bin/下执行.startup.sh
			注意：需要开放端口，外部才能访问！
				修改防火墙开放端口[centos7.0以下的版本]：vim /etc/sysconfig/iptables，
				添加一行：-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 8080:9000 -j ACCEPT，
				重启防火墙service iptables restart
			
			Eclipse：解压 -> 安装目录执行./eclipse
			
			MySql：略
				
	四、大数据Shell编程：	
		简介：
			1)Linux 运维工程师在进行服务器集群管理时，需要编写Shell程序来进行服务器管理
			2)对于JavaEE和Python程序员来说，工作的需要，要求编写一些Shell脚本进行程序或者是服务器的维护，比如编写一个定时备份数据库的脚本
			3)对于大数据程序员来说，需要编写Shell程序来管理集群
			
			Shell是一个命令行CMD解释器，它为用户提供了一个向Linux内核发送请求以便运行程序的界面系统
			级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序	
		Shell入门：
			脚本要求：
				1) 脚本以#!/bin/bash 开头
				2) 脚本需要有可执行权限+x（如：chmod 744 XX.sh）
				3）单行注释：#
				4）多行注释：：<<!                        !
			执行方式：
				1) 输入脚本的绝对路径或相对路径，如：./1.sh
				2) sh+脚本，不推荐，如： sh 1.sh
				   
		Shell变量：
			变量分为：系统变量和用户自定义变量，系统变量：$HOME、$PWD、$SHELL、$USER 等等，如：echo "home=$HOME"
			显示当前shell中所有变量：set
			
			自定义变量：
				规则：
					1) 变量名称可以由字母、数字和下划线组成，但是不能以数字开头
					2) 等号两侧不能有空格
					3) 变量名称一般习惯为大写
				使用：
				1)定义变量：变量=值，如：A=10 				echo "A=$A"
				2)撤销变量：unset 变量，如：unset A
				3) 声明静态变量：readonly 变量，注意：不能 unset，如：readonly B=100 				echo "B=$B"
				将命令的返回值赋值给变量：
					1）A=`ls -la` 反引号，运行里面的命令，并把结果返回给变量 A
					2）A=$(ls -la) 等价于反引号
			
			环境变量：
				基本语法：
				1) export 变量名=变量值 											-将shell变量输出为环境变量
				2) source 配置文件													-让修改后的配置信息立即生效
				3) echo $变量名														-查询环境变量的值
			    如：配置Tomcat环境变量，vim /etc/profile 添加一行：export TOMCAT_HOME=/opt/tomcat，保存退出
					source /etc/profile 让修改后的配置信息立即生效，使用：echo $TOMCAT_HOME
			
			位置参数变量：
				含义：当执行一个shell脚本时，如果希望获取到命令行的参数信息，就可以使用到位置参数变量，比如 ： ./m1.sh 100 200, 	
					  这个就是一个执行shell 的命令行，可以在 m1.sh脚本中获取到参数信息
				基本语法：
					$n  					-n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，10以上的参数需要用大括号包含，如${10}
					$* 						-*代表命令行中所有的参数，$*把所有的参数看成一个整体
					$@						-@代表命令行中所有的参数，不过$@把每个参数区分对待
					$#						-#代表命令行中所有参数的个数
					
			预定义变量：
				含义： shell设计者事先已经定义好的变量，可以直接在shell脚本中使用
				基本语法：
					$$   				    -当前进程的进程号（PID）
					$!						-后台运行的最后一个进程的进程号（PID）
					$？ 					-最后一次执行的命令的返回状态。如果这个变量的值为 0，证明上一个命令正确执行；
											 如果这个变量的值为非 0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了
					
			运算符：
				1) “$((运算式))”或“$[运算式]”
				2) expr m + n 注意 expr 运算符间要有空格
				3) expr m - n
				4) expr \*, /, % 乘，除，取余
				如：计算：(2+3)*6
					方式一：$(((2+3)*6))
					方式二：$[(2+3)*6]
					方式三：TEMP `expr 2 + 3`
							RESULT=`expr $TEMP \* 6`
			
			条件判断：
				基本语法：
					[ condition ]（注意 condition 前后要有空格）#非空返回 true，可使用$?验证（0 为 true，>1 为 false）
					如：[condition] && echo OK || echo notOK                   -条件满足，执行后面的语句
			判断条件：
				1)比较运算符
					= 字符串比较
					-lt 小于
					-le 小于等于
					-eq 等于
					-gt 大于
					-ge 大于等于
					-ne 不等于
					如：if [ 23 -gt 22 ]
						then
							echo "dayu"
						fi	
				2)按照文件权限进行判断
					-r 有读的权限 [ -r 文件 ]
					-w 有写的权限 [ -w 文件 ]
					-x 有执行的权限 [ -x 文件 ]
				3)按照文件类型进行判断
					-f 文件存在并且是一个常规的文件
					-e 文件存在
					-d 文件存在并是一个目录
					如：if [ -e /home/1.txt ]
						then
							echo "exist"
						fi	
			流程控制：
				if条件判断：
					基本语法：
					if [ 条件判断式 ];then
					程序
					fi
					或者
					if [ 条件判断式 ]
					then
					程序
					elif [ 条件判断式 ]
					then
					程序
					fi
				注意：（1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格 (2) 推荐使用第二种方式
				
				case语句：
					case $变量名 in
					"值 1"）
					如果变量的值等于值 1，则执行程序 1
					;;
					"值 2"）
					如果变量的值等于值 2，则执行程序 2
					;;
					…省略其他分支…
					*）
					如果变量的值都不是以上的值，则执行此程序
					;;
					esac
				
				for循环控制：
					for 变量 in 值 1 值 2 值 3…
					do
					程序
					done
					或者
					for (( 初始值;循环控制条件;变量变化 ))
					do
					程序
					done
				while循环控制：
					while [ 条件判断式 ]
					do
					程序
					done
				
			读取控制台输入：
				read(选项)(参数)
				选项：
					-p：指定读取值时的提示符
					-t：指定读取值时等待的时间（秒），如果没有在指定的时间内输入，就不再等待了
				参数：
					变量：指定读取值的变量名
				如：
					read -t 10 -p "请输入一个数NUM=" NUM
					echo "你输入的值是NUM=$NUM" 
			函数：
				分类：系统函数、自定义函数
				系统函数：
					basename：返回完整路径最后 / 的部分，常用于获取文件名
					basename [pathname] [suffix] 							-basename 命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来，
																			 suffix 为后缀，如果 suffix 被指定了，basename 会将 pathname 或 string 中的 suffix 去掉
					如：basename /home/1.txt								-结果： 1.txt
						basename /home/1.txt .txt  							-结果： 1
	
					dirname：返回完整路径最后 / 的前面的部分，常用于返回路径部分
					dirname 文件绝对路径                                    -从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）
					如：dirname /home/1.txt									-结果： /home
				
				自定义函数：
					[ function ] funname[()]
					{
					Action;
					[return int;]
					}
					调用直接写函数名：funname [值]
					如：计算2个输入参数的和
						function getSum(){
							SUM=$[SN1+$N2]
							echo "参数的和=$SUM"
						}
						
						read -p "请输入一个数N1=" N1
						read -p "请再输入一个数N2=" N2
						
						调用：getSum $N1 $N2
			
			综合案例：
				需求分析（提示：使用crond定时器）
				1)每天凌晨 2:10 备份 数据库 DB 到 /data/backup/db
				2)备份开始和备份结束能够给出相应的提示信息
				3)备份后的文件要求以备份时间为文件名，并打包成 .tar.gz 的形式，比如：2018-03-12_230201.tar.gz
				4) 在备份的同时，检查是否有 10 天前备份的数据库文件，如果有就将其删除
				编写脚本(备份逻辑)：
					vim /usr/sbin/mysql_db_backup.sh
						BACKUP=/data/backup/db
						echo "------------------------开始备份---------------------"
						echo "------------------------备份的路径是：$BACKUP/$DATETIME.tar.gz---------------------"
						#主机
						HOST=localhost
						#用户名
						DB_USER=root
						#密码
						DB_PWD=123456
						#备份数据库的名称
						DATEBASE=DB
						#创建备份的路径，如果备份的路径文件夹存在，就使用，否则则创建
						[ ! -d "$BACKUP/$DATETIME" ] && mkdir -p "$BACKUP/$DATETIME"
						#执行mysql备份数据库的指令
						mysqldump -u${DB_USER} -p${DB_PWD} --host=$HOST $DATEBASE | gzip > $BACKUP/$DATETIME/$DATETIME.tar.gz
						#打包备份文件
						cd $BACKUP
						tar -zcvf $DATETIME.tar.gz $DATETIME
						#删除临时文件
						rm -rf $BACKUP/$DATETIME
						#删除10天前的备份文件
						find $BACKUP -mtime +10 -name "*.tar.gz" -exec rm -rf {} \; 
						echo "------------------------备份文件成功---------------------"
				将脚本添加到crontab配置里：
					10 2 * * * /usr/sbin/mysql_db_backup.sh
					
	五、其他：
			Ubuntu：
				含义：Ubuntu（友帮拓、优般图、乌班图）是一个以桌面应用为主的开源 GNU/Linux 操作系统，Ubuntu是基于 GNU/Linux，支持 x86、amd64（即 x64）和 ppc（Pocket PC，掌上电脑的操作平台） 架构，由全球化的专业开发团队（Canonical Ltd）
					  打造的。专业的 Python 开发者一般会选择 Ubuntu 这款 Linux 系统作为生产平台
				安装：略，支持中文需要安装中文语言包
					  左侧图标栏System Settings（系统设置）->Language Support（语言支持）选项卡 ->Install / Remove Languages ->
					  Chinese(Simplified) -> Apply Changes ->把汉语（中国）拖到第一位 -> 重启
				用户：安装 ubuntu 成功后，都是普通用户权限，并没有最高 root 权限，如果需要使用 root 权限的时候，
					  通常都会在命令前面加上 sudo。使用 su 命令来直接切换到 root 用户的，但是如果没有给 root 设置初始密码，就会抛出 su :
					  Authentication failure 这样的问题
				设置密码：
					  1) 输入 sudo passwd 命令，输入一般用户密码并设定 root 用户密码
					  2) 设定 root 密码成功后，输入 su 命令，并输入刚才设定的 root 密码，就可以切换成 root 了
					     提示符$代表一般用户，提示符#代表 root 用户
					  3) 输入 exit 命令，退出 root 并返回一般用户
				软件管理：
					apt 是 Advanced Packaging Tool 的简称，是一款安装包管理工具
					在 Ubuntu 下，我们可以使用 apt 命令可用于软件包的安装、删除、清理等，类似于 Windows 中的软件管理工具
				使用：
					sudo apt-get update 										-更新镜像源
					sudo apt-get install package 								-安装包
					sudo apt-get remove package 								-删除包
					sudo apt-cache search package 								-搜索软件包
					sudo apt-cache show package									-获取包的相关信息，如说明、大小、版本等
					sudo apt-get install package --reinstall					-重新安装包
					
					sudo apt-get -f install										-修复安装
					sudo apt-get remove package --purge 						-删除包，包括配置文件等
					sudo apt-get build-dep package 								-安装相关的编译环境

					sudo apt-get upgrade 										-更新已安装的包
					sudo apt-get dist-upgrade 									-升级系统
					sudo apt-cache depends package 								-了解使用该包依赖那些包
					sudo apt-cache rdepends package 							-查看该包被哪些包依赖
					sudo apt-get source package									-下载该包的源代码
				
				国内镜像源：https://mirrors.tuna.tsinghua.edu.cn/
				
				备份镜像源：sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup
				
			使用SSH远程登录：
				SSH 为 Secure Shell 的缩写，由 IETF 的网络工作小组（Network Working Group）所制定；SSH 为
				建立在应用层和传输层基础上的安全协议
				SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录，以
				及用户之间进行资料拷贝。几乎所有 UNIX 平台—包括 HP-UX、Linux、AIX、Solaris、Digital UNIX、
				Irix，以及其他平台，都可运行 SSH。
				使用 SSH 服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A 机器想被 B
				机器远程控制，那么，A 机器需要安装 SSH 服务器，B 机器需要安装 SSH 客户端
				和 CentOS 不一样，Ubuntu 默认没有安装 SSHD 服务，因此，我们不能进行远程登录
				
				安装：
					sudo apt-get install openssh-server                         -安装了 SSH 服务端和客户端
					service sshd restart                                        -启动了 sshd 服务，监听端口 22
					
					
				从Linux 客户机连接Linux 服务机：
					基本语法：
					ssh 用户名@IP，如：ssh atguigu@192.168.188.131，使用 ssh 访问，如访问出现错误。可查看是否有该文件 ～/.ssh/known_ssh 尝试删除该文件解决
					登出：exit 或者 logout				
				
----------------------------------------------Nginx WEB服务器的使用----------------------------------------------------------------------------------												
	一、基本介绍：参考：https://www.runoob.com/linux/nginx-install-setup.html
			含义：Nginx("engine x")是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的Web和反向代理服务器，也是一个IMAP/POP3/SMTP邮件代理服务器。
				  在高连接并发的情况下，Nginx是Apache服务器不错的替代品，目前的版本有：稳定版、开发版和历史稳定版。业界使用其产品的公司如：腾讯、新浪
			安装：
				下载地址: http://nginx.org/download/nginx-1.4.2.tar.gz				  
				yum install pcre pcre-devel									-nginx依赖于pcre库,要先安装pcre、pcre-devel
				cd /usr/local/src/
				wget http://nginx.org/download/nginx-1.4.2.tar.gz
				tar -zxvf nginx-1.4.2.tar.gz 								-解压
				cd nginx-1.4.2
				./configure --prefix=/usr/local/nginx						-配置安装路径
				make && make install 										-安装
			启动：
				目录结构：cd /usr/local/nginx，会看到：				
					conf 														-配置文件  
					html 														-网页文件
					logs  														-日志文件 
					sbin  														-主要二进制程序
				启动：./sbin/nginx
				注意：默认端口80，如果被占用，需要停止80端口所对应的程序或服务
					  1）查看端口：netstat -antp | grep 80                      -得到80端口对应的进程ID
					  2）暴力停止：Pkill -9 PID
			信号源控制：
				1）TERM、INT													-快速关闭
				2）QUIT 														-优雅的关闭进程,即等请求结束后再关闭
				3）HUP 															-改变了配置文件,平滑的重读配置文件（新配置替代旧配置）
				4）USR1 														-重读日志,在日志按月/日分割时有用
				5）USR2															-平滑的升级
				6）WINCH														-优雅关闭旧的进程(配合USR2来进行升级)
				具体语法:
					Kill -信号选项 nginx的PPID（主进程ID），如：Kill -HUP 4873
					Kill -信号控制 `cat [安装路径下]/logs/nginx.pid`，如：Kill -USR1 `cat [安装路径下]/logs/nginx.pid`					
		
		附[总结]：
		nginx常用指令：
			启动命令：[安装路径下]/usr/local/nginx/sbin/nginx
			停止命令：[安装路径下]/usr/local/nginx/sbin/nginx -s stop 强制关闭
					  [安装路径下]/usr/local/nginx/sbin/nginx -s quit 安全关闭
			重启命令：[安装路径下]/usr/local/nginx/sbin/nginx -s reload		  
			测试配置文件：[安装路径下]/usr/local/nginx/sbin/nginx -t
			检测是否出错：[安装路径下]/usr/local/nginx/sbin/nginx -c
			查看帮助：[安装路径下]/usr/local/nginx/sbin/nginx -h
			查看进程：ps -ef | grep nginx
			平滑重启：kill -HUP Nginx主进程号
		查看端口：
			netstat -antp |grep 80									-端口是否被占用
			ps -ef | grep 8081 										-进程
			top 
			
	二、配置文件：					
			位置：/usr/local/nginx/conf/nginx.conf
			文件解析：
				全局区：
					#工作的子进程,可以自行修改,但太大无益,因为要争夺CPU,一般设置为 CPU数*核数
					worker_processes 1; 
					
					#是否以守护进程开启
					daemon：默认on 可选值on/off
					
					#是否主进程中开启多个子线程
					master_process：默认on 可选值on/off
					
					#一般配置nginx连接的特性
					Event {					
					 #如1个worker能同时允许多少连接，如下指一个子进程最大允许连1024个连接
					 worker_connections  1024;
					}
					
					#配置http服务器的主要段
					http { 
						 #日志
						 #access_log  logs/access.log  main;
						 #超时时间
						 keepalive_timeout 75;
						 #全局设置
						 location / {
							deny 192.168.1.2;						//拒绝指定客户端访问
							allow 192.168.1.3/300;					//允许指定客户端访问[IP段]
							denyall;
						 }						 
						 #虚拟主机段
						 Server1 {							   
								#定位,把特殊的路径或文件再次定位，如image目录单独处理；.php单独处理
								Location {
								}
								#解析超时
								resolver_timeout 30s;

						 }
						 Server2 {
						 }
					}
					
					如：
						基于域名虚拟主机
						server {
							#监听端口  
							listen 80;
							#监听域名，需要域名映射，在hosts文件中添加映射；支持配置多个域名，使用空格隔开；
							server_name 1xuepai.com; 
							#定位,把特殊的路径或文件再次定位
							#日志
							#access_log  logs/host.access.log  main;
							location / {
									#根目录定位，绝对路径
									root /var/www/1xuepai.com; 
									#alias 别名
									index index.html;
							}
						}
						基于端口虚拟主机
						server {
							#监听端口
							listen 8080;
							#监听端口
							server_name 192.168.1.175;
							location / {
									#根目录定位，绝对路径
									root /var/www/html8080;
									index index.html;
							}
						}
					
					日志管理：
						在nginx的http段与server段,都有类似信息#access_log信息;
						如在server段：#access_log logs/host.access.log  main说明该server段访问日志的文件是logs/host.access.log,
						使用的格式”main”格式。除了main格式，也可以自定义其他格式
						
					格式说明：
						main格式：
							log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
											  '$status $body_bytes_sent "$http_referer" '
											  '"$http_user_agent" "$http_x_forwarded_for"';

							main格式是默认定义好一种日志的格式,并起个名字，便于引用
							main类型的日志,记录的 remote_addr.... http_x_forwarded_for等选项

							1: 日志格式选项说明（以main格式为例）：											
									远程IP - 远程用户/用户时间 请求方法(GET/POST)
									请求状态 请求体body长度 referer来源信息
									http_user_agent用户代理/蜘蛛，被转发的请求的原始IP
									http_x_forwarded_for:在经过代理时,代理把本来IP加在此头信息中,传输你的原始IP

							2: 声明一个log_format并命名：
									log_format mylog '$remote_addr- "$request" '
													 '$status $body_bytes_sent "$http_referer" '
														'"$http_user_agent" "$http_x_forwarded_for"';
						
							3：使用：
								如：在http/server/location,我们就可以引用 mylog 自定义的日志格式
									Nginx允许针对不同的server做不同的Log ，(有的web服务器不支持,如lighttp)
								
								声明log   	log位置         	 log格式;
								access_log logs/access_mylog.log  mylog;   
								
					案例：Shell编程+Crontab定时任务+nginx信号管理，完成日志按日期存储						
						  分析思路: 
							凌晨00:01,把昨天的日志重命名，放在相应的目录下，再USR1信息号控制nginx重新生成新的日志文件							
							
							脚本:
							编辑脚本vim logback.sh
							#!/bin/bash
							base_path='/usr/local/nginx/logs'
							log_path=$(date -d yesterday +"%Y%m")
							day=$(date -d yesterday +"%d")
							mkdir -p $base_path/$log_path
							mv $base_path/access.log $base_path/$log_path/access_$day.log
							#echo $base_path/$log_path/access_$day.log
							kill -USR1 `cat /usr/local/nginx/logs/nginx.pid`  -- 加载日志

							定时任务
							Crontab 编辑定时任务，crontab -e 
							01 00 * * * /xxx/path/logback.sh  每天0时1分(建议在02-04点之间,系统负载小)
							
					定位Location管理[server段]：
						含义：location 有”定位”的意思, 根据Uri来进行不同的定位
							  在虚拟主机的配置中,是必不可少的,location可以把网站的不同部分,定位到不同的处理方式上，比如, 碰到.php，如何调用PHP解释器?就需要location					
						
						基础语法：
							#中括号可以不写任何参数,此时称为一般匹配，也可以写参数	
							location [=|~|~*|^~] patt {
							}							  						
							因此,大类型可以分为3种
							location patt{}  [一般匹配]
							location = patt {} [精准匹配]							
							location ~ patt{} [正则匹配]
						
						Uri解析过程：						
							   先判断有没有精准匹配,如果有,则停止匹配过程，返回结果
							   #如果 $uri == patt,匹配成功，使用configA
							   location = patt {
									config A
							   }
							   如：
								  #精准匹配
								  location = / {
											root   /var/www/html/;
											index  index.htm index.html;
								  }	
								  #一般匹配
								  location / {
											root   /usr/local/nginx/html;
											index  index.html index.htm;
								  }
							说明：
								如果访问http://xxx.com/，定位流程是：								　
								1: 精准匹配中”/”,得到index页为index.htm，结束
								2: 再次访问http://xxx.com/index.htm,此次内部转跳uri已经是”/index.htm”,根目录为/usr/local/nginx/html								
								3: 最终结果,访问了/usr/local/nginx/html/index.htm
							
							再来看，正则也来参与
								location / {
									root   /usr/local/nginx/html;
									index  index.html index.htm;
								}

								location ~ image {
									root /var/www/;
									index index.html;
								}
							说明：
								如果我们访问http://xx.com/image/logo.png
								此时，“/”与”/image/logo.png”匹配
								同时，”image”正则 与”image/logo.png”也能匹配,谁发挥作用?
								正则表达式的成果将会使用，图片真正会访问/var/www/image/logo.png 								
							再如：
								location / {
									root   /usr/local/nginx/html;
									index  index.html index.htm;
								}
								 
								location /foo {
									root /var/www/html;
									index index.html;
								}
							我们访问 http://xxx.com/foo
							对于uri“/foo”,两个location的patt,都能匹配，即 ‘/’能从左前缀匹配 ‘/foo’, ‘/foo’也能左前缀匹配’/foo’，							
							此时, 真正访问/var/www/html/index.html，原因:’/foo’匹配的更长,因此使用它
						结论：
							1）请求Uri，先判断是否有精准匹配，如果命中，返回结果并结束解析过程
							2）判断普通命中，如果多个命中，记录最长的命中结果，注意只是记录不结束，以最长的为准（与普通匹配顺序无关）
							3）判断正则匹配，按顺序从上到下匹配，只要有1个匹配，结束匹配过程并返回结果（与正则顺序有关）
						
					重写rewrite：功能类似于location
						前提：Linux系统得安装pcre库（Perl Compatible Regular Expressions，perl兼容正则表达式），安装pcre库是为了使Nginx支持HTTP Rewrite模块
						含义：该指令通过正则表达式的使用来改变URI.可以同时存在一个或者多个指令，按照顺序一次对URL进行匹配和处理
						位置：该指令在server块或server块内的location块中配置 
						相关指令：
							关键字：
								if  (条件) {}  #设定条件,再进行重写 
								set #设置变量
								return #返回状态码 
								break #跳出rewrite
								rewrite #重写

							语法格式：
							If 空格 (条件) {
								重写模式
							}
							说明：
								条件写法：3种
								1: “=”来判断相等, 用于字符串比较
								2: “~” 用正则来匹配(此处的正则区分大小写)， ~* 不区分大小写的正则								  
								3: -f -d -e来判断是否为文件,为目录,是否存在
							全局变量：
								$args							-请求URL中的请求参数，如：http://www.myweb.name/server/source?arg1=value1&arg2=value2中的arg1=value1&arg2=value2
								$content_length					-请求头中的Content_Length字段
								$content_type					-请求头中的Content_type字段
								$document_root					-请求的根路径
								$document_uri					-请求中的uri，不包括请求指令，如：http://www.myweb.name/server/source?arg1=value1&arg2=value2中的/server/source
								$host							-请求URL中的主机部分，如：http://www.myweb.name/server中的www.myweb.name，在nginx中server中的server_name配置项配置
								$http_user_agent				-客户端的代理信息
								$http_cookie					-客户端的cookie信息
								$limit_rate						-Nginx服务器对网络速率的限制，限速
								$remote_addr					-客户端地址
								$remote_port					-客户端与服务端连接的端口号
								$remote_user					-客户端用户
								$request_body_file        		-请求体本地资源文件名称
								$request_method					-请求方法，如：GET/POST
								$request_filename				-请求资源文件路径名	
								$request_uri					-请求中的uri，包括请求指令	
								$query_string					-与$args相同
								$scheme							-客户端请求使用的协议，如：HTTP、HTTPS、FTP协议
								$server_protocol         		-客户端请求协议的版本，如：HTTP/1.0、HTTP/1.1
								$server_addr					-服务器地址
								$server_name					-服务器名称
								$server_port					-服务器端口号
								$uri							-与$document_uri相同								
							例子:
								#客户端为100的禁止访问
								if  ($remote_addr = 192.168.1.100) {
									return 403;
								}
								#判断IE浏览器并重写
								if ($http_user_agent ~ MSIE) {
										rewrite ^.*$ /ie.htm;
										break; #不break会循环重定向
								}

								if (!-e $document_root$fastcgi_script_name) {
									rewrite ^.*$ /404.html break; #此处还要加break
								} 
								说明：
									以xx.com/dsafsd.html这个不存在页面为例，观察访问日志(tail logs/access.log), 日志中显示的访问路径,依然是GET /dsafsd.html HTTP/1.1						
									提示: 服务器内部的rewrite和302重定向跳转不一样，跳转的话URL都变了，变成重新http请求404.html，
										  而内部rewrite，上下文没变，就是说fastcgi_script_name（默认SCRIPT_FILENAME = $fastcgi_script_name）仍然是dsafsd.html，因此会循环重定向，所有需要break										 
							
							set:设置变量，可以用来达到多条件判断时作标志用来达到apache下的rewrite_condition的效果							
								如下: 
								#判断浏览器并重写，且不用break
								if ($http_user_agent ~* msie) {
												set $isie 1;
								}
								if ($fastcgi_script_name = ie.html) {
									set $isie 0;
								}
								if ($isie 1) {
									rewrite ^.*$ ie.html;
								}
							
						Rewrite语法：Rewrite 正则表达式  重定向后的位置					
						goods-3.html ---->goods.php?goods_id=3
						goods-([\d]+)\.html --->goods.php?goods_id =$1  

						location /ecshop {						
							index index.php;
							rewrite goods-([\d]+)\.html$ /ecshop/goods.php?id=$1;
							rewrite category-(\d+)-b(\d+)\.html /ecshop/category.php?id=$1&brand=$2;					
						}
						#注意:用Url重写时, 正则里如果有”{}”,正则要用双引号""包起来
						
					nginx + php 编译流程：	
						fastcgi含义：
							apache是把php当做自己的一个模块来启动的，而nginx则是把http请求变量转发给php进程，
							即php独立进程，与nginx进行通信称为fastcgi运行方式
						总结：	
							把请求的信息转发给默认9000端口的PHP进程，让PHP进程处理指定目录下的PHP文件					
							如下例子:
							location ~ \.php$ {
								root html;
								fastcgi_pass   127.0.0.1:9000;
								fastcgi_index  index.php;
								fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
								include        fastcgi_params;
							}
							说明：
								1: 碰到php文件
								2: 把根目录定位到html
								3: 把请求上下文转交给9000端口PHP进程
								4: 并告诉PHP进程，当前的脚本是 $document_root$fastcgi_script_name(注:PHP会去找这个脚本并处理)
								
					网页压缩与传播速度优化：
						案例：
							观察网易新闻news.163.com的头信息
							请求:
							Accept-Encoding:gzip,deflate,sdch,br
							响应:
							Content-Encoding:gzip
							Content-Length:36093
							再把页面另存下来,观察,约10W字节,实际传输的36093字节,原因-就在于gzip压缩上							
							原理: 
							浏览器---请求---->声明可以接受gzip压缩或deflate压缩或compress或sdch压缩或br压缩
							从http协议的角度看---请求头声明acceopt-encoding: gzip deflate sdch br (是指压缩算法,其中sdch是google倡导的一种压缩方式,目前支持的服务器尚不多)
							服务器-->回应---把内容用gzip方式压缩---->发给浏览器
									浏览<-----解码gzip-----接收gzip压缩内容----

							推算一下节省的带宽:
							假设 news.163.com  PV（page view页面浏览量）  2亿
							2*10^8  *  9*10^4 字节 == 
							2*10^8 * 9 * 10^4  * 10^-9 = 12*K*G = 18T，节省的带宽是非常惊人的！
						
						使用：
							位置：HTTP段
							参数说明：
								#gzip配置的参数
								gzip on|off  #是否开启gzip
								gzip_buffers 32 4K| 16 8K #缓冲(压缩在内存中缓冲几块? 每块多大?)
								gzip_comp_level [1-9] #推荐6 压缩级别(级别越高,压的越小,越浪费CPU计算资源)
								gzip_disable #正则匹配URI 什么样的Uri不进行gzip
								gzip_min_length 200 #开始压缩的最小长度(再小就不要压缩了,意义不在)
								gzip_http_version 1.0|1.1 #开始压缩的http协议版本(可以不设置,目前几乎全是1.1协议)
								gzip_proxied          #设置请求者代理服务器,该如何缓存内容
								gzip_types text/plain  application/xml #对哪些类型的文件用压缩 如txt,xml,html ,css
								gzip_vary on|off  #是否传输gzip压缩标志

								注意: 
								图片/mp3这样的二进制文件,不必压缩,因为压缩率比较小, 比如100->80字节,而且压缩也是耗费CPU资源的								
								比较小的文件不必压缩
					
					Nginx缓存设置，提高网站的性能：		
						介绍：对于网站的图片,尤其是新闻站, 图片一旦发布, 改动的可能是非常小的.希望能否在用户访问一次后,图片缓存在用户的浏览器端,且时间比较长的缓存，在nginx中用expires设置
						使用：在[虚拟主机]location或if段里来写
							  格式：  
							  expires 30s;
							  expires 30m;
							  expires 2h;
							  expires 30d;
							注意：服务器的日期要准确,如果服务器的日期落后于实际日期,可能导致缓存失效！  
							如：
							location  ^~ /imgs/ {
								root opt/wangjialuo/;
								expires 1d;									
							}
						
						缓存另一种手段：利用304状态码
							304状态码：
							   如果客户端发送了一个带条件的GET请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个304状态码。
							   简单的表达就是：服务端已经执行了GET，但文件未变化！
							原理是: 服务器响应文件内容时同时响应etag标签(内容的签名,内容一变,它也变)和 last_modified_since 2个标签值
									浏览器下次去请求时,头信息发送这两个标签, 服务器检测文件有没有发生变化，如无，直接头信息返回 etag,last_modified_since
									浏览器知道内容无改变,于是直接调用本地缓存。这个过程,也请求了服务器,但是传着的内容极少。		
									对于变化周期较短的,如静态html,js,css,比较适于用这个方式！
							
					Nginx反向代理与负载均衡：
						介绍：apache
							反向代理[动静分离]：nginx利用proxy实现，如：客户端发起一个PHP请求，Nginx服务器自己不处理，通过proxy_pass转发给其他服务器（如：apache）
												来处理，而且Nginx服务器只负责处理静态的数据
							负载均衡：nginx利用upstream实现，反向代理后端如果有多台服务器（如：多台apache）,自然形成负载均衡
							
							问题：如何利用proxy_pass指向多台服务器?
							实现过程：把多台服务器用upstream指定绑定在一起并起个组名,然后proxy_pass指向该组；
							负载均衡流程：
																	upstream[指向组名]	     服务器1[组成员]
								客户端 ----------------> nginx ----------------------------> 服务器2[组成员]
																					         服务器3[组成员]
							负载均衡算法IRule：
								分类：区别于Java后台的SpringBoot技术栈的负载均衡
									1）轮询，默认的策略，如果server挂掉，能自动剔除
									2）加权，默认是1
									    如：									   
										upstream  1xuepai.com {   
											server   192.168.99.100:42000 weight=1; 
											server   192.168.99.100:42001 weight=2;  
										}
									3）最少链接数，把请求分配到连接数最少的server
										如：
										upstream  1xuepai.com {   
											least_conn;
											server   192.168.99.100:42000; 
											server   192.168.99.100:42001;  
										}
									4）IP_HASH一致性哈希，每个请求会按照访问ip的hash值分配，这样同一客户端连续的Web请求都会被分发到同一server进行处理，可以解决session共享的问题。如果server挂掉，能自动剔除
										如：
										upstream  1xuepai.com {   
											ip_hash;
											server   192.168.99.100:42000; 
											server   192.168.99.100:42001;  
										}
							位置：upstream配置在http段内，server段外；proxy_pass配置在server段内的location端里，即：server段与upstream端平级
							
							示例：
								#这里域名要和下面proxy_pass的一样，且不能使用下划线，timeMachine不能写成time_machine
								#负载均衡池
								upstream  timeMachine.com {
									#fail_timeout：当该时间内服务器没响应，则认为服务器失效，默认10s
									#max_fails：允许连接失败次数，默认为1
									server    10.240.35.113:8081 weight=1 fail_timeout=3s max_fails=3;
									server    10.240.35.113:8082  weight=2 backup; # 备机
									server    10.240.35.113:8084 down; # down主机暂停服务
								}
								 
								server { 								
									#keepalive_requests 120; # 单连接请求上限次数
									listen       8083; # 监听端口
									#监听地址，写ip地址或主机名、域名都可 
									server_name  10.240.35.113;
								 
									#location块，请求的url过滤
									location / {   
										proxy_pass http://timeMachine.com;
										proxy_redirect default;
										#proxy_connect_timeout：与服务器连接的超时时间，默认60s
										proxy_connect_timeout 3s;
									}
								 
									error_page   500 502 503 504  /50x.html; # 错误页
									location = /50x.html {
										root   html;
									}
									
								}
								
								结论：等待时间 = proxy_connect_timeout + fail_timeout * max_fails
					
					Nginx图片服务器：
						目的：为了缓存web服务器的访问压力，可以单独搭建文件服务器[图片、视频等]
						使用：
							在server段添加配置，如：
							location /images/ {
								root /var/www/nginx_imgs/	                #图片存放位置
							}
							
	三、nginx与keepalived实现HA，可参考：https://www.jb51.net/article/142443.htm
			目的：高并发情况下为了考虑Nginx的单点故障，真正做到架构高可用性HA，实现nginx的故障转移，同时做好监控报警。
			keepalived定义：
				Keepalived是一个基于VRRP协议[Virtual Router Redundancy Protocol，即 虚拟路由冗余协议，是实现路由器高可用HA的容错协议]来实现的服务高可用方案，可以利用其来避免IP单点故障，
				类似的工具还有heartbeat、corosync、pacemaker。
				但是它一般不会单独出现，而是与其它负载均衡技术（如lvs、haproxy、nginx）一起工作来达到集群的高可用。
			原理流程：			
						keepalived[充当路由角色]										 服务器1
				客户端 ----------------------------> nginx[master] --------------------> 服务器2
						访问公共IP[虚拟IP]			 nginx[backup]						 服务器3
						
				注释：nginx[master]和 nginx[backup]都需要与keepalived建立绑定关系，当nginx[master]宕机后，会自动的切换
					  到nginx[backup]使用；当恢复nginx[master]后又重新切换回nginx[master]使用。			
			使用：
				示例：环境准备
					2台nginx[linux]服务器（如：192.168.1.129与192.168.1.130，nginx端口默认80）都安装好keepalived服务
					一个虚拟IP[公共IP] 192.168.1.131
					安装keepalived服务
						#准备下载好的keepalived安装包文件
						#解压并进入解压目录
						tar -zxvf keepalived-1.2.18.tar.gz
						cd keepalived-1.2.18
						#指定安装目录并安装
						./configure --prefix=/opt/keepalived
						make && make install
					将keepalived安装成Linux系统服务
						mkdir /etc/keepalived/
						cp /opt/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/
						cp /opt/keepalived/etc/sysconfig/keepalived /etc/sysconfig/
						ln -s /opt/sbin/keepalived /usr/sbin/
						ln -s /opt/keepalived/sbin/keepalived /sbin/
				    设置keepalived服务开机启动
						chkconfig keepalived on
					  
					keepalived目录结构：
						安装目录下有：
							bin		-genhash
							etc		-keepalived
												 -keepalived.conf[重点]
												 -samples
									-rc.d
												 -init.d
									-sysconfig
												 -keepalived
							sbin
							share
					  
					同时修改安装路径下的keepalived.conf的配置与/etc/keepalived/keepalived.conf的配置[2台服务都要配置]
					配置信息如下：
						! Configuration File for keepalived (!、#都是注释)
					　　global_defs { #全局配置
					　　notification_email { #通知邮件，接收邮件
					　　　　acassen@firewall.loc
					　　　　failover@firewall.loc
					　　　　sysadmin@firewall.loc
					　　}
					　　notification_email_from Alexandre.Cassen@firewall.loc #发送邮件
					　　smtp_server 192.168.200.1 #发送邮件服务器IP
					　　smtp_connect_timeout 30 #发送邮件超时时间
					　　router_id LVS_01 #这个配置要唯一，一般配置成Linux的主机名hostname，可以在/etc/hosts里添加设置
					　　}　

					　　vrrp_script chk_nginx {
					　　　　script "/etc/keepalived/nginx_check.sh" ## 检测 nginx 状态的脚本路径
					　　　　interval 2 ## 检测时间间隔
					　　　　weight -20 ## 如果条件成立，权重-20
					　　}　

					　　vrrp_instance VI_1 { #实例 VI_1 名字可以随意 但是不建议修改
					　　　　state MASTER # 主服务器MASTER 从服务器 BACKUP
					　　　　interface em1 # em1 网卡，查看网卡：ifconfig 或 ip addr
					　　　　virtual_router_id 129 #virtual_router_id 主备要不一致，要与当前的机器IP保持一致
					　　　　mcast_src_ip 192.168.1.129 		#本机IP
							nopreempt 		#优先级高的异常恢复后抢占问题
							priority 100 　　# 优先级 数字越大优先级越高 priority 的值 主服务器要大于从服务器
					　　　　advert_int 1　　#设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒
					　　　　authentication { # 主从通信验证类型及密码 
					　　　　　　auth_type PASS　　#设置vrrp验证类型，主要有PASS和AH两种
					　　　　　　auth_pass 1111　　#设置vrrp验证密码，在同一个vrrp_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信
					　　　　}　

					　　　　## 将track_script块加入instance配置块
					　　　　track_script {
					　　　　　　chk_nginx ## 执行 Nginx 监控的服务
					　　　　}

							## 虚拟IP[VIP]池
					　　　　virtual_ipaddress {
					　　　　192.168.1.131/135 #VRRP HA 虚拟IP地址 如果有多个VIP，继续换行填写
					　　　　}
					　　}　

					编写Nginx状态检测脚本：vim /etc/keepalived/nginx_check.sh
					内容如下：
						#!/bin/bash
					　　A=`ps -C nginx –no-header |wc -l`
					　　if [ $A -eq 0 ];then
					　　　　/opt/nginx/sbin/nginx				#nginx安装目录
					　　　　sleep 2
					　　　　if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then

					　　　　　　killall keepalived
					　　　　fi
					　　fi
					保存后，给脚本赋执行权限：chmod +x/etc/keepalived/nginx_check.sh
					注意点：Keepalived主从配置文件不同点
					　　1）router_id不一致
					　　2）state 主服务器是MASTER，从服务器是BACKUP
					　　3）priority 主服务器大于从服务器
					keepalived使用：
					　　启动： service keepalived start
					　　停止： service keepalived stop
					　　重启： service keepalived restart						
					测试：开启2台服务的nginx和keepalived服务，然后关闭任意一个nginx观察访问的
						  虚拟IP：192.168.1.131 所使用的的nginx服务是129的？还是130的？
				
----------------------------------------------大数据定制技术----------------------------------------------------------------------------------	
	一、Hadoop
		简介：
	
	二、Zookeeper
		简介：
		
	三、Kafka
		简介：
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	