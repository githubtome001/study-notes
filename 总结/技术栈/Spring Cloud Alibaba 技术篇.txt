----------------------------------------------Spring Cloud Alibaba 技术篇----------------------------------------------------------------------------------------------------						  					 
	前沿：Spring Cloud Netflix项目进入维护模式，意味着不在开发新的组件与功能；2018年Spring Cloud Alibaba正式入驻Spring Cloud官方孵化器；
	官方地址：https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md
	基本介绍：Spring Cloud Alibaba 致力于提供微服务开发的一站式解决方案，依托 Spring Cloud Alibaba，只需要添加一些注解和少量配置，就可以将 Spring Cloud 应用接入阿里微服务解决方案，
			  通过阿里中间件来迅速搭建分布式应用系统。
	组件：
		Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。
		Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。
		RocketMQ：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。
		Dubbo：Apache Dubbo™ 是一款高性能 Java RPC 框架。
		Seata：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。
		Alibaba Cloud ACM：一款在分布式架构环境中对应用配置进行集中管理和推送的应用配置中心产品。
		Alibaba Cloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。
		Alibaba Cloud SchedulerX: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。
		Alibaba Cloud SMS: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。
    基本技术栈：
			  1、服务限流降级：默认支持 Servlet、Feign、RestTemplate、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控
			  2、服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持
			  3、分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新
			  4、消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力
			  5、分布式事务：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题
			  6、阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据
			  7、分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务
			  8、阿里云短信服务：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道					  
	1、服务注册与发现（配置中心、消息总线）
		-Nacos（= Spring Cloud Eureka + Spring Cloud Config + Spring Cloud Bus）：
				简介：Spring Cloud Alibaba 项目中开发分布式应用微服务的子组件，致力于服务发现、配置和管理微服务，基于 DNS 和基于 RPC 的服务发现。
				由来：前4个字母分别为Naming和Configuration的前两个字母，最后的s是Service。
				官方定义：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。
				服务注册中心比较：
					服务注册与发现模型      CAP模型      控制台管理       社区活跃度
						Eureka               AP            支持             低（2.X版本闭源）
						Zookeeper            CP            不支持           中
						Consoul              CP            支持             高
						Nacos                AP            支持             高
				安装：
					前提：本地Java8 + Maven环境已经OK；下载：https://github.com/alibaba/nacos/releases/tag/1.1.4；解压并运行：bin/startup.cmd；访问：http://localhost:8848/nacos；默认用户名与密码：nacos
				关键特性：
					1、服务发现和服务健康监测；
					2、动态配置服务（通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-config 实现配置的动态变更）；
					3、动态DNS服务与服务及其元数据管理（通过 Nacos Server 和 spring-cloud-starter-alibaba-nacos-discovery 实现服务的注册与发现）
				官方文档：https://spring.io/projects/spring-cloud-alibaba；
				使用（服务注册与发现、提供方与消费方）：
				服务注册中心
				1,引用模块：POM.XML
					<!--Nacos的服务注册与发现模块-->
					<dependency>
						<groupId>org.springframework.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
					</dependency>
					<!--监控-->
					<dependency>
						<groupId>org.springframework.boot</groupId>
						<artifactId>spring-boot-starter-actuator</artifactId>
					</dependency>
					<!--统一管理-->
					<dependencyManagement>
						<dependencies>
							<!--spring cloud Hoxton.SR1-->
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-dependencies</artifactId>
								<version>Hoxton.SR1</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
							<!--Spring cloud alibaba 2.1.0.RELEASE-->
							<dependency>
								<groupId>com.alibaba.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>2.1.0.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
						</dependencies>
				</dependencyManagement>
				2,配置参数：application.properties或YML文件的配置
				格式：
					 spring:
						  application:
							name: 程序名，服务提供方或服务消费方
						  cloud:
							nacos:
							  discovery:
								server-addr: localhost:8848 # 服务IP与端口
									
									metadata: # 元数据管理
									  name1: healthy1 
									  name2: healthy2
					 management:  #开启监控
						  endpoints:
							web:
							  exposure:
								include: '*'
				3,申明使用：程序main入口，添加@EnableDiscoveryClient注解，开启服务治理与发现；
				说明：Nacos 底层支持负载均衡，即集成了Ribbon；所以在服务的消费方，可以直接使用RestTemplate API相关功能，只需注入容器即可使用；				
				如：@Configuration
					public class ApplicationContextConfig {
						@Bean
						@LoadBalanced
						public RestTemplate getRestTemplate() {
							return new RestTemplate();
						}
					}
				Nacos与cap：支持cp与ap的切换，说明：c所有节点看到的数据是一致的，a所有请求都有响应；AP支持的是注册临时实例；CP支持的是注册持久化实例；该模式下注册实例之前必须先注册服务，如果服务不存在，则会返回错误。
				切换指令：curl -X PUT `$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'；
				服务配置中心（动态配置，相当于SpringCloud Config + SpringCloud Config Bus）：
				基础篇：
				1,引用模块：POM.XML	
				<!-- nacos config-->
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
				</dependency>
				<!-- SpringCloud ailibaba nacos-->
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
				</dependency>
				<!--统一管理-->
				<dependencyManagement>
						<dependencies>
							<!--spring cloud Hoxton.SR1-->
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-dependencies</artifactId>
								<version>Hoxton.SR1</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
							<!--Spring cloud alibaba 2.1.0.RELEASE-->
							<dependency>
								<groupId>com.alibaba.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>2.1.0.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
						</dependencies>
				</dependencyManagement>
				2,配置参数：创建bootstrap.yml或创建bootstrap.properties文件（系统级别）#先制定从服务配置中心拉取配置，再配application.yml或application.properties；
				格式:
				#bootstrap.yml
					server:
					  port: 3377
					spring:
					  application:
						name: nacos-config-client #客户端
					  cloud:
						nacos:
						  discovery:
							server-addr: localhost:8848 #Nacos服务注册中心地址
						  config:
							server-addr: localhost:8848 #Nacos作为配置中心地址
							file-extension: yaml #指定yaml格式配置，目前官网仅支持yaml和properties；
							group: TEST_GROUP #组
							namespace: 6fecc7ae-f02f-49ef-ace7-80d2f671df77 #命名空间
							#配置文件格式：
							#${prefix}-${spring.profile.active}.${file-extension}
							#${spring.application.name}-${spring.profile.active}.${file-extension}；即：#nacos-config-client-dev.yml
				#application.yml			
					spring:
					  profiles:
						active: dev #表示开发环境
				3,申明使用：程序main入口，添加@EnableDiscoveryClient注解，开启服务治理与发现；具体业务控制器添加@RefreshScope，让其支持Nacos的动态刷新功能（相当于Cloud + Bus）；
				4、Nacos添加配置信息：
					DataID：
						格式：${prefix}-${spring.profile.active}.${file-extension}；
						      prefix默认为spring.application.name的值；
							  spring.profile.active 即为当前环境对应的profile，可以通过配置项 spring.profile.active来配置；
							  file-exetension为配置内容的数据格式，可以通过配置项spring.cloud.nacos.config.file-extension来配置；
				5、测试：通过修改配置信息，已实现自动刷新；			  						
				6、多环境多项目管理:多个微服务项目、开发、生产、测试环境配置如何管理问题；
				   Namespace + Group + DataID 三者关系？为什么这么设计？
				   类似Java里面的package名和类名，namespace是可以用于区分部署环境的，用来隔离的，Group和DataID逻辑上区分两个目标对象。
				   默认情况：Namespace=public ，Group=DEFAULT_GROUP,默认Cluster是DEFAULT； 
				   DataID方案：
				      默认空间+默认分组+新建dev和test两个DataID；如：nacos-config-client-dev.yaml、nacos-config-client-test.yaml
				      即指定客户端spring.profile.active配置和配置文件的DataID来使不同环境下读取不同的配置；
				   Group方案：
					  通过组来区分环境，如：DEV_GROUP、TEST_GROUP下有nacos-config-client-info.yaml
					  即在bootstrap.yml中添加一行组的配置信息，如：group: TEST_GROUP #组
				   Namespace方案：
				      通过新建命名空间，如：DEV、TEST
					  即在bootstrap.yml中添加一行命名空间的配置信息，如：namespace: 6fecc7ae-f02f-49ef-ace7-80d2f671df77 #命名空间
				高级篇：	  
				Nacos集群与持久化配置：
					官网说明：
					    参考网址：https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html
						架构设计：
							客户端	+	VIP （Nginx集群+ KeepAlived）    +    Nacos（IP1）、Nacos（IP2）、Nacos（IP3）  +  MySQL（集群）
					数据库说明：
					    默认Nacos支持的事嵌入式数据库实现数据的存储，如何配置多个Nacos节点，数据的存储一致性存在问题，为了解决这一问题，Nacos采用了集中式存储方式来支持集群部署，目前只支持MySQL存储；
					    Nacos支持3中部署模式：
							单机模式-用于测试与单机
							集群模式-用于生产确保高可用
							多集群模式-适用于多数据中心场景
					持久化配置：
						Nacos默认自带的是嵌入式数据库derby；
						切换到MySQL：（Windows版）
							1、在Nacos安装目录下找到conf/nacos-mysql.sql脚本文件并执行；
							2、在Nacos安装目录下找到conf/application.properties配置文件中，尾部添加如下配置：
							   spring.datasource.platform=mysql
							   db.num=1
							   db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
							   db.user=root #用户名
							   db.password=root #密码
							3、启动Nacos，执行指令：./startup.cmd
						Linux版Nacos+MySQL生产环境配置：
							方案：1个Nginx+3个Nacos注册中心+1个MySQL
							步骤：
								1、安装Linux版的Nacos；
								2、集群配置：
									  a.在Nacos安装目录下找到conf/nacos-mysql.sql脚本文件并执行到MySQL；
								      b.在Nacos安装目录下找到conf/application.properties配置文件中添加数据库配置信息；
									  c.Nacos的集群配置，安装目录下找到conf/cluster.conf配置文件，添加如下配置：
										192.168.111.222：3333
										192.168.111.222：4444
										192.168.111.222：5555 #3个Nacos服务对外提供的服务IP与端口，IP使用用hostname -i指令获得；
									  d.编辑Nacos的启动脚本startup.sh，使它能够接受不同的启动端口，格式：./startup.sh -p 3333，即启动端口为3333的Nacos服务；
									    编辑文件：
											找到 while getops ":m:f:s:" opt 改成 while getops ":m:f:s:p:" opt
									        添加配置：
												p)
												   PORT=$OPTARG;;
											找到 nohup $JAVA ${JAVA_OPT} nacos.nacos >> 改成 nohup -Dserver.port=${PORT} $JAVA ${JAVA_OPT} nacos.nacos >> 
									  e.Nginx的配置，负载均衡：
											#指定配置文件nginx.conf 启动；./nginx -C 指定路径/nginx.conf；
											#配置信息
											upstream cluster{
												192.168.111.222：3333
												192.168.111.222：4444
												192.168.111.222：5555
											}
											server{
												listen:1111;
												server_name:localhost;
												
												location / {
													#root html;
													#index index.html index.htm;
													proxy_pass:http://cluster;
												}
												
											}
								3、测试：测试客户端注册到Nacos中，客户端添加配置信息：spring.cloud.nacos.discovery.server-addr=192.168.111.222:1111 # VIP做了代理处理；											
	2、服务的熔断与降级：
		 -Sentinel（Hystrix）：
			官网：http://github.com/alibaba/Sentinel
			介绍：随着微服务的流行，服务和服务之间的稳定性变得越来越重要。Sentinel 作为流量防卫组件，以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。
			处理问题：服务雪崩、服务降级、服务熔断、服务限流
			特征：
				 1、丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等
				 2、完备的实时监控
				 3、广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合
				 4、完善的 SPI（串行外设接口） 扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。
			组成：
				 核心库（Java 客户端）
				 控制台（Dashboard）、默认端口为:8080		
				 使用：		
					 1、部署Sentinel Dashboard	
						下载地址：https://github.com/alibaba/Sentinel/releases
						启动(默认端口：8080)：
						java -jar sentinel-dashboard-1.6.0.jar
						java -jar -Dserver.port=8888 sentinel-dashboard-1.6.0.jar
						默认用户名密码：sentinel					
					 2、核心库的配置使用：
						1,引用模块：POM.XML	
						<!-- SpringCloud ailibaba nacos-->
						<dependency>
							<groupId>com.alibaba.cloud</groupId>
							<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
						</dependency>
						<!-- SpringCloud ailibaba sentinel-datasource-nacos 持久化需要用到-->
						<dependency>
							<groupId>com.alibaba.csp</groupId>
							<artifactId>sentinel-datasource-nacos</artifactId>
						</dependency>
						<!-- SpringCloud ailibaba sentinel-->
						<dependency>
							<groupId>com.alibaba.cloud</groupId>
							<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
						</dependency>
						<!--统一管理-->
						<dependencyManagement>
							<dependencies>
								<!--spring cloud Hoxton.SR1-->
								<dependency>
									<groupId>org.springframework.cloud</groupId>
									<artifactId>spring-cloud-dependencies</artifactId>
									<version>Hoxton.SR1</version>
									<type>pom</type>
									<scope>import</scope>
								</dependency>
								<!--Spring cloud alibaba 2.1.0.RELEASE-->
								<dependency>
									<groupId>com.alibaba.cloud</groupId>
									<artifactId>spring-cloud-alibaba-dependencies</artifactId>
									<version>2.1.0.RELEASE</version>
									<type>pom</type>
									<scope>import</scope>
								</dependency>
							</dependencies>
						</dependencyManagement>
						2,配置参数：创建bootstrap.yml或创建bootstrap.properties文件
						格式：
						    server:
								port: 8401
							spring:
							  application:
								name: cloudalibaba-sentinal-service
							  cloud:
								nacos:
								  discovery:
									#Nacos服务注册中心地址
									server-addr: localhost:8848
								sentinel:
								  transport:
									#配置Sentin dashboard地址
									dashboard: localhost:8080
									# 默认8719端口，假如被占用了会自动从8719端口+1进行扫描，直到找到未被占用的端口
									port: 8719
								  datasource: #规则持久化
									ds1:
									  nacos:
										server-addr: localhost:8848
										dataId: cloudalibaba-sentinel-service
										groupId: DEFAULT_GROUP
										data-type: json
										rule-type: flow
							management: #监控
							  endpoints:
								web:
								  exposure:
									include: '*'
							feign:
							  sentinel:
								enabled: true #激活Sentinel对Feign的支持								
						3、申明使用：入口处添加注解@EnableDiscoveryClient，开发服务的注册发现；
						4、Sentinel Dashboard 功能基本说明：						
						   结论：Sentinel采用的懒加载，说明服务只有调用一次后才有监控效果；
						   功能点：实时监控、簇点链路、流控规则、降级规则、热点规则、系统规则、授权规则、集群流控、机器列表
						   流控规则：
						       基本介绍：
								   资源名：唯一名称，默认请求路径；
								   针对来源：默认default，可以填服务名，对调用者进行限流；
								   阈值类型/单机阈值
									   QPS：每秒钟请求数量，当调用达到阈值，进行限流；
									   线程数：当调用的API线程数达到阈值，进行限流；
								   是否集群：不需要集群
								   流控模式：
									   直接：当API调用达到限流条件，直接限流；
									   关联：当关联的资源达到限流条件，限流自己；
									   链路：只记录指定链路上的流量（指定资源从入口资源进来的流量，达到阈值，进行限流）、API级别的针对来源；
								   流控效果：
									   快速失败：直接失败、抛异常；
									   Warm Up：预热，根据codeFactor（冷加载因子的值，默认3）从阈值/codeFactor，经过预热时长，才能达到设置的QPS阈值；
									   排队等待：匀速排队，严格控制请求通过的间隔时间；阈值类型为QPS；
							   流控模式：
								   直接：默认， 直接-> 快速失败，默认；达到限流执行，给出默认限流提示：Blocked by Sentinel （flow limiting） 
								   关联：当与A关联的资源B达到阈值后，就限流A自己；即B惹事，A挂了；QPS类型；
								   链路：
							   流控效果：
							       快速失败：直接-> 快速失败（默认的流控处理）；
								   Warm Up：预热，限流冷启动；使用场景：秒杀场景瞬间会有很多流量，会把系统打死，预热的目的把系统保护起来，慢慢把流量放进来，慢慢带达到阈值；
								   排队等待：匀速排队，严格控制请求通过的间隔时间；阈值类型为QPS；
						   降级规则：
							   降级策略：
								RT：平均响应时间，秒级，平均响应时间（如：1min）超过阈值RT（如：200ms）且时间窗口内超过的请求QPS>=5，两个条件同时满足触发降级，窗口期过后关闭断路器；RT最大4900；
									通过-Dcsp.sentinel.statistic.max。rt=xxx配置
								异常比例：秒级，QPS>=5且执行异常比例（如：int age = 10/0）超过阈值（如：0.2）触发降级，窗口期过后关闭降级；异常比例的阈值在0-1之间；
								异常数：分钟级，异常数超过阈值（如：设置成5）触发降级，窗口期过后关闭降级；时间窗口必须大于或等于60；如：执行结果调用5次都是失败，进入PAGE页面，下一次就不进入错误页面，直接熔断降级；
							   Sentinel的断路器是没有半开状态的，半开的状态，系统会自动检测是否请求有异常，没有异常就关闭断路器恢复使用，有异常则继续打开断路器不可用；  
                           热点规则：
						       热点key限流：官网：https://github.com/alibaba/Sentinel/wiki/%E7%83%AD%E7%82%B9%E5%8F%82%E6%95%B0%E9%99%90%E6%B5%81；
							   兜底方法：系统默认与用户自定义，像Blocked by Sentinel （flow limiting）系统默认，自定义：@SentinelResource 进行降级处理；
								   如：
								   @GetMapping("/testHotKey")
								   //@SentinelResource(value = "testHotKey") //Sentinel的默认提示都是： Blocked by Sentinel (flow limiting)；
								   @SentinelResource(value = "testHotKey",blockHandler = "deal_testHotKey")
								   public String testHotKey(@RequestParam(value = "p1",required = false)String p1,@RequestParam(value = "p2",required = false)String p2) {														 
										return "testHotKey";
								   }
								   #兜底方法，假设QPS=1，参数索引设0，则第一个参数只要QPS超过每秒1次，马上降级处理；
								   public String deal_testHotKey(String p1, String p2, BlockException exception) {
										return "testHotKey, o(╥﹏╥)o";
								   }
						       #Sentinel配置热点规则：资源名，限流模式仅支持QPS，参数索引，单机阈值，统计窗口时长，是否集群...；
							   #访问：http://localhost:8401/testHotKey?p1=1；
						       参数例外项：
								   描述：当超过1秒钟一个后，达到阈值1后马上被限流，若期望p1参数当它是某一个特殊值时，它的限流值和平时不一样，则配置参数例外项；
								   前提条件：热点参数的注意点，参数必须为基本类型或者String；
								   注意：@SentinelResource主管配置违规，走兜底方法；运行异常走运行异常；
						   系统规则：
							   含义：系统保护规则从应用级别的入口流量进行控制，是应用整体维度的，不是资源维度的；仅对入口流量生效，支持以下几种模式：
									 load自适应：对Linux/Unix机器生效，一般设置为CPU核数*2.5
									 CPU利用率：
									 平均RT：
									 并发线程数：
									 入口QPS：
						   @SentinelResource详细说明：
							   按资源名称限流：如下
									@GetMapping("/byResource")
									@SentinelResource(value = "byResource",blockHandler = "handleException")
									public CommonResult byResource() {
										return  new CommonResult(200,"按照资源名称限流测试",new Payment(2020L,"serial001"));
									}
									public CommonResult handleException(BlockException exception) {
										return  new CommonResult(444,exception.getClass().getCanonicalName() + "\t 服务不可用");
									}
							   Sentinel流控规则配置：资源名：byResource；
							   按照Url地址限流：如下   
								   @GetMapping("/rateLimit/byUrl")
								   @SentinelResource(value = "byUrl")
								   public CommonResult byUrl() {
										return  new CommonResult(200,"按照byUrl限流测试",new Payment(2020L,"serial002"));
								   }
							   Sentinel流控规则配置：资源名：/rateLimit/byUrl；
						   兜底方法面临问题：
							   系统默认，与业务代码耦合，导致代码膨胀，需要全局统一处理；
						   用户自定义限流处理逻辑：如下
							   @GetMapping("/rateLimit/customerBlockHandler")
							   @SentinelResource(value = "customerBlockHandler",blockHandlerClass = CustomerBlockHandler.class,blockHandler = "handlerException1")									
							   public CommonResult customerBlockHandler() {
								return  new CommonResult(200,"按照客户自定义限流测试",new Payment(2020L,"serial003"));
							   }
							   #自定义限流处理
							   public class CustomerBlockHandler {
									public static CommonResult handlerException1(BlockException exception) {
										return  new CommonResult(444,"按照客户自定义的Glogal 全局异常处理1",new Payment(2020L,"serial003"));
									}
									public static CommonResult handlerException2(BlockException exception) {
										return  new CommonResult(444,"按照客户自定义的Glogal 全局异常处理2",new Payment(2020L,"serial003"));
									}
							   }
						   服务熔断功能：
							   Ribbon系列：
								  服务提供者：
									  1、引入依赖POM.XML
									   <!-- SpringCloud ailibaba nacos-->
										<dependency>
											<groupId>com.alibaba.cloud</groupId>
											<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
										</dependency>
										<!-- SpringCloud ailibaba sentinel-->
										<dependency>
											<groupId>com.alibaba.cloud</groupId>
											<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
										</dependency>
										<dependency>
									   2、application.yml的配置
									   server:
										  port: 9003
										spring:
										  application:
											name: nacos-payment-provider
										  cloud:
											nacos:
											  discovery:
												server-addr: localhost:8848
										management: #监控
										  endpoints:
											web:
											  exposure:
												include: '*'
									   3、入口添加注解@EnableDiscoveryClient；
									   4、业务类
									   @RestController
										public class PaymentController {
											@Value("${server.port}")
											private  String serverPort;
											public static HashMap<Long, Payment > map = new HashMap<>();
											static {
												map.put(1L,new Payment(1L,"1111"));
												map.put(2L,new Payment(2L,"2222"));
												map.put(3L,new Payment(3L,"3333"));
											}
											@GetMapping(value = "/paymentSQL/{id}")
											public CommonResult<Payment> paymentSQL(@PathVariable("id") Long id) {
												Payment payment = map.get(id);
												CommonResult<Payment> result = new CommonResult<>(200,"from mysql,serverPort: " + serverPort,payment);
												return result;
											}
										}
								  服务消费者：
							           1、引入依赖POM.XML（同提供者）
									   2、application.yml的配置
									   server:
										  port: 84
										spring:
										  application:
											name: nacos-order-consumer
										  cloud:
											nacos:
											  discovery:
												server-addr: localhost:8848
											sentinel:
											  transport:
												dashboard: localhost:8080
												port: 8719
										#消费者将去访问的微服务名称
										server-url:
										  nacos-user-service: http://nacos-payment-provider
										#激活Sentinel对Feign的支持
										feign:
										  sentinel:
											enabled: true
										3、入口添加注解@EnableDiscoveryClient；
										4、配置类将RestTemplate注入容器；
										@Configuration
										public class ApplicationContextConfig {
											@Bean
											@LoadBalanced
											public RestTemplate getRestTemplate() {
												return new RestTemplate();
											}
										}
										5、业务类
										@RestController
										@Slf4j
										public class CircleBreakerController {
											public static  final  String SERVICE_URL = "http://nacos-payment-provider";
											@Resource
											private RestTemplate restTemplate;
											@RequestMapping("/consumer/fallback/{id}")
										//  @SentinelResource(value = "fallback")
										//  @SentinelResource(value = "fallback",fallback ="handlerFallback")
										//  fallback管运行异常，blockHandler管配置违规，都配blockHandler起作用
											@SentinelResource(value = "fallback",fallback ="handlerFallback",blockHandler = "blockHandler") 
											public CommonResult<Payment> fallback(@PathVariable Long id) {
												CommonResult<Payment> result = restTemplate.getForObject(SERVICE_URL + "/paymentSQL/" + id,CommonResult.class,id);
												if(id == 4){
													throw new IllegalArgumentException("IllegalArgument ,非法参数异常...");
												}else if(result.getData() == null) {
													throw new NullPointerException("NullPointerException,该ID没有对应记录，空指针异常");
												}
												return  result;
											}
											public CommonResult handlerFallback(@PathVariable Long id,Throwable e) {
												Payment payment = new Payment(id,"null");
												return new CommonResult(444,"异常handlerFallback，exception内容： " + e.getMessage(), payment);
											}
											public CommonResult blockHandler(@PathVariable Long id,BlockException e) {
												Payment payment = new Payment(id,"null");
												return new CommonResult(444,"blockHandler-sentinel 限流，BlockException： " + e.getMessage(), payment);
											}
											//OpenFeign
											@Resource
											private PaymentService paymentService;
											@GetMapping(value = "/consumer/paymentSQL/{id}")
											public CommonResult<Payment> paymentSQL(@PathVariable("id") Long id){
												return paymentService.paymentSQL(id);
											}
										}
										6、Feign 接口声明
										@FeignClient(value = "nacos-payment-provider",fallback = PaymentFallbackService.class)
										public interface PaymentService {
											@GetMapping(value = "/paymentSQL/{id}")
											public CommonResult< Payment > paymentSQL(@PathVariable("id") Long id);

										}
										#兜底降级处理
										@Component
										public class PaymentFallbackService implements PaymentService{
											@Override
											public CommonResult< Payment > paymentSQL(Long id) {
												return new CommonResult<>(444,"服务降级返回，PaymentFallbackService",new Payment(id,"ErrorSerial"));
											}
										}
							   Feign系列：
								  1、引入依赖POM.XML，对OpenFeign的支持；
								  2、application.yml的配置，服务端口，Sentinel对Feign支持等；
								  3、入口添加注解@EnableFeignClients；
						   Sentinel规则持久化：
							  问题：一旦重启应用，Sentinel规则将消失，生产环境需要将配置规则进行持久化；
							  方案：将限流配置规则持久化进Nacos保存即可；
							  使用：
								  1、引入依赖POM.XML
								  <dependency>
									<groupId>com.alibaba.csp</groupId>
									<artifactId>sentinel-datasource-nacos</artifactId>
								  </dependency>
								  2、application.yml的配置
								    server:
										port: 8401
									spring:
									  application:
										name: cloudalibaba-sentinal-service
									  cloud:
										nacos:
										  discovery:
											#Nacos服务注册中心地址
											server-addr: localhost:8848
										sentinel:
										  transport:
											#配置Sentin dashboard地址
											dashboard: localhost:8080
											# 默认8719端口，假如被占用了会自动从8719端口+1进行扫描，直到找到未被占用的端口
											port: 8719
										  datasource: #规则持久化
											ds1: #数据源
											  nacos:
												server-addr: localhost:8848
												dataId: cloudalibaba-sentinel-service
												groupId: DEFAULT_GROUP
												data-type: json
												rule-type: flow #流控规则
							      3、Nacos添加配置规则：结构如下
								     [
										{
											"resource": "/rateLimit/byUrl", #资源名称
											"limitApp": "default", #来源应用
											"grade": 1, #阈值类型，0线程数，1QPS
											"count": 1, #单机阈值
											"strategy": 0, #流控模式，0直接，1关联，2链路
											"controlBehavior": 0, #流控效果，0快速失败，1Warm Up，2排队等待
											"clusterMode": false #是否集群
										}
									]									 
						   # 注意
							 Sentinel控制台中修改规则：仅存在于服务的内存中，重启后丢失配置，Nacos控制台中修改规则：服务的内存中规则会更新，Nacos中持久化规则也会更新，重启后依然保持。							 
						5、优点:
								1、sentinel配置变动后通知非常的迅速吗，秒杀springcloud原来的config几条街，
								   毕竟原来的config是基于git，不提供可视化界面，动态变更还需要依赖bus来通知所有的客户端变化；
								2、与hystrix相比，sentinel更加的轻量级,并且支持动态的限流调整，更加友好的界面ui。
	3、分布式事务处理：							
		分布式事务问题：分布式以前，单机单库没这个问题；分布式之后，全局的事务问题难以保证；一次业务操作需要跨多个数据源（如：订单、仓库、账户）或需要跨多个系统（如：订单、仓库、账户）进行远程调用，就会产生分布式事务问题；						
		Seata：
			含义：Senta是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务；
				  2019年1月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案，Simple Extensible Autonomous Transaction Architecture，简单可扩展自治事务框架；
			官网：http://seata.io/zh-cn/；
			分布式事务处理过程ID+三组件模型：
				Transaction ID全局唯一的事务ID - XID；
				TC - 事务协调者，理解成Seata服务器，维护全局和分支事务的状态，驱动全局事务提交或回滚。
				TM - 事务管理器，理解成事务的发起方，定义全局事务的范围：开始全局事务、提交或回滚全局事务。
				RM - 资源管理器，理解成事务的参与方，即数据库，管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。
			执行过程：
									Begin/Commit/Rollback
				微服务（TM） <--------------------------------------> TC
					（RM）<-------------------------------------->    TC			
					（DB）  Register Branch / Branch Commit/Rollback
			执行细节：
				TM开启分布式事务（TM向TC注册全局事务记录）；
				按业务场景，编排数据库、服务等事务内资源（RM向TC汇报资源准备状态）；
				TM结束分布式事务，事务一阶段结束（TM通知TC提交/回滚分布式事务)；
				TC汇总事务信息，决定分布式事务是提交还是回滚；
				TC通知所有RM提交/回滚资源，事务二阶段结束；
			模式：
				AT模式：提供无侵入自动补偿的事务模式，目前已支持 MySQL、 Oracle 、PostgreSQL和 TiDB的AT模式，H2 开发中；
				TCC模式：支持 TCC 模式并可与 AT 混用，灵活度更高；
				SAGA模式：为长事务提供有效的解决方案；
				XA模式：开发中；
				高可用：
			AT模式如何做到对业务的无侵入？？？
				一阶段加载：业务SQL ---->前置快照（before image）---->执行SQL------>后置快照（after image）---->行锁----->提交业务SQL
				二阶段提交：一阶段正常执行完，删除快照与行锁即可；
				二阶段回滚：校验脏写--------->还原数据------->删除中间数据（快照与行锁）
			使用：本地事务@Transational，全局事务添加@GlobalTransactional；				
			Seata-Server服务安装：
				官方地址：https://seata.io/zh-cn/blog/download.html；
				下载解压并执行以下操作：
					1、修改配置文件file.conf：主要修改自定义事务组名称+事务日志存储模式为db+数据库连接信息；		
						服务模块（service）修改事务组，
						存储模块（store）修改事务的存储模式为db；
					2、执行建库脚本db_store.sql，在MySQL数据库seata下会生成3张表（branck_table、global_table、lock_table）；	
					3、修改配置文件registry.conf：配置注册信息为nacos，以及nacos服务的连接信息；			
				使用：
					准备：分别启动Nacos服务，以及Seata-Server服务，seata-server.bat脚本执行；
					演示：订单、库存、账户案例说明，用户下一个订单，经历了三个服务（订单、库存、账户），各个服务对应各自的数据库（seata-order、seata-storage、seata-account）；
						  流程：下订单-减库存-减账户余额-改订单状态，涉及到分布式事务处理问题；
						1、建立好各自的数据库以及表信息，seata-order（t_order）、seata-storage（t_storage）、seata-account（t_account）；
						2、建立好各自的数据库的日志回滚表，直接执行eata官方提供的脚本sdb_undo_log.sql即可；
					各个微服务核心模块使用：
						订单模块
							1、POM.XML，引入Nacos、Seata、OpenFeign等相关依赖；
							<!-- nacos -->
							<dependency>
								<groupId>com.alibaba.cloud</groupId>
								<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
							</dependency>
							<!-- seata-->
							<dependency>
								<groupId>com.alibaba.cloud</groupId>
								<artifactId>spring-cloud-starter-alibaba-seata</artifactId>
								<exclusions>
									<exclusion>
										<groupId>io.seata</groupId>
										<artifactId>seata-all</artifactId>
									</exclusion>
								</exclusions>
							</dependency>
							<dependency>
								<groupId>io.seata</groupId>
								<artifactId>seata-all</artifactId>
								<version>0.9.0</version>
							</dependency>
							<dependency>
								<groupId>org.springframework.cloud</groupId>
								<artifactId>spring-cloud-starter-openfeign</artifactId>
							</dependency>
							2、YML配置
							server:
							  port: 2001
							spring:
							  application:
								name: seata-order-service
							  cloud:
								alibaba:
								  seata:
									# 自定义事务组名称需要与seata-server中的对应
									tx-service-group: fsp_tx_group
								nacos:
								  discovery:
									server-addr: 127.0.0.1:8848
							  datasource:
								# 当前数据源操作类型
								type: com.alibaba.druid.pool.DruidDataSource
								# mysql驱动类
								driver-class-name: com.mysql.cj.jdbc.Driver
								url: jdbc:mysql://localhost:3306/seata_order?useUnicode=true&characterEncoding=UTF-8&useSSL=false&serverTimezone=GMT%2B8
								username: root
								password: 123456
							feign:
							  hystrix:
								enabled: false
							3、file.conf、registry.conf拷贝到资源路径下；
							4、业务逻辑层，下订单
							@Service
							@Slf4j
							public class OrderServiceImpl implements OrderService {
								@Resource
								private OrderDao orderDao;//DAO层
								@Resource
								private AccountService accountService;//接口，Feign调用的是账户服务；
								@Resource
								private StorageService storageService;
								/**
								 * 下订单->减库存->减余额->改状态
								 * GlobalTransactional seata开启分布式事务，异常时回滚,name保证唯一即可
								 * @param order 订单对象
								 */
								@Override
								@GlobalTransactional(name = "fsp-create-order",rollbackFor = Exception.class)
								public void create(Order order) {
									// 1 新建订单
									orderDao.create(order);
									// 2 扣减库存
									storageService.decrease(order.getProductId(), order.getCount());
									// 3 扣减账户
									accountService.decrease(order.getUserId(), order.getMoney());
									// 4 修改订单状态,从0到1,1代表已完成
									orderDao.update(order.getUserId(), 0);
								}
							}
							@FeignClient(value = "seata-account-service")//服务名
							public interface AccountService {
								/**
								 * 减余额
								 * @param userId
								 * @param money
								 * @return
								 */
								@PostMapping(value = "/account/decrease")
								CommonResult decrease(@RequestParam("userId") Long userId, @RequestParam("money") BigDecimal money);
							}	
				测试：
					业务逻辑层添加分布式全局事务注解后，可以解决分布式事务问题；
	4、集群高并发情况下分布式唯一全局ID生成：				
		需求：在分布式系统中需要对大量的消息数据进行唯一性标识，如：美团订单、优惠券等；
		ID生成规则部分硬性要求：
			1、全局唯一，不能出现重复的ID号，既然是唯一标识；
			2、趋势递增，在MySQL的InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用Btree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能；
			3、单调递增，保证下一个ID一定大于上一个ID，例如事务版本号，IM增量消息、排序等特殊需求；
			4、信息安全，如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定的URL即可；如果是订单号就更危险了，竞争对手可以直接知道我们一天的单量。所以在一些应用场景下，需要ID无规则，让竞争对手不好猜；
			5、含时间戳，能够在开发中快速了解分布式ID的生成时间；
		ID生成系统的可用性要求：
			高可用：发一个获取分布式ID的请求，服务器就可以保证99.999%的情况下给我创建一个唯一的分布式ID；
			低延迟：发一个获取分布式ID的请求，服务器响应速度要快；
			高QPS：假如并发一口气10万个创建分布式ID请求同时过来，服务器要顶得住并一下子成功创建10万个唯一的分布式ID；
		通用方案：
			UUID：性能高，本地生成，没有网络延迟，36位字符组成，可以保证唯一性，但入数据库性能差；MySQL官方推荐的主键ID越短越好，低于36位；
			数据库自增主键：利用数据库自增ID和MySQL的replace into实现的；单机合适，集群分布式不合适（水平扩展困难、高QPS下性能差、频繁的I/O读写操作）、设置不同的增长步长；
			基于Redis生成全局Id策略：因为Redis是单线程的天生保证原子性，可以使用原子操作INCR和INCRBY来实现；集群分布式可以实现、设置不同的增长步长；
			snowflake：
				介绍：Twitter的分布式自增ID算法snowflake（雪花算法）；1秒钟可生成26万个趋势自增的可排序的ID，计算结果是一个64bit大小的整数，最终转化为字符串的长度最多可以有19位；
				核心组成部分：
					0-1bit符号位，0表示整数，1表示负数，取0；
					000000-41bit时间戳位，换算成时间69年；
					000000-10bit工作进程（机器ID）位，共计1024节点；包括5位datacenterId与5位workeId；
					000000-12bit序列号位，每毫秒生成4095个ID序号；
				使用糊涂工具包提供雪花算法，地址：https://github.com/looly/hutool/，引入：hutool-captcha 模块即可；
				工具类：
				@Slf4j
				@Component
				public class IdGeneratorSnowflake {
					/** 工作机器ID(0~31) */
					private long workerId = 0;
					/** 数据中心ID(0~31) */
					private long datacenterId = 1;
					private Snowflake snowflake = IdUtil.createSnowflake(workerId,datacenterId);
					@PostConstruct
					public void init() {
						try {
							workerId = NetUtil.ipv4ToLong(NetUtil.getLocalhostStr());
							log.info("当前机器的workerId:{}", workerId);
						} catch (Exception e) {
							log.info("当前机器的workerId获取失败", e);
							workerId = NetUtil.getLocalhostStr().hashCode();
							log.info("当前机器 workId:{}", workerId);
						}
					}
					public synchronized long snowflakeId() {
						return snowflake.nextId();
					}
					public synchronized long snowflakeId(long workerId, long datacenterId) {
						snowflake = IdUtil.createSnowflake(workerId, datacenterId);
						return snowflake.nextId();
					}
					public static void main(String[] args) {						
						System.out.println(new IdGeneratorSnowflake().snowflakeId());//1240536286826201088
					}
				}
				缺点：存在时钟回拨，可能出现ID重复；趋势递增并不是严格递增；解决方案参考：百度开源的分布式唯一ID生成器UidGenerator，Leaf-美团点评分布式ID生成系统；
				完整代码：
				import org.apache.commons.lang3.RandomUtils;
				import org.apache.commons.lang3.StringUtils;
				import org.apache.commons.lang3.SystemUtils;				 
				import java.net.Inet4Address;
				import java.net.UnknownHostException;				 
				/**
				 * Twitter_Snowflake<br>
				 * SnowFlake的结构如下(每部分用-分开):<br>
				 * 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 <br>
				 * 1位标识，由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0<br>
				 * 41位时间截(毫秒级)，注意，41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截)
				 * 得到的值），这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的（如下下面程序IdWorker类的startTime属性）。41位的时间截，可以使用69年，年T = (1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69<br>
				 * 10位的数据机器位，可以部署在1024个节点，包括5位datacenterId和5位workerId<br>
				 * 12位序列，毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号<br>
				 * 加起来刚好64位，为一个Long型。<br>
				 * SnowFlake的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。
				 */
				public class SnowflakeIdWorker {
				 
					// ==============================Fields===========================================
					/** 开始时间截 (2015-01-01) */
					private final long twepoch = 1489111610226L;
				 
					/** 机器id所占的位数 */
					private final long workerIdBits = 5L;
				 
					/** 数据标识id所占的位数 */
					private final long dataCenterIdBits = 5L;
				 
					/** 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */
					private final long maxWorkerId = -1L ^ (-1L << workerIdBits);
				 
					/** 支持的最大数据标识id，结果是31 */
					private final long maxDataCenterId = -1L ^ (-1L << dataCenterIdBits);
				 
					/** 序列在id中占的位数 */
					private final long sequenceBits = 12L;
				 
					/** 机器ID向左移12位 */
					private final long workerIdShift = sequenceBits;
				 
					/** 数据标识id向左移17位(12+5) */
					private final long dataCenterIdShift = sequenceBits + workerIdBits;
				 
					/** 时间截向左移22位(5+5+12) */
					private final long timestampLeftShift = sequenceBits + workerIdBits + dataCenterIdBits;
				 
					/** 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */
					private final long sequenceMask = -1L ^ (-1L << sequenceBits);
				 
					/** 工作机器ID(0~31) */
					private long workerId;
				 
					/** 数据中心ID(0~31) */
					private long dataCenterId;
				 
					/** 毫秒内序列(0~4095) */
					private long sequence = 0L;
				 
					/** 上次生成ID的时间截 */
					private long lastTimestamp = -1L;
				 
					private static SnowflakeIdWorker idWorker;
				 
					static {
						idWorker = new SnowflakeIdWorker(getWorkId(),getDataCenterId());
					}
				 
					//==============================Constructors=====================================
					/**
					 * 构造函数
					 * @param workerId 工作ID (0~31)
					 * @param dataCenterId 数据中心ID (0~31)
					 */
					public SnowflakeIdWorker(long workerId, long dataCenterId) {
						if (workerId > maxWorkerId || workerId < 0) {
							throw new IllegalArgumentException(String.format("workerId can't be greater than %d or less than 0", maxWorkerId));
						}
						if (dataCenterId > maxDataCenterId || dataCenterId < 0) {
							throw new IllegalArgumentException(String.format("dataCenterId can't be greater than %d or less than 0", maxDataCenterId));
						}
						this.workerId = workerId;
						this.dataCenterId = dataCenterId;
					}
				 
					// ==============================Methods==========================================
					/**
					 * 获得下一个ID (该方法是线程安全的)
					 * @return SnowflakeId
					 */
					public synchronized long nextId() {
						long timestamp = timeGen();
				 
						//如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常
						if (timestamp < lastTimestamp) {
							throw new RuntimeException(
									String.format("Clock moved backwards.  Refusing to generate id for %d milliseconds", lastTimestamp - timestamp));
						}
				 
						//如果是同一时间生成的，则进行毫秒内序列
						if (lastTimestamp == timestamp) {
							sequence = (sequence + 1) & sequenceMask;
							//毫秒内序列溢出
							if (sequence == 0) {
								//阻塞到下一个毫秒,获得新的时间戳
								timestamp = tilNextMillis(lastTimestamp);
							}
						}
						//时间戳改变，毫秒内序列重置
						else {
							sequence = 0L;
						}
				 
						//上次生成ID的时间截
						lastTimestamp = timestamp;
				 
						//移位并通过或运算拼到一起组成64位的ID
						return ((timestamp - twepoch) << timestampLeftShift)
								| (dataCenterId << dataCenterIdShift)
								| (workerId << workerIdShift)
								| sequence;
					}
				 
					/**
					 * 阻塞到下一个毫秒，直到获得新的时间戳
					 * @param lastTimestamp 上次生成ID的时间截
					 * @return 当前时间戳
					 */
					protected long tilNextMillis(long lastTimestamp) {
						long timestamp = timeGen();
						while (timestamp <= lastTimestamp) {
							timestamp = timeGen();
						}
						return timestamp;
					}
				 
					/**
					 * 返回以毫秒为单位的当前时间
					 * @return 当前时间(毫秒)
					 */
					protected long timeGen() {
						return System.currentTimeMillis();
					}
				 
					private static Long getWorkId(){
						try {
							String hostAddress = Inet4Address.getLocalHost().getHostAddress();
							int[] ints = StringUtils.toCodePoints(hostAddress);
							int sums = 0;
							for(int b : ints){
								sums += b;
							}
							return (long)(sums % 32);
						} catch (UnknownHostException e) {
							// 如果获取失败，则使用随机数备用
							return RandomUtils.nextLong(0,31);
						}
					}
				 
					private static Long getDataCenterId(){
						int[] ints = StringUtils.toCodePoints(SystemUtils.getHostName());
						int sums = 0;
						for (int i: ints) {
							sums += i;
						}
						return (long)(sums % 32);
					}
				 
				 
					/**
					 * 静态工具类
					 *
					 * @return
					 */
					public static synchronized Long generateId(){
						long id = idWorker.nextId();
						return id;
					}
				 
					//==============================Test=============================================
					/** 测试 */
					public static void main(String[] args) {
						System.out.println(System.currentTimeMillis());
						long startTime = System.nanoTime();
						for (int i = 0; i < 50000; i++) {
							long id = SnowflakeIdWorker.generateId();
							System.out.println(id);
						}
						System.out.println((System.nanoTime()-startTime)/1000000+"ms");
					}
				}

								